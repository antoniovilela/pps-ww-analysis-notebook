{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/eos/home-a/antoniov/SWAN_projects/env/uproot4/lib/python3.7/site-packages', '/cvmfs/sft.cern.ch/lcg/views/LCG_97apython3/x86_64-centos7-gcc8-opt/python', '/cvmfs/sft.cern.ch/lcg/views/LCG_97apython3/x86_64-centos7-gcc8-opt/lib', '', '/cvmfs/sft.cern.ch/lcg/views/LCG_97apython3/x86_64-centos7-gcc8-opt/lib/python3.7/site-packages', '/cvmfs/sft.cern.ch/lcg/releases/LCG_97python3/Python/3.7.6/x86_64-centos7-gcc8-opt/lib/python37.zip', '/cvmfs/sft.cern.ch/lcg/releases/LCG_97python3/Python/3.7.6/x86_64-centos7-gcc8-opt/lib/python3.7', '/cvmfs/sft.cern.ch/lcg/releases/LCG_97python3/Python/3.7.6/x86_64-centos7-gcc8-opt/lib/python3.7/lib-dynload', '/eos/home-a/antoniov/SWAN_projects/env/uproot4/lib/python3.7/site-packages', '/cvmfs/sft.cern.ch/lcg/views/LCG_97apython3/x86_64-centos7-gcc8-opt/lib/python3.7/site-packages/IPython/extensions', '/scratch/antoniov/.ipython']\n",
      "sklearn: 0.23.2\n",
      "tensorflow: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "#import mplhep\n",
    "from matplotlib.colors import LogNorm\n",
    "from joblib import dump, load\n",
    "#plt.style.use(mplhep.style.CMS)\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'/eos/home-a/antoniov/SWAN_projects/env/uproot4/lib/python3.7/site-packages')\n",
    "print ( sys.path )\n",
    "\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print ( \"sklearn: {}\".format(sklearn.__version__) )\n",
    "print ( \"tensorflow: {}\".format(tf.__version__) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#tf.config.experimental.list_physical_devices('GPU')\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "if gpus:\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    \n",
    "print ( gpus )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'2017B': 2.360904801, '2017C1': 5.313012839, '2017C2': 3.264135878, '2017D': 4.074723964, '2017E': 8.958810514, '2017F1': 1.708478656, '2017F2': 7.877903151, '2017F3': 3.632463163}\n",
      "Luminosity: 37.190432966\n"
     ]
    }
   ],
   "source": [
    "# run_ranges_periods, df_run_ranges, fiducial_cuts, fiducial_cuts_all, aperture_period_map, aperture_parametrisation, check_aperture, get_data, process_data_protons_multiRP\n",
    "from processing import *\n",
    "\n",
    "save_figures = False\n",
    "\n",
    "run_tables = False\n",
    "train_model = False\n",
    "learning_rate_scan = False\n",
    "run_grid_search = False\n",
    "save_model = False\n",
    "\n",
    "scaler_path = \"model/standard_scaler_test-multiRP_2021_01_22-17_27_08.joblib\"\n",
    "model_path = \"model/keras_model_test-multiRP_2021_01_22-17_46_10.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021/02/04 10:51:35\n",
      "Total time elapsed: 0\n"
     ]
    }
   ],
   "source": [
    "label = \"GGToWW-AQGC-A0W1e-6\"\n",
    "\n",
    "fileNames_signal = [\n",
    "    \"output-GGToWW-AQGC-A0W1e-6.h5\"\n",
    "]\n",
    "\n",
    "import time\n",
    "print( time.strftime(\"%Y/%m/%d %H:%M:%S\", time.localtime() ) )\n",
    "time_s_ = time.time()\n",
    "\n",
    "df_counts_signal, df_protons_multiRP_signal, df_protons_singleRP_signal, df_ppstracks_signal = 4 * [None]\n",
    "df_protons_multiRP_signal_index, df_protons_multiRP_signal_events, df_ppstracks_signal_index = 3 * [None]\n",
    "\n",
    "if run_tables:\n",
    "    with pd.HDFStore( \"reduced-data-store-{}.h5\".format( label ), complevel=5 ) as store_:\n",
    "\n",
    "        df_counts_signal, df_protons_multiRP_signal, df_protons_singleRP_signal, df_ppstracks_signal = get_data( fileNames_signal )\n",
    "        df_protons_multiRP_signal_index, df_protons_multiRP_signal_events, df_ppstracks_signal_index = process_data_protons_multiRP( df_protons_multiRP_signal, df_ppstracks_signal, apply_fiducial=True, runOnMC=True )\n",
    "\n",
    "        store_[ \"counts\" ] = df_counts_signal\n",
    "        store_[ \"protons_multiRP\"] = df_protons_multiRP_signal_index\n",
    "        store_[ \"events_multiRP\" ] = df_protons_multiRP_signal_events\n",
    "\n",
    "with pd.HDFStore( \"reduced-data-store-{}.h5\".format( label ), 'r' ) as store_:\n",
    "    df_counts_signal = store_[ \"counts\" ]\n",
    "    df_protons_multiRP_signal_index = store_[ \"protons_multiRP\" ]\n",
    "    df_protons_multiRP_signal_events = store_[ \"events_multiRP\" ]\n",
    "        \n",
    "time_e_ = time.time()\n",
    "print ( \"Total time elapsed: {:.0f}\".format( time_e_ - time_s_ ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>xi</th>\n",
       "      <th>thx</th>\n",
       "      <th>thy</th>\n",
       "      <th>t</th>\n",
       "      <th>ismultirp</th>\n",
       "      <th>rpid</th>\n",
       "      <th>arm</th>\n",
       "      <th>jet0_pt</th>\n",
       "      <th>jet0_eta</th>\n",
       "      <th>jet0_phi</th>\n",
       "      <th>...</th>\n",
       "      <th>yhigh</th>\n",
       "      <th>eff_all_weighted</th>\n",
       "      <th>eff_all_2017B</th>\n",
       "      <th>eff_all_2017C1</th>\n",
       "      <th>eff_all_2017C2</th>\n",
       "      <th>eff_all_2017D</th>\n",
       "      <th>eff_all_2017E</th>\n",
       "      <th>eff_all_2017F1</th>\n",
       "      <th>eff_all_2017F2</th>\n",
       "      <th>eff_all_2017F3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>run</th>\n",
       "      <th>lumiblock</th>\n",
       "      <th>event</th>\n",
       "      <th>slice</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"20\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">2266</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">226551</th>\n",
       "      <th>0</th>\n",
       "      <td>0.047449</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>-0.022129</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>203.932037</td>\n",
       "      <td>-1.086163</td>\n",
       "      <td>0.536344</td>\n",
       "      <td>...</td>\n",
       "      <td>4.298</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.465410</td>\n",
       "      <td>0.556090</td>\n",
       "      <td>0.443605</td>\n",
       "      <td>0.472268</td>\n",
       "      <td>0.067031</td>\n",
       "      <td>0.435046</td>\n",
       "      <td>0.337598</td>\n",
       "      <td>0.325470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.051457</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>-0.342805</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>203.932037</td>\n",
       "      <td>-1.086163</td>\n",
       "      <td>0.536344</td>\n",
       "      <td>...</td>\n",
       "      <td>4.698</td>\n",
       "      <td>0.413233</td>\n",
       "      <td>0.450737</td>\n",
       "      <td>0.525839</td>\n",
       "      <td>0.503517</td>\n",
       "      <td>0.482181</td>\n",
       "      <td>0.388314</td>\n",
       "      <td>0.395700</td>\n",
       "      <td>0.360058</td>\n",
       "      <td>0.250709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2270</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">226932</th>\n",
       "      <th>0</th>\n",
       "      <td>0.032306</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000075</td>\n",
       "      <td>-0.234367</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>320.698212</td>\n",
       "      <td>-1.370161</td>\n",
       "      <td>-1.832831</td>\n",
       "      <td>...</td>\n",
       "      <td>4.298</td>\n",
       "      <td>0.218844</td>\n",
       "      <td>0.389240</td>\n",
       "      <td>0.471496</td>\n",
       "      <td>0.394722</td>\n",
       "      <td>0.409256</td>\n",
       "      <td>0.029614</td>\n",
       "      <td>0.227123</td>\n",
       "      <td>0.067900</td>\n",
       "      <td>0.157089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.119526</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.020655</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>320.698212</td>\n",
       "      <td>-1.370161</td>\n",
       "      <td>-1.832831</td>\n",
       "      <td>...</td>\n",
       "      <td>4.698</td>\n",
       "      <td>0.427809</td>\n",
       "      <td>0.469562</td>\n",
       "      <td>0.543481</td>\n",
       "      <td>0.501683</td>\n",
       "      <td>0.487398</td>\n",
       "      <td>0.402601</td>\n",
       "      <td>0.392528</td>\n",
       "      <td>0.367567</td>\n",
       "      <td>0.307674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2273</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">227227</th>\n",
       "      <th>0</th>\n",
       "      <td>0.056667</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>-0.233687</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>335.731934</td>\n",
       "      <td>0.259870</td>\n",
       "      <td>-3.108820</td>\n",
       "      <td>...</td>\n",
       "      <td>4.298</td>\n",
       "      <td>0.340731</td>\n",
       "      <td>0.461063</td>\n",
       "      <td>0.557300</td>\n",
       "      <td>0.443280</td>\n",
       "      <td>0.480026</td>\n",
       "      <td>0.062661</td>\n",
       "      <td>0.435031</td>\n",
       "      <td>0.345071</td>\n",
       "      <td>0.329394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.053138</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.030825</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>335.731934</td>\n",
       "      <td>0.259870</td>\n",
       "      <td>-3.108820</td>\n",
       "      <td>...</td>\n",
       "      <td>4.698</td>\n",
       "      <td>0.405743</td>\n",
       "      <td>0.431485</td>\n",
       "      <td>0.521758</td>\n",
       "      <td>0.487558</td>\n",
       "      <td>0.471257</td>\n",
       "      <td>0.381897</td>\n",
       "      <td>0.382895</td>\n",
       "      <td>0.356504</td>\n",
       "      <td>0.248657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2275</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">227457</th>\n",
       "      <th>0</th>\n",
       "      <td>0.068920</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>-0.129756</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>345.850189</td>\n",
       "      <td>-1.458291</td>\n",
       "      <td>3.099098</td>\n",
       "      <td>...</td>\n",
       "      <td>4.298</td>\n",
       "      <td>0.339407</td>\n",
       "      <td>0.459643</td>\n",
       "      <td>0.555368</td>\n",
       "      <td>0.436176</td>\n",
       "      <td>0.479024</td>\n",
       "      <td>0.068559</td>\n",
       "      <td>0.429204</td>\n",
       "      <td>0.338547</td>\n",
       "      <td>0.329445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.115064</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>-0.042887</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>345.850189</td>\n",
       "      <td>-1.458291</td>\n",
       "      <td>3.099098</td>\n",
       "      <td>...</td>\n",
       "      <td>4.698</td>\n",
       "      <td>0.430894</td>\n",
       "      <td>0.467675</td>\n",
       "      <td>0.543631</td>\n",
       "      <td>0.505666</td>\n",
       "      <td>0.492860</td>\n",
       "      <td>0.404837</td>\n",
       "      <td>0.405494</td>\n",
       "      <td>0.373789</td>\n",
       "      <td>0.305451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">2276</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">227546</th>\n",
       "      <th>0</th>\n",
       "      <td>0.090561</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>-0.292089</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>608.518616</td>\n",
       "      <td>-0.521754</td>\n",
       "      <td>-0.481351</td>\n",
       "      <td>...</td>\n",
       "      <td>4.298</td>\n",
       "      <td>0.343356</td>\n",
       "      <td>0.461455</td>\n",
       "      <td>0.554216</td>\n",
       "      <td>0.455506</td>\n",
       "      <td>0.485712</td>\n",
       "      <td>0.064948</td>\n",
       "      <td>0.436865</td>\n",
       "      <td>0.348077</td>\n",
       "      <td>0.330137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.125299</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>-0.142210</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>608.518616</td>\n",
       "      <td>-0.521754</td>\n",
       "      <td>-0.481351</td>\n",
       "      <td>...</td>\n",
       "      <td>4.698</td>\n",
       "      <td>0.428177</td>\n",
       "      <td>0.466368</td>\n",
       "      <td>0.544417</td>\n",
       "      <td>0.516074</td>\n",
       "      <td>0.489326</td>\n",
       "      <td>0.398199</td>\n",
       "      <td>0.402425</td>\n",
       "      <td>0.366728</td>\n",
       "      <td>0.305079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">227579</th>\n",
       "      <th>0</th>\n",
       "      <td>0.118551</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.037683</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>819.906433</td>\n",
       "      <td>0.347934</td>\n",
       "      <td>1.723913</td>\n",
       "      <td>...</td>\n",
       "      <td>4.298</td>\n",
       "      <td>0.392048</td>\n",
       "      <td>0.481202</td>\n",
       "      <td>0.584780</td>\n",
       "      <td>0.544542</td>\n",
       "      <td>0.580318</td>\n",
       "      <td>0.066487</td>\n",
       "      <td>0.446163</td>\n",
       "      <td>0.427186</td>\n",
       "      <td>0.405266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.169634</td>\n",
       "      <td>-0.000121</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.583498</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>819.906433</td>\n",
       "      <td>0.347934</td>\n",
       "      <td>1.723913</td>\n",
       "      <td>...</td>\n",
       "      <td>4.698</td>\n",
       "      <td>0.431079</td>\n",
       "      <td>0.472849</td>\n",
       "      <td>0.543924</td>\n",
       "      <td>0.510822</td>\n",
       "      <td>0.498993</td>\n",
       "      <td>0.404181</td>\n",
       "      <td>0.400537</td>\n",
       "      <td>0.374576</td>\n",
       "      <td>0.294277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2281</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">228042</th>\n",
       "      <th>0</th>\n",
       "      <td>0.105618</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.202386</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>764.408875</td>\n",
       "      <td>-0.631614</td>\n",
       "      <td>2.295349</td>\n",
       "      <td>...</td>\n",
       "      <td>4.298</td>\n",
       "      <td>0.390281</td>\n",
       "      <td>0.477271</td>\n",
       "      <td>0.578496</td>\n",
       "      <td>0.541558</td>\n",
       "      <td>0.594513</td>\n",
       "      <td>0.063268</td>\n",
       "      <td>0.442902</td>\n",
       "      <td>0.422589</td>\n",
       "      <td>0.405114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.128394</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>-0.572105</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>764.408875</td>\n",
       "      <td>-0.631614</td>\n",
       "      <td>2.295349</td>\n",
       "      <td>...</td>\n",
       "      <td>4.698</td>\n",
       "      <td>0.251885</td>\n",
       "      <td>0.148840</td>\n",
       "      <td>0.146538</td>\n",
       "      <td>0.155432</td>\n",
       "      <td>0.125854</td>\n",
       "      <td>0.354170</td>\n",
       "      <td>0.348983</td>\n",
       "      <td>0.310014</td>\n",
       "      <td>0.276988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2282</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">228117</th>\n",
       "      <th>0</th>\n",
       "      <td>0.037401</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>-0.195481</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>321.463196</td>\n",
       "      <td>0.091370</td>\n",
       "      <td>-2.196248</td>\n",
       "      <td>...</td>\n",
       "      <td>4.298</td>\n",
       "      <td>0.321063</td>\n",
       "      <td>0.485559</td>\n",
       "      <td>0.533747</td>\n",
       "      <td>0.436285</td>\n",
       "      <td>0.459491</td>\n",
       "      <td>0.088170</td>\n",
       "      <td>0.381734</td>\n",
       "      <td>0.280034</td>\n",
       "      <td>0.279082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.058293</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>-0.266754</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>321.463196</td>\n",
       "      <td>0.091370</td>\n",
       "      <td>-2.196248</td>\n",
       "      <td>...</td>\n",
       "      <td>4.698</td>\n",
       "      <td>0.420142</td>\n",
       "      <td>0.478493</td>\n",
       "      <td>0.526416</td>\n",
       "      <td>0.488702</td>\n",
       "      <td>0.478250</td>\n",
       "      <td>0.396586</td>\n",
       "      <td>0.399215</td>\n",
       "      <td>0.366944</td>\n",
       "      <td>0.283296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">2286</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">228503</th>\n",
       "      <th>0</th>\n",
       "      <td>0.097616</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>-0.181746</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>649.005920</td>\n",
       "      <td>-0.110481</td>\n",
       "      <td>1.066668</td>\n",
       "      <td>...</td>\n",
       "      <td>4.298</td>\n",
       "      <td>0.386599</td>\n",
       "      <td>0.498423</td>\n",
       "      <td>0.574343</td>\n",
       "      <td>0.544441</td>\n",
       "      <td>0.555001</td>\n",
       "      <td>0.067507</td>\n",
       "      <td>0.456059</td>\n",
       "      <td>0.421817</td>\n",
       "      <td>0.386511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.106465</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>-0.027956</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>649.005920</td>\n",
       "      <td>-0.110481</td>\n",
       "      <td>1.066668</td>\n",
       "      <td>...</td>\n",
       "      <td>4.698</td>\n",
       "      <td>0.429720</td>\n",
       "      <td>0.459885</td>\n",
       "      <td>0.544015</td>\n",
       "      <td>0.507484</td>\n",
       "      <td>0.491206</td>\n",
       "      <td>0.406128</td>\n",
       "      <td>0.394253</td>\n",
       "      <td>0.372266</td>\n",
       "      <td>0.303562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">228565</th>\n",
       "      <th>0</th>\n",
       "      <td>0.088063</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>-0.045153</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>360.785767</td>\n",
       "      <td>0.943119</td>\n",
       "      <td>-1.373860</td>\n",
       "      <td>...</td>\n",
       "      <td>4.298</td>\n",
       "      <td>0.349976</td>\n",
       "      <td>0.463828</td>\n",
       "      <td>0.558480</td>\n",
       "      <td>0.472551</td>\n",
       "      <td>0.503531</td>\n",
       "      <td>0.066496</td>\n",
       "      <td>0.430159</td>\n",
       "      <td>0.358088</td>\n",
       "      <td>0.332457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.043299</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>-0.084826</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>360.785767</td>\n",
       "      <td>0.943119</td>\n",
       "      <td>-1.373860</td>\n",
       "      <td>...</td>\n",
       "      <td>4.698</td>\n",
       "      <td>0.338345</td>\n",
       "      <td>0.432822</td>\n",
       "      <td>0.521732</td>\n",
       "      <td>0.488212</td>\n",
       "      <td>0.469620</td>\n",
       "      <td>0.269663</td>\n",
       "      <td>0.330337</td>\n",
       "      <td>0.269476</td>\n",
       "      <td>0.049302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  xi       thx       thy         t  ismultirp  \\\n",
       "run lumiblock event  slice                                                      \n",
       "1   2266      226551 0      0.047449  0.000007  0.000021 -0.022129          1   \n",
       "                     0      0.051457  0.000022  0.000089 -0.342805          1   \n",
       "    2270      226932 0      0.032306  0.000006 -0.000075 -0.234367          1   \n",
       "                     0      0.119526 -0.000012  0.000005 -0.020655          1   \n",
       "    2273      227227 0      0.056667 -0.000025 -0.000072 -0.233687          1   \n",
       "                     0      0.053138 -0.000011 -0.000024 -0.030825          1   \n",
       "    2275      227457 0      0.068920 -0.000051  0.000024 -0.129756          1   \n",
       "                     0      0.115064  0.000022  0.000018 -0.042887          1   \n",
       "    2276      227546 0      0.090561 -0.000041  0.000076 -0.292089          1   \n",
       "                     0      0.125299 -0.000021  0.000054 -0.142210          1   \n",
       "              227579 0      0.118551 -0.000022 -0.000012 -0.037683          1   \n",
       "                     0      0.169634 -0.000121 -0.000032 -0.583498          1   \n",
       "    2281      228042 0      0.105618  0.000070  0.000014 -0.202386          1   \n",
       "                     0      0.128394  0.000116  0.000039 -0.572105          1   \n",
       "    2282      228117 0      0.037401  0.000059  0.000036 -0.195481          1   \n",
       "                     0      0.058293  0.000015  0.000080 -0.266754          1   \n",
       "    2286      228503 0      0.097616 -0.000020  0.000064 -0.181746          1   \n",
       "                     0      0.106465  0.000008  0.000020 -0.027956          1   \n",
       "              228565 0      0.088063 -0.000030  0.000009 -0.045153          1   \n",
       "                     0      0.043299  0.000032  0.000032 -0.084826          1   \n",
       "\n",
       "                            rpid  arm     jet0_pt  jet0_eta  jet0_phi  ...  \\\n",
       "run lumiblock event  slice                                             ...   \n",
       "1   2266      226551 0       123    0  203.932037 -1.086163  0.536344  ...   \n",
       "                     0       123    1  203.932037 -1.086163  0.536344  ...   \n",
       "    2270      226932 0       123    0  320.698212 -1.370161 -1.832831  ...   \n",
       "                     0       123    1  320.698212 -1.370161 -1.832831  ...   \n",
       "    2273      227227 0       123    0  335.731934  0.259870 -3.108820  ...   \n",
       "                     0       123    1  335.731934  0.259870 -3.108820  ...   \n",
       "    2275      227457 0       123    0  345.850189 -1.458291  3.099098  ...   \n",
       "                     0       123    1  345.850189 -1.458291  3.099098  ...   \n",
       "    2276      227546 0       123    0  608.518616 -0.521754 -0.481351  ...   \n",
       "                     0       123    1  608.518616 -0.521754 -0.481351  ...   \n",
       "              227579 0       123    0  819.906433  0.347934  1.723913  ...   \n",
       "                     0       123    1  819.906433  0.347934  1.723913  ...   \n",
       "    2281      228042 0       123    0  764.408875 -0.631614  2.295349  ...   \n",
       "                     0       123    1  764.408875 -0.631614  2.295349  ...   \n",
       "    2282      228117 0       123    0  321.463196  0.091370 -2.196248  ...   \n",
       "                     0       123    1  321.463196  0.091370 -2.196248  ...   \n",
       "    2286      228503 0       123    0  649.005920 -0.110481  1.066668  ...   \n",
       "                     0       123    1  649.005920 -0.110481  1.066668  ...   \n",
       "              228565 0       123    0  360.785767  0.943119 -1.373860  ...   \n",
       "                     0       123    1  360.785767  0.943119 -1.373860  ...   \n",
       "\n",
       "                            yhigh  eff_all_weighted  eff_all_2017B  \\\n",
       "run lumiblock event  slice                                           \n",
       "1   2266      226551 0      4.298          0.339100       0.465410   \n",
       "                     0      4.698          0.413233       0.450737   \n",
       "    2270      226932 0      4.298          0.218844       0.389240   \n",
       "                     0      4.698          0.427809       0.469562   \n",
       "    2273      227227 0      4.298          0.340731       0.461063   \n",
       "                     0      4.698          0.405743       0.431485   \n",
       "    2275      227457 0      4.298          0.339407       0.459643   \n",
       "                     0      4.698          0.430894       0.467675   \n",
       "    2276      227546 0      4.298          0.343356       0.461455   \n",
       "                     0      4.698          0.428177       0.466368   \n",
       "              227579 0      4.298          0.392048       0.481202   \n",
       "                     0      4.698          0.431079       0.472849   \n",
       "    2281      228042 0      4.298          0.390281       0.477271   \n",
       "                     0      4.698          0.251885       0.148840   \n",
       "    2282      228117 0      4.298          0.321063       0.485559   \n",
       "                     0      4.698          0.420142       0.478493   \n",
       "    2286      228503 0      4.298          0.386599       0.498423   \n",
       "                     0      4.698          0.429720       0.459885   \n",
       "              228565 0      4.298          0.349976       0.463828   \n",
       "                     0      4.698          0.338345       0.432822   \n",
       "\n",
       "                            eff_all_2017C1  eff_all_2017C2  eff_all_2017D  \\\n",
       "run lumiblock event  slice                                                  \n",
       "1   2266      226551 0            0.556090        0.443605       0.472268   \n",
       "                     0            0.525839        0.503517       0.482181   \n",
       "    2270      226932 0            0.471496        0.394722       0.409256   \n",
       "                     0            0.543481        0.501683       0.487398   \n",
       "    2273      227227 0            0.557300        0.443280       0.480026   \n",
       "                     0            0.521758        0.487558       0.471257   \n",
       "    2275      227457 0            0.555368        0.436176       0.479024   \n",
       "                     0            0.543631        0.505666       0.492860   \n",
       "    2276      227546 0            0.554216        0.455506       0.485712   \n",
       "                     0            0.544417        0.516074       0.489326   \n",
       "              227579 0            0.584780        0.544542       0.580318   \n",
       "                     0            0.543924        0.510822       0.498993   \n",
       "    2281      228042 0            0.578496        0.541558       0.594513   \n",
       "                     0            0.146538        0.155432       0.125854   \n",
       "    2282      228117 0            0.533747        0.436285       0.459491   \n",
       "                     0            0.526416        0.488702       0.478250   \n",
       "    2286      228503 0            0.574343        0.544441       0.555001   \n",
       "                     0            0.544015        0.507484       0.491206   \n",
       "              228565 0            0.558480        0.472551       0.503531   \n",
       "                     0            0.521732        0.488212       0.469620   \n",
       "\n",
       "                            eff_all_2017E  eff_all_2017F1  eff_all_2017F2  \\\n",
       "run lumiblock event  slice                                                  \n",
       "1   2266      226551 0           0.067031        0.435046        0.337598   \n",
       "                     0           0.388314        0.395700        0.360058   \n",
       "    2270      226932 0           0.029614        0.227123        0.067900   \n",
       "                     0           0.402601        0.392528        0.367567   \n",
       "    2273      227227 0           0.062661        0.435031        0.345071   \n",
       "                     0           0.381897        0.382895        0.356504   \n",
       "    2275      227457 0           0.068559        0.429204        0.338547   \n",
       "                     0           0.404837        0.405494        0.373789   \n",
       "    2276      227546 0           0.064948        0.436865        0.348077   \n",
       "                     0           0.398199        0.402425        0.366728   \n",
       "              227579 0           0.066487        0.446163        0.427186   \n",
       "                     0           0.404181        0.400537        0.374576   \n",
       "    2281      228042 0           0.063268        0.442902        0.422589   \n",
       "                     0           0.354170        0.348983        0.310014   \n",
       "    2282      228117 0           0.088170        0.381734        0.280034   \n",
       "                     0           0.396586        0.399215        0.366944   \n",
       "    2286      228503 0           0.067507        0.456059        0.421817   \n",
       "                     0           0.406128        0.394253        0.372266   \n",
       "              228565 0           0.066496        0.430159        0.358088   \n",
       "                     0           0.269663        0.330337        0.269476   \n",
       "\n",
       "                            eff_all_2017F3  \n",
       "run lumiblock event  slice                  \n",
       "1   2266      226551 0            0.325470  \n",
       "                     0            0.250709  \n",
       "    2270      226932 0            0.157089  \n",
       "                     0            0.307674  \n",
       "    2273      227227 0            0.329394  \n",
       "                     0            0.248657  \n",
       "    2275      227457 0            0.329445  \n",
       "                     0            0.305451  \n",
       "    2276      227546 0            0.330137  \n",
       "                     0            0.305079  \n",
       "              227579 0            0.405266  \n",
       "                     0            0.294277  \n",
       "    2281      228042 0            0.405114  \n",
       "                     0            0.276988  \n",
       "    2282      228117 0            0.279082  \n",
       "                     0            0.283296  \n",
       "    2286      228503 0            0.386511  \n",
       "                     0            0.303562  \n",
       "              228565 0            0.332457  \n",
       "                     0            0.049302  \n",
       "\n",
       "[20 rows x 62 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_protons_multiRP_signal_index[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>jet0_pt</th>\n",
       "      <th>jet0_eta</th>\n",
       "      <th>jet0_phi</th>\n",
       "      <th>jet0_energy</th>\n",
       "      <th>jet0_mass</th>\n",
       "      <th>jet0_corrmass</th>\n",
       "      <th>jet0_tau1</th>\n",
       "      <th>jet0_tau2</th>\n",
       "      <th>jet0_vertexz</th>\n",
       "      <th>muon0_pt</th>\n",
       "      <th>...</th>\n",
       "      <th>xhigh</th>\n",
       "      <th>ylow</th>\n",
       "      <th>yhigh</th>\n",
       "      <th>MX</th>\n",
       "      <th>YX</th>\n",
       "      <th>diffMWW_MX</th>\n",
       "      <th>ratioMWW_MX</th>\n",
       "      <th>shiftedRatioMWW_MX</th>\n",
       "      <th>diffYWW_YX</th>\n",
       "      <th>eff_all_weighted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>run</th>\n",
       "      <th>lumiblock</th>\n",
       "      <th>event</th>\n",
       "      <th>slice</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"20\" valign=\"top\">1</th>\n",
       "      <th>2266</th>\n",
       "      <th>226551</th>\n",
       "      <th>0</th>\n",
       "      <td>203.932037</td>\n",
       "      <td>-1.086163</td>\n",
       "      <td>0.536344</td>\n",
       "      <td>341.780396</td>\n",
       "      <td>12.367921</td>\n",
       "      <td>13.346299</td>\n",
       "      <td>0.195710</td>\n",
       "      <td>0.150335</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120.215347</td>\n",
       "      <td>...</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "      <td>642.360352</td>\n",
       "      <td>-0.040549</td>\n",
       "      <td>86.471313</td>\n",
       "      <td>1.134615</td>\n",
       "      <td>0.134615</td>\n",
       "      <td>0.089981</td>\n",
       "      <td>0.140127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2270</th>\n",
       "      <th>226932</th>\n",
       "      <th>0</th>\n",
       "      <td>320.698212</td>\n",
       "      <td>-1.370161</td>\n",
       "      <td>-1.832831</td>\n",
       "      <td>676.274719</td>\n",
       "      <td>14.117490</td>\n",
       "      <td>15.467690</td>\n",
       "      <td>0.133448</td>\n",
       "      <td>0.087425</td>\n",
       "      <td>0.0</td>\n",
       "      <td>318.476288</td>\n",
       "      <td>...</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "      <td>807.822815</td>\n",
       "      <td>-0.654139</td>\n",
       "      <td>25.677490</td>\n",
       "      <td>1.031786</td>\n",
       "      <td>0.031786</td>\n",
       "      <td>0.014924</td>\n",
       "      <td>0.093624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2273</th>\n",
       "      <th>227227</th>\n",
       "      <th>0</th>\n",
       "      <td>335.731934</td>\n",
       "      <td>0.259870</td>\n",
       "      <td>-3.108820</td>\n",
       "      <td>362.594666</td>\n",
       "      <td>76.227875</td>\n",
       "      <td>80.442612</td>\n",
       "      <td>0.267451</td>\n",
       "      <td>0.157171</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.688713</td>\n",
       "      <td>...</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "      <td>713.363708</td>\n",
       "      <td>0.032141</td>\n",
       "      <td>5.643250</td>\n",
       "      <td>1.007911</td>\n",
       "      <td>0.007911</td>\n",
       "      <td>-0.021656</td>\n",
       "      <td>0.138249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2275</th>\n",
       "      <th>227457</th>\n",
       "      <th>0</th>\n",
       "      <td>345.850189</td>\n",
       "      <td>-1.458291</td>\n",
       "      <td>3.099098</td>\n",
       "      <td>789.123108</td>\n",
       "      <td>65.536522</td>\n",
       "      <td>72.338219</td>\n",
       "      <td>0.233271</td>\n",
       "      <td>0.122797</td>\n",
       "      <td>0.0</td>\n",
       "      <td>299.653168</td>\n",
       "      <td>...</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "      <td>1157.675781</td>\n",
       "      <td>-0.256271</td>\n",
       "      <td>80.416260</td>\n",
       "      <td>1.069463</td>\n",
       "      <td>0.069463</td>\n",
       "      <td>-0.020461</td>\n",
       "      <td>0.146248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2276</th>\n",
       "      <th>227546</th>\n",
       "      <th>0</th>\n",
       "      <td>608.518616</td>\n",
       "      <td>-0.521754</td>\n",
       "      <td>-0.481351</td>\n",
       "      <td>700.701050</td>\n",
       "      <td>72.550140</td>\n",
       "      <td>76.958649</td>\n",
       "      <td>0.172939</td>\n",
       "      <td>0.069660</td>\n",
       "      <td>0.0</td>\n",
       "      <td>563.565247</td>\n",
       "      <td>...</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "      <td>1384.800293</td>\n",
       "      <td>-0.162342</td>\n",
       "      <td>-90.013062</td>\n",
       "      <td>0.934999</td>\n",
       "      <td>-0.065001</td>\n",
       "      <td>0.002882</td>\n",
       "      <td>0.147017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227579</th>\n",
       "      <th>0</th>\n",
       "      <td>819.906433</td>\n",
       "      <td>0.347934</td>\n",
       "      <td>1.723913</td>\n",
       "      <td>876.879761</td>\n",
       "      <td>71.808578</td>\n",
       "      <td>75.820915</td>\n",
       "      <td>0.110508</td>\n",
       "      <td>0.028728</td>\n",
       "      <td>0.0</td>\n",
       "      <td>263.500122</td>\n",
       "      <td>...</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "      <td>1843.537598</td>\n",
       "      <td>-0.179152</td>\n",
       "      <td>38.018921</td>\n",
       "      <td>1.020623</td>\n",
       "      <td>0.020623</td>\n",
       "      <td>0.000554</td>\n",
       "      <td>0.169004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2281</th>\n",
       "      <th>228042</th>\n",
       "      <th>0</th>\n",
       "      <td>764.408875</td>\n",
       "      <td>-0.631614</td>\n",
       "      <td>2.295349</td>\n",
       "      <td>931.069336</td>\n",
       "      <td>79.922424</td>\n",
       "      <td>84.708092</td>\n",
       "      <td>0.132495</td>\n",
       "      <td>0.041123</td>\n",
       "      <td>0.0</td>\n",
       "      <td>171.430862</td>\n",
       "      <td>...</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "      <td>1513.853882</td>\n",
       "      <td>-0.097639</td>\n",
       "      <td>218.413086</td>\n",
       "      <td>1.144276</td>\n",
       "      <td>0.144276</td>\n",
       "      <td>-0.009106</td>\n",
       "      <td>0.098306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2282</th>\n",
       "      <th>228117</th>\n",
       "      <th>0</th>\n",
       "      <td>321.463196</td>\n",
       "      <td>0.091370</td>\n",
       "      <td>-2.196248</td>\n",
       "      <td>340.011597</td>\n",
       "      <td>82.721222</td>\n",
       "      <td>87.231079</td>\n",
       "      <td>0.315415</td>\n",
       "      <td>0.074349</td>\n",
       "      <td>0.0</td>\n",
       "      <td>182.239685</td>\n",
       "      <td>...</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "      <td>607.001648</td>\n",
       "      <td>-0.221897</td>\n",
       "      <td>16.448364</td>\n",
       "      <td>1.027098</td>\n",
       "      <td>0.027098</td>\n",
       "      <td>0.016659</td>\n",
       "      <td>0.134892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2286</th>\n",
       "      <th>228503</th>\n",
       "      <th>0</th>\n",
       "      <td>649.005920</td>\n",
       "      <td>-0.110481</td>\n",
       "      <td>1.066668</td>\n",
       "      <td>663.828003</td>\n",
       "      <td>74.978531</td>\n",
       "      <td>79.055267</td>\n",
       "      <td>0.136293</td>\n",
       "      <td>0.045175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>253.108688</td>\n",
       "      <td>...</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "      <td>1325.282715</td>\n",
       "      <td>-0.043387</td>\n",
       "      <td>-54.555908</td>\n",
       "      <td>0.958835</td>\n",
       "      <td>-0.041165</td>\n",
       "      <td>-0.015054</td>\n",
       "      <td>0.166130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228565</th>\n",
       "      <th>0</th>\n",
       "      <td>360.785767</td>\n",
       "      <td>0.943119</td>\n",
       "      <td>-1.373860</td>\n",
       "      <td>543.946289</td>\n",
       "      <td>77.647881</td>\n",
       "      <td>82.982536</td>\n",
       "      <td>0.313472</td>\n",
       "      <td>0.088111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.222229</td>\n",
       "      <td>...</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "      <td>802.749634</td>\n",
       "      <td>0.354960</td>\n",
       "      <td>58.867126</td>\n",
       "      <td>1.073332</td>\n",
       "      <td>0.073332</td>\n",
       "      <td>-0.002891</td>\n",
       "      <td>0.118413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2289</th>\n",
       "      <th>228803</th>\n",
       "      <th>0</th>\n",
       "      <td>553.738098</td>\n",
       "      <td>-0.434034</td>\n",
       "      <td>1.881415</td>\n",
       "      <td>614.260864</td>\n",
       "      <td>44.962955</td>\n",
       "      <td>47.573677</td>\n",
       "      <td>0.107484</td>\n",
       "      <td>0.078630</td>\n",
       "      <td>0.0</td>\n",
       "      <td>226.494659</td>\n",
       "      <td>...</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "      <td>1227.886719</td>\n",
       "      <td>-0.519140</td>\n",
       "      <td>-93.609375</td>\n",
       "      <td>0.923764</td>\n",
       "      <td>-0.076236</td>\n",
       "      <td>0.137671</td>\n",
       "      <td>0.131651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2291</th>\n",
       "      <th>229042</th>\n",
       "      <th>0</th>\n",
       "      <td>226.409943</td>\n",
       "      <td>-1.545674</td>\n",
       "      <td>-2.217598</td>\n",
       "      <td>557.465759</td>\n",
       "      <td>11.176210</td>\n",
       "      <td>12.146883</td>\n",
       "      <td>0.124646</td>\n",
       "      <td>0.086952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>176.396255</td>\n",
       "      <td>...</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "      <td>643.823792</td>\n",
       "      <td>-0.473463</td>\n",
       "      <td>67.639893</td>\n",
       "      <td>1.105060</td>\n",
       "      <td>0.105060</td>\n",
       "      <td>-0.053038</td>\n",
       "      <td>0.097914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2296</th>\n",
       "      <th>229520</th>\n",
       "      <th>0</th>\n",
       "      <td>359.400543</td>\n",
       "      <td>1.004407</td>\n",
       "      <td>1.754919</td>\n",
       "      <td>563.566833</td>\n",
       "      <td>35.827625</td>\n",
       "      <td>38.593582</td>\n",
       "      <td>0.147956</td>\n",
       "      <td>0.106880</td>\n",
       "      <td>0.0</td>\n",
       "      <td>238.812363</td>\n",
       "      <td>...</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "      <td>1005.956360</td>\n",
       "      <td>-0.010036</td>\n",
       "      <td>72.123718</td>\n",
       "      <td>1.071697</td>\n",
       "      <td>0.071697</td>\n",
       "      <td>0.057480</td>\n",
       "      <td>0.148145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2297</th>\n",
       "      <th>229605</th>\n",
       "      <th>0</th>\n",
       "      <td>314.089844</td>\n",
       "      <td>-0.833665</td>\n",
       "      <td>2.804024</td>\n",
       "      <td>440.285126</td>\n",
       "      <td>83.700836</td>\n",
       "      <td>89.312263</td>\n",
       "      <td>0.238683</td>\n",
       "      <td>0.113802</td>\n",
       "      <td>0.0</td>\n",
       "      <td>114.872162</td>\n",
       "      <td>...</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "      <td>675.655457</td>\n",
       "      <td>-0.298634</td>\n",
       "      <td>35.762207</td>\n",
       "      <td>1.052930</td>\n",
       "      <td>0.052930</td>\n",
       "      <td>-0.040324</td>\n",
       "      <td>0.046095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2298</th>\n",
       "      <th>229715</th>\n",
       "      <th>0</th>\n",
       "      <td>303.292603</td>\n",
       "      <td>-0.605599</td>\n",
       "      <td>1.378168</td>\n",
       "      <td>376.307190</td>\n",
       "      <td>70.477509</td>\n",
       "      <td>74.703987</td>\n",
       "      <td>0.361345</td>\n",
       "      <td>0.191824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>140.965668</td>\n",
       "      <td>...</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "      <td>706.302795</td>\n",
       "      <td>0.137830</td>\n",
       "      <td>63.765625</td>\n",
       "      <td>1.090281</td>\n",
       "      <td>0.090281</td>\n",
       "      <td>-0.047110</td>\n",
       "      <td>0.126019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2299</th>\n",
       "      <th>229845</th>\n",
       "      <th>0</th>\n",
       "      <td>477.458221</td>\n",
       "      <td>0.370481</td>\n",
       "      <td>2.630056</td>\n",
       "      <td>518.588989</td>\n",
       "      <td>64.603653</td>\n",
       "      <td>68.314133</td>\n",
       "      <td>0.146203</td>\n",
       "      <td>0.045197</td>\n",
       "      <td>0.0</td>\n",
       "      <td>261.667572</td>\n",
       "      <td>...</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "      <td>1247.290771</td>\n",
       "      <td>-0.492779</td>\n",
       "      <td>117.186401</td>\n",
       "      <td>1.093953</td>\n",
       "      <td>0.093953</td>\n",
       "      <td>-0.023022</td>\n",
       "      <td>0.139722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229840</th>\n",
       "      <th>0</th>\n",
       "      <td>320.155548</td>\n",
       "      <td>-1.622710</td>\n",
       "      <td>1.047925</td>\n",
       "      <td>847.165649</td>\n",
       "      <td>59.823513</td>\n",
       "      <td>63.107220</td>\n",
       "      <td>0.261954</td>\n",
       "      <td>0.129281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>310.928772</td>\n",
       "      <td>...</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "      <td>1271.329590</td>\n",
       "      <td>-0.418647</td>\n",
       "      <td>-127.235962</td>\n",
       "      <td>0.899919</td>\n",
       "      <td>-0.100081</td>\n",
       "      <td>-0.030150</td>\n",
       "      <td>0.146440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2301</th>\n",
       "      <th>230059</th>\n",
       "      <th>0</th>\n",
       "      <td>441.006378</td>\n",
       "      <td>0.965674</td>\n",
       "      <td>-0.766519</td>\n",
       "      <td>669.868591</td>\n",
       "      <td>71.749168</td>\n",
       "      <td>77.266960</td>\n",
       "      <td>0.216342</td>\n",
       "      <td>0.052447</td>\n",
       "      <td>0.0</td>\n",
       "      <td>376.423645</td>\n",
       "      <td>...</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "      <td>1236.970337</td>\n",
       "      <td>-0.106248</td>\n",
       "      <td>139.882935</td>\n",
       "      <td>1.113085</td>\n",
       "      <td>0.113085</td>\n",
       "      <td>0.035250</td>\n",
       "      <td>0.161261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2302</th>\n",
       "      <th>230119</th>\n",
       "      <th>0</th>\n",
       "      <td>798.193726</td>\n",
       "      <td>0.189021</td>\n",
       "      <td>-1.469288</td>\n",
       "      <td>819.300537</td>\n",
       "      <td>69.325928</td>\n",
       "      <td>72.965050</td>\n",
       "      <td>0.066868</td>\n",
       "      <td>0.028144</td>\n",
       "      <td>0.0</td>\n",
       "      <td>371.175262</td>\n",
       "      <td>...</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "      <td>1440.446411</td>\n",
       "      <td>0.063868</td>\n",
       "      <td>140.099976</td>\n",
       "      <td>1.097262</td>\n",
       "      <td>0.097262</td>\n",
       "      <td>0.012423</td>\n",
       "      <td>0.168383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230195</th>\n",
       "      <th>0</th>\n",
       "      <td>544.700195</td>\n",
       "      <td>1.061919</td>\n",
       "      <td>1.596644</td>\n",
       "      <td>887.435242</td>\n",
       "      <td>61.842346</td>\n",
       "      <td>67.256874</td>\n",
       "      <td>0.107923</td>\n",
       "      <td>0.052825</td>\n",
       "      <td>0.0</td>\n",
       "      <td>212.969421</td>\n",
       "      <td>...</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "      <td>1613.275391</td>\n",
       "      <td>0.092361</td>\n",
       "      <td>-85.068237</td>\n",
       "      <td>0.947270</td>\n",
       "      <td>-0.052730</td>\n",
       "      <td>0.111519</td>\n",
       "      <td>0.176841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               jet0_pt  jet0_eta  jet0_phi  jet0_energy  \\\n",
       "run lumiblock event  slice                                                \n",
       "1   2266      226551 0      203.932037 -1.086163  0.536344   341.780396   \n",
       "    2270      226932 0      320.698212 -1.370161 -1.832831   676.274719   \n",
       "    2273      227227 0      335.731934  0.259870 -3.108820   362.594666   \n",
       "    2275      227457 0      345.850189 -1.458291  3.099098   789.123108   \n",
       "    2276      227546 0      608.518616 -0.521754 -0.481351   700.701050   \n",
       "              227579 0      819.906433  0.347934  1.723913   876.879761   \n",
       "    2281      228042 0      764.408875 -0.631614  2.295349   931.069336   \n",
       "    2282      228117 0      321.463196  0.091370 -2.196248   340.011597   \n",
       "    2286      228503 0      649.005920 -0.110481  1.066668   663.828003   \n",
       "              228565 0      360.785767  0.943119 -1.373860   543.946289   \n",
       "    2289      228803 0      553.738098 -0.434034  1.881415   614.260864   \n",
       "    2291      229042 0      226.409943 -1.545674 -2.217598   557.465759   \n",
       "    2296      229520 0      359.400543  1.004407  1.754919   563.566833   \n",
       "    2297      229605 0      314.089844 -0.833665  2.804024   440.285126   \n",
       "    2298      229715 0      303.292603 -0.605599  1.378168   376.307190   \n",
       "    2299      229845 0      477.458221  0.370481  2.630056   518.588989   \n",
       "              229840 0      320.155548 -1.622710  1.047925   847.165649   \n",
       "    2301      230059 0      441.006378  0.965674 -0.766519   669.868591   \n",
       "    2302      230119 0      798.193726  0.189021 -1.469288   819.300537   \n",
       "              230195 0      544.700195  1.061919  1.596644   887.435242   \n",
       "\n",
       "                            jet0_mass  jet0_corrmass  jet0_tau1  jet0_tau2  \\\n",
       "run lumiblock event  slice                                                   \n",
       "1   2266      226551 0      12.367921      13.346299   0.195710   0.150335   \n",
       "    2270      226932 0      14.117490      15.467690   0.133448   0.087425   \n",
       "    2273      227227 0      76.227875      80.442612   0.267451   0.157171   \n",
       "    2275      227457 0      65.536522      72.338219   0.233271   0.122797   \n",
       "    2276      227546 0      72.550140      76.958649   0.172939   0.069660   \n",
       "              227579 0      71.808578      75.820915   0.110508   0.028728   \n",
       "    2281      228042 0      79.922424      84.708092   0.132495   0.041123   \n",
       "    2282      228117 0      82.721222      87.231079   0.315415   0.074349   \n",
       "    2286      228503 0      74.978531      79.055267   0.136293   0.045175   \n",
       "              228565 0      77.647881      82.982536   0.313472   0.088111   \n",
       "    2289      228803 0      44.962955      47.573677   0.107484   0.078630   \n",
       "    2291      229042 0      11.176210      12.146883   0.124646   0.086952   \n",
       "    2296      229520 0      35.827625      38.593582   0.147956   0.106880   \n",
       "    2297      229605 0      83.700836      89.312263   0.238683   0.113802   \n",
       "    2298      229715 0      70.477509      74.703987   0.361345   0.191824   \n",
       "    2299      229845 0      64.603653      68.314133   0.146203   0.045197   \n",
       "              229840 0      59.823513      63.107220   0.261954   0.129281   \n",
       "    2301      230059 0      71.749168      77.266960   0.216342   0.052447   \n",
       "    2302      230119 0      69.325928      72.965050   0.066868   0.028144   \n",
       "              230195 0      61.842346      67.256874   0.107923   0.052825   \n",
       "\n",
       "                            jet0_vertexz    muon0_pt  ...   xhigh    ylow  \\\n",
       "run lumiblock event  slice                            ...                   \n",
       "1   2266      226551 0               0.0  120.215347  ...  24.334 -10.098   \n",
       "    2270      226932 0               0.0  318.476288  ...  24.334 -10.098   \n",
       "    2273      227227 0               0.0   59.688713  ...  24.334 -10.098   \n",
       "    2275      227457 0               0.0  299.653168  ...  24.334 -10.098   \n",
       "    2276      227546 0               0.0  563.565247  ...  24.334 -10.098   \n",
       "              227579 0               0.0  263.500122  ...  24.334 -10.098   \n",
       "    2281      228042 0               0.0  171.430862  ...  24.334 -10.098   \n",
       "    2282      228117 0               0.0  182.239685  ...  24.334 -10.098   \n",
       "    2286      228503 0               0.0  253.108688  ...  24.334 -10.098   \n",
       "              228565 0               0.0   82.222229  ...  24.334 -10.098   \n",
       "    2289      228803 0               0.0  226.494659  ...  24.334 -10.098   \n",
       "    2291      229042 0               0.0  176.396255  ...  24.334 -10.098   \n",
       "    2296      229520 0               0.0  238.812363  ...  24.334 -10.098   \n",
       "    2297      229605 0               0.0  114.872162  ...  24.334 -10.098   \n",
       "    2298      229715 0               0.0  140.965668  ...  24.334 -10.098   \n",
       "    2299      229845 0               0.0  261.667572  ...  24.334 -10.098   \n",
       "              229840 0               0.0  310.928772  ...  24.334 -10.098   \n",
       "    2301      230059 0               0.0  376.423645  ...  24.334 -10.098   \n",
       "    2302      230119 0               0.0  371.175262  ...  24.334 -10.098   \n",
       "              230195 0               0.0  212.969421  ...  24.334 -10.098   \n",
       "\n",
       "                            yhigh           MX        YX  diffMWW_MX  \\\n",
       "run lumiblock event  slice                                             \n",
       "1   2266      226551 0      4.298   642.360352 -0.040549   86.471313   \n",
       "    2270      226932 0      4.298   807.822815 -0.654139   25.677490   \n",
       "    2273      227227 0      4.298   713.363708  0.032141    5.643250   \n",
       "    2275      227457 0      4.298  1157.675781 -0.256271   80.416260   \n",
       "    2276      227546 0      4.298  1384.800293 -0.162342  -90.013062   \n",
       "              227579 0      4.298  1843.537598 -0.179152   38.018921   \n",
       "    2281      228042 0      4.298  1513.853882 -0.097639  218.413086   \n",
       "    2282      228117 0      4.298   607.001648 -0.221897   16.448364   \n",
       "    2286      228503 0      4.298  1325.282715 -0.043387  -54.555908   \n",
       "              228565 0      4.298   802.749634  0.354960   58.867126   \n",
       "    2289      228803 0      4.298  1227.886719 -0.519140  -93.609375   \n",
       "    2291      229042 0      4.298   643.823792 -0.473463   67.639893   \n",
       "    2296      229520 0      4.298  1005.956360 -0.010036   72.123718   \n",
       "    2297      229605 0      4.298   675.655457 -0.298634   35.762207   \n",
       "    2298      229715 0      4.298   706.302795  0.137830   63.765625   \n",
       "    2299      229845 0      4.298  1247.290771 -0.492779  117.186401   \n",
       "              229840 0      4.298  1271.329590 -0.418647 -127.235962   \n",
       "    2301      230059 0      4.298  1236.970337 -0.106248  139.882935   \n",
       "    2302      230119 0      4.298  1440.446411  0.063868  140.099976   \n",
       "              230195 0      4.298  1613.275391  0.092361  -85.068237   \n",
       "\n",
       "                            ratioMWW_MX  shiftedRatioMWW_MX  diffYWW_YX  \\\n",
       "run lumiblock event  slice                                                \n",
       "1   2266      226551 0         1.134615            0.134615    0.089981   \n",
       "    2270      226932 0         1.031786            0.031786    0.014924   \n",
       "    2273      227227 0         1.007911            0.007911   -0.021656   \n",
       "    2275      227457 0         1.069463            0.069463   -0.020461   \n",
       "    2276      227546 0         0.934999           -0.065001    0.002882   \n",
       "              227579 0         1.020623            0.020623    0.000554   \n",
       "    2281      228042 0         1.144276            0.144276   -0.009106   \n",
       "    2282      228117 0         1.027098            0.027098    0.016659   \n",
       "    2286      228503 0         0.958835           -0.041165   -0.015054   \n",
       "              228565 0         1.073332            0.073332   -0.002891   \n",
       "    2289      228803 0         0.923764           -0.076236    0.137671   \n",
       "    2291      229042 0         1.105060            0.105060   -0.053038   \n",
       "    2296      229520 0         1.071697            0.071697    0.057480   \n",
       "    2297      229605 0         1.052930            0.052930   -0.040324   \n",
       "    2298      229715 0         1.090281            0.090281   -0.047110   \n",
       "    2299      229845 0         1.093953            0.093953   -0.023022   \n",
       "              229840 0         0.899919           -0.100081   -0.030150   \n",
       "    2301      230059 0         1.113085            0.113085    0.035250   \n",
       "    2302      230119 0         1.097262            0.097262    0.012423   \n",
       "              230195 0         0.947270           -0.052730    0.111519   \n",
       "\n",
       "                            eff_all_weighted  \n",
       "run lumiblock event  slice                    \n",
       "1   2266      226551 0              0.140127  \n",
       "    2270      226932 0              0.093624  \n",
       "    2273      227227 0              0.138249  \n",
       "    2275      227457 0              0.146248  \n",
       "    2276      227546 0              0.147017  \n",
       "              227579 0              0.169004  \n",
       "    2281      228042 0              0.098306  \n",
       "    2282      228117 0              0.134892  \n",
       "    2286      228503 0              0.166130  \n",
       "              228565 0              0.118413  \n",
       "    2289      228803 0              0.131651  \n",
       "    2291      229042 0              0.097914  \n",
       "    2296      229520 0              0.148145  \n",
       "    2297      229605 0              0.046095  \n",
       "    2298      229715 0              0.126019  \n",
       "    2299      229845 0              0.139722  \n",
       "              229840 0              0.146440  \n",
       "    2301      230059 0              0.161261  \n",
       "    2302      230119 0              0.168383  \n",
       "              230195 0              0.176841  \n",
       "\n",
       "[20 rows x 45 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_protons_multiRP_signal_events[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021/02/04 10:51:36\n",
      "Total time elapsed: 4\n"
     ]
    }
   ],
   "source": [
    "resample_factor = 20\n",
    "label = \"data-random-resample_20\"\n",
    "\n",
    "fileNames_bkg = [\n",
    "    \"output-data-random-resample_20-2017B.h5\",\n",
    "    \"output-data-random-resample_20-2017C.h5\",\n",
    "    \"output-data-random-resample_20-2017D.h5\",\n",
    "    \"output-data-random-resample_20-2017E.h5\",\n",
    "    \"output-data-random-resample_20-2017F.h5\"\n",
    "]\n",
    "\n",
    "import time\n",
    "print( time.strftime(\"%Y/%m/%d %H:%M:%S\", time.localtime() ) )\n",
    "time_s_ = time.time()\n",
    "\n",
    "df_counts_bkg, df_protons_multiRP_bkg, df_protons_singleRP_bkg, df_ppstracks_bkg = 4 * [None]\n",
    "df_protons_multiRP_bkg_index, df_protons_multiRP_bkg_events, df_ppstracks_bkg_index = 3 * [None]\n",
    "\n",
    "if run_tables:\n",
    "    with pd.HDFStore( \"reduced-data-store-{}.h5\".format( label ), complevel=5 ) as store_:\n",
    "\n",
    "        df_counts_bkg, df_protons_multiRP_bkg, df_protons_singleRP_bkg, df_ppstracks_bkg = get_data( fileNames_bkg )\n",
    "        df_protons_multiRP_bkg_index, df_protons_multiRP_bkg_events, df_ppstracks_bkg_index = process_data_protons_multiRP( df_protons_multiRP_bkg, df_ppstracks_bkg, apply_fiducial=True, within_aperture=True, random_protons=True, runOnMC=False )\n",
    "\n",
    "        store_[ \"counts\" ] = df_counts_bkg\n",
    "        store_[ \"protons_multiRP\"] = df_protons_multiRP_bkg_index\n",
    "        store_[ \"events_multiRP\" ] = df_protons_multiRP_bkg_events\n",
    "\n",
    "with pd.HDFStore( \"reduced-data-store-{}.h5\".format( label ), 'r' ) as store_:\n",
    "    df_counts_bkg = store_[ \"counts\" ]\n",
    "    df_protons_multiRP_bkg_index = store_[ \"protons_multiRP\" ]\n",
    "    df_protons_multiRP_bkg_events = store_[ \"events_multiRP\" ]\n",
    "        \n",
    "time_e_ = time.time()\n",
    "print ( \"Total time elapsed: {:.0f}\".format( time_e_ - time_s_ ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>xi</th>\n",
       "      <th>thx</th>\n",
       "      <th>thy</th>\n",
       "      <th>t</th>\n",
       "      <th>ismultirp</th>\n",
       "      <th>rpid</th>\n",
       "      <th>arm</th>\n",
       "      <th>jet0_pt</th>\n",
       "      <th>jet0_eta</th>\n",
       "      <th>jet0_phi</th>\n",
       "      <th>...</th>\n",
       "      <th>trackx2</th>\n",
       "      <th>tracky2</th>\n",
       "      <th>trackpixshift2</th>\n",
       "      <th>rpid2</th>\n",
       "      <th>period</th>\n",
       "      <th>within_aperture</th>\n",
       "      <th>xlow</th>\n",
       "      <th>xhigh</th>\n",
       "      <th>ylow</th>\n",
       "      <th>yhigh</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>run</th>\n",
       "      <th>lumiblock</th>\n",
       "      <th>event</th>\n",
       "      <th>slice</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">297101</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">216</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">302550016</th>\n",
       "      <th>0</th>\n",
       "      <td>0.053462</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>-0.000089</td>\n",
       "      <td>-0.375001</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>253.968201</td>\n",
       "      <td>-0.707324</td>\n",
       "      <td>2.950644</td>\n",
       "      <td>...</td>\n",
       "      <td>5.739746</td>\n",
       "      <td>1.239502</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>2017B</td>\n",
       "      <td>True</td>\n",
       "      <td>1.995</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.108715</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-0.232221</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>253.968201</td>\n",
       "      <td>-0.707324</td>\n",
       "      <td>2.950644</td>\n",
       "      <td>...</td>\n",
       "      <td>8.485718</td>\n",
       "      <td>4.192871</td>\n",
       "      <td>0</td>\n",
       "      <td>123</td>\n",
       "      <td>2017B</td>\n",
       "      <td>True</td>\n",
       "      <td>2.422</td>\n",
       "      <td>24.620</td>\n",
       "      <td>-9.698</td>\n",
       "      <td>4.698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">297292</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">87</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">162028576</th>\n",
       "      <th>0</th>\n",
       "      <td>0.041559</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>-0.347821</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>211.335602</td>\n",
       "      <td>0.778516</td>\n",
       "      <td>-2.783820</td>\n",
       "      <td>...</td>\n",
       "      <td>4.923218</td>\n",
       "      <td>-0.440033</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>2017B</td>\n",
       "      <td>True</td>\n",
       "      <td>1.995</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.078012</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>-0.000074</td>\n",
       "      <td>-0.290854</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>211.335602</td>\n",
       "      <td>0.778516</td>\n",
       "      <td>-2.783820</td>\n",
       "      <td>...</td>\n",
       "      <td>6.192932</td>\n",
       "      <td>2.448730</td>\n",
       "      <td>0</td>\n",
       "      <td>123</td>\n",
       "      <td>2017B</td>\n",
       "      <td>True</td>\n",
       "      <td>2.422</td>\n",
       "      <td>24.620</td>\n",
       "      <td>-9.698</td>\n",
       "      <td>4.698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">297050</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">23</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">23347598</th>\n",
       "      <th>0</th>\n",
       "      <td>0.039482</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>-0.137540</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>261.334717</td>\n",
       "      <td>-1.010473</td>\n",
       "      <td>1.135294</td>\n",
       "      <td>...</td>\n",
       "      <td>3.563660</td>\n",
       "      <td>0.499451</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>2017B</td>\n",
       "      <td>True</td>\n",
       "      <td>1.995</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.070625</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>-0.692292</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>261.334717</td>\n",
       "      <td>-1.010473</td>\n",
       "      <td>1.135294</td>\n",
       "      <td>...</td>\n",
       "      <td>6.810974</td>\n",
       "      <td>-2.298340</td>\n",
       "      <td>0</td>\n",
       "      <td>123</td>\n",
       "      <td>2017B</td>\n",
       "      <td>True</td>\n",
       "      <td>2.422</td>\n",
       "      <td>24.620</td>\n",
       "      <td>-9.698</td>\n",
       "      <td>4.698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">65</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">78788008</th>\n",
       "      <th>0</th>\n",
       "      <td>0.094874</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000094</td>\n",
       "      <td>-0.349458</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>223.571411</td>\n",
       "      <td>-0.640955</td>\n",
       "      <td>-1.560675</td>\n",
       "      <td>...</td>\n",
       "      <td>10.110229</td>\n",
       "      <td>3.003418</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>2017B</td>\n",
       "      <td>True</td>\n",
       "      <td>1.995</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.088130</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.144942</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>223.571411</td>\n",
       "      <td>-0.640955</td>\n",
       "      <td>-1.560675</td>\n",
       "      <td>...</td>\n",
       "      <td>7.101318</td>\n",
       "      <td>1.888062</td>\n",
       "      <td>0</td>\n",
       "      <td>123</td>\n",
       "      <td>2017B</td>\n",
       "      <td>True</td>\n",
       "      <td>2.422</td>\n",
       "      <td>24.620</td>\n",
       "      <td>-9.698</td>\n",
       "      <td>4.698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">499</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">605989248</th>\n",
       "      <th>0</th>\n",
       "      <td>0.046020</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>-0.037710</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>247.993744</td>\n",
       "      <td>-0.750684</td>\n",
       "      <td>-1.880714</td>\n",
       "      <td>...</td>\n",
       "      <td>4.423218</td>\n",
       "      <td>-0.360596</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>2017B</td>\n",
       "      <td>True</td>\n",
       "      <td>1.995</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.044068</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>-0.000112</td>\n",
       "      <td>-0.680873</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>247.993744</td>\n",
       "      <td>-0.750684</td>\n",
       "      <td>-1.880714</td>\n",
       "      <td>...</td>\n",
       "      <td>3.565552</td>\n",
       "      <td>1.477051</td>\n",
       "      <td>0</td>\n",
       "      <td>123</td>\n",
       "      <td>2017B</td>\n",
       "      <td>True</td>\n",
       "      <td>2.422</td>\n",
       "      <td>24.620</td>\n",
       "      <td>-9.698</td>\n",
       "      <td>4.698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">297411</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">315</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">507722016</th>\n",
       "      <th>0</th>\n",
       "      <td>0.115323</td>\n",
       "      <td>-0.000139</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.740176</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>202.485367</td>\n",
       "      <td>-1.909996</td>\n",
       "      <td>-2.974470</td>\n",
       "      <td>...</td>\n",
       "      <td>7.683716</td>\n",
       "      <td>2.870117</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>2017B</td>\n",
       "      <td>True</td>\n",
       "      <td>1.995</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.084233</td>\n",
       "      <td>-0.000093</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>-0.349463</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>202.485367</td>\n",
       "      <td>-1.909996</td>\n",
       "      <td>-2.974470</td>\n",
       "      <td>...</td>\n",
       "      <td>3.641418</td>\n",
       "      <td>0.165436</td>\n",
       "      <td>0</td>\n",
       "      <td>123</td>\n",
       "      <td>2017B</td>\n",
       "      <td>True</td>\n",
       "      <td>2.422</td>\n",
       "      <td>24.620</td>\n",
       "      <td>-9.698</td>\n",
       "      <td>4.698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">377</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">608603776</th>\n",
       "      <th>0</th>\n",
       "      <td>0.036584</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000136</td>\n",
       "      <td>-0.817616</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>241.069107</td>\n",
       "      <td>-0.283513</td>\n",
       "      <td>2.425223</td>\n",
       "      <td>...</td>\n",
       "      <td>3.077118</td>\n",
       "      <td>0.926208</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>2017B</td>\n",
       "      <td>True</td>\n",
       "      <td>1.995</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.045994</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>-0.218576</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>241.069107</td>\n",
       "      <td>-0.283513</td>\n",
       "      <td>2.425223</td>\n",
       "      <td>...</td>\n",
       "      <td>3.387848</td>\n",
       "      <td>0.865784</td>\n",
       "      <td>0</td>\n",
       "      <td>123</td>\n",
       "      <td>2017B</td>\n",
       "      <td>True</td>\n",
       "      <td>2.422</td>\n",
       "      <td>24.620</td>\n",
       "      <td>-9.698</td>\n",
       "      <td>4.698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">297177</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">103</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">155861312</th>\n",
       "      <th>0</th>\n",
       "      <td>0.098408</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>-0.094515</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>205.575821</td>\n",
       "      <td>1.169397</td>\n",
       "      <td>-2.365454</td>\n",
       "      <td>...</td>\n",
       "      <td>10.905273</td>\n",
       "      <td>2.309814</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>2017B</td>\n",
       "      <td>True</td>\n",
       "      <td>1.995</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.041949</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>-0.036352</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>205.575821</td>\n",
       "      <td>1.169397</td>\n",
       "      <td>-2.365454</td>\n",
       "      <td>...</td>\n",
       "      <td>2.877838</td>\n",
       "      <td>-0.229462</td>\n",
       "      <td>0</td>\n",
       "      <td>123</td>\n",
       "      <td>2017B</td>\n",
       "      <td>True</td>\n",
       "      <td>2.422</td>\n",
       "      <td>24.620</td>\n",
       "      <td>-9.698</td>\n",
       "      <td>4.698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">112</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">168879968</th>\n",
       "      <th>0</th>\n",
       "      <td>0.031520</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.000105</td>\n",
       "      <td>-0.458029</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>249.287399</td>\n",
       "      <td>-0.463456</td>\n",
       "      <td>0.465937</td>\n",
       "      <td>...</td>\n",
       "      <td>3.524567</td>\n",
       "      <td>0.535889</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>2017B</td>\n",
       "      <td>True</td>\n",
       "      <td>1.995</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.082399</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.044631</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>249.287399</td>\n",
       "      <td>-0.463456</td>\n",
       "      <td>0.465937</td>\n",
       "      <td>...</td>\n",
       "      <td>7.332764</td>\n",
       "      <td>0.769836</td>\n",
       "      <td>0</td>\n",
       "      <td>123</td>\n",
       "      <td>2017B</td>\n",
       "      <td>True</td>\n",
       "      <td>2.422</td>\n",
       "      <td>24.620</td>\n",
       "      <td>-9.698</td>\n",
       "      <td>4.698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">297057</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">359</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">451236608</th>\n",
       "      <th>0</th>\n",
       "      <td>0.106113</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>-0.282647</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>394.285278</td>\n",
       "      <td>0.398574</td>\n",
       "      <td>-1.529445</td>\n",
       "      <td>...</td>\n",
       "      <td>12.475220</td>\n",
       "      <td>-0.932312</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>2017B</td>\n",
       "      <td>True</td>\n",
       "      <td>1.995</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.121658</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>-0.137009</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>394.285278</td>\n",
       "      <td>0.398574</td>\n",
       "      <td>-1.529445</td>\n",
       "      <td>...</td>\n",
       "      <td>7.340759</td>\n",
       "      <td>-0.935608</td>\n",
       "      <td>0</td>\n",
       "      <td>123</td>\n",
       "      <td>2017B</td>\n",
       "      <td>True</td>\n",
       "      <td>2.422</td>\n",
       "      <td>24.620</td>\n",
       "      <td>-9.698</td>\n",
       "      <td>4.698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        xi       thx       thy         t  \\\n",
       "run    lumiblock event     slice                                           \n",
       "297101 216       302550016 0      0.053462  0.000038 -0.000089 -0.375001   \n",
       "                           0      0.108715  0.000028 -0.000071 -0.232221   \n",
       "297292 87        162028576 0      0.041559  0.000090  0.000020 -0.347821   \n",
       "                           0      0.078012  0.000043 -0.000074 -0.290854   \n",
       "297050 23        23347598  0      0.039482 -0.000016 -0.000056 -0.137540   \n",
       "                           0      0.070625  0.000103  0.000083 -0.692292   \n",
       "       65        78788008  0      0.094874 -0.000006 -0.000094 -0.349458   \n",
       "                           0      0.088130  0.000026 -0.000054 -0.144942   \n",
       "       499       605989248 0      0.046020  0.000002  0.000030 -0.037710   \n",
       "                           0      0.044068  0.000065 -0.000112 -0.680873   \n",
       "297411 315       507722016 0      0.115323 -0.000139 -0.000009 -0.740176   \n",
       "                           0      0.084233 -0.000093  0.000012 -0.349463   \n",
       "       377       608603776 0      0.036584 -0.000039 -0.000136 -0.817616   \n",
       "                           0      0.045994  0.000036 -0.000064 -0.218576   \n",
       "297177 103       155861312 0      0.098408 -0.000010 -0.000046 -0.094515   \n",
       "                           0      0.041949 -0.000018  0.000023 -0.036352   \n",
       "       112       168879968 0      0.031520  0.000010 -0.000105 -0.458029   \n",
       "                           0      0.082399  0.000013 -0.000029 -0.044631   \n",
       "297057 359       451236608 0      0.106113  0.000021  0.000082 -0.282647   \n",
       "                           0      0.121658 -0.000046  0.000034 -0.137009   \n",
       "\n",
       "                                  ismultirp  rpid  arm     jet0_pt  jet0_eta  \\\n",
       "run    lumiblock event     slice                                               \n",
       "297101 216       302550016 0              1   123    0  253.968201 -0.707324   \n",
       "                           0              1   123    1  253.968201 -0.707324   \n",
       "297292 87        162028576 0              1   123    0  211.335602  0.778516   \n",
       "                           0              1   123    1  211.335602  0.778516   \n",
       "297050 23        23347598  0              1   123    0  261.334717 -1.010473   \n",
       "                           0              1   123    1  261.334717 -1.010473   \n",
       "       65        78788008  0              1   123    0  223.571411 -0.640955   \n",
       "                           0              1   123    1  223.571411 -0.640955   \n",
       "       499       605989248 0              1   123    0  247.993744 -0.750684   \n",
       "                           0              1   123    1  247.993744 -0.750684   \n",
       "297411 315       507722016 0              1   123    0  202.485367 -1.909996   \n",
       "                           0              1   123    1  202.485367 -1.909996   \n",
       "       377       608603776 0              1   123    0  241.069107 -0.283513   \n",
       "                           0              1   123    1  241.069107 -0.283513   \n",
       "297177 103       155861312 0              1   123    0  205.575821  1.169397   \n",
       "                           0              1   123    1  205.575821  1.169397   \n",
       "       112       168879968 0              1   123    0  249.287399 -0.463456   \n",
       "                           0              1   123    1  249.287399 -0.463456   \n",
       "297057 359       451236608 0              1   123    0  394.285278  0.398574   \n",
       "                           0              1   123    1  394.285278  0.398574   \n",
       "\n",
       "                                  jet0_phi  ...    trackx2   tracky2  \\\n",
       "run    lumiblock event     slice            ...                        \n",
       "297101 216       302550016 0      2.950644  ...   5.739746  1.239502   \n",
       "                           0      2.950644  ...   8.485718  4.192871   \n",
       "297292 87        162028576 0     -2.783820  ...   4.923218 -0.440033   \n",
       "                           0     -2.783820  ...   6.192932  2.448730   \n",
       "297050 23        23347598  0      1.135294  ...   3.563660  0.499451   \n",
       "                           0      1.135294  ...   6.810974 -2.298340   \n",
       "       65        78788008  0     -1.560675  ...  10.110229  3.003418   \n",
       "                           0     -1.560675  ...   7.101318  1.888062   \n",
       "       499       605989248 0     -1.880714  ...   4.423218 -0.360596   \n",
       "                           0     -1.880714  ...   3.565552  1.477051   \n",
       "297411 315       507722016 0     -2.974470  ...   7.683716  2.870117   \n",
       "                           0     -2.974470  ...   3.641418  0.165436   \n",
       "       377       608603776 0      2.425223  ...   3.077118  0.926208   \n",
       "                           0      2.425223  ...   3.387848  0.865784   \n",
       "297177 103       155861312 0     -2.365454  ...  10.905273  2.309814   \n",
       "                           0     -2.365454  ...   2.877838 -0.229462   \n",
       "       112       168879968 0      0.465937  ...   3.524567  0.535889   \n",
       "                           0      0.465937  ...   7.332764  0.769836   \n",
       "297057 359       451236608 0     -1.529445  ...  12.475220 -0.932312   \n",
       "                           0     -1.529445  ...   7.340759 -0.935608   \n",
       "\n",
       "                                  trackpixshift2  rpid2  period  \\\n",
       "run    lumiblock event     slice                                  \n",
       "297101 216       302550016 0                   0     23   2017B   \n",
       "                           0                   0    123   2017B   \n",
       "297292 87        162028576 0                   0     23   2017B   \n",
       "                           0                   0    123   2017B   \n",
       "297050 23        23347598  0                   0     23   2017B   \n",
       "                           0                   0    123   2017B   \n",
       "       65        78788008  0                   0     23   2017B   \n",
       "                           0                   0    123   2017B   \n",
       "       499       605989248 0                   0     23   2017B   \n",
       "                           0                   0    123   2017B   \n",
       "297411 315       507722016 0                   0     23   2017B   \n",
       "                           0                   0    123   2017B   \n",
       "       377       608603776 0                   0     23   2017B   \n",
       "                           0                   0    123   2017B   \n",
       "297177 103       155861312 0                   0     23   2017B   \n",
       "                           0                   0    123   2017B   \n",
       "       112       168879968 0                   0     23   2017B   \n",
       "                           0                   0    123   2017B   \n",
       "297057 359       451236608 0                   0     23   2017B   \n",
       "                           0                   0    123   2017B   \n",
       "\n",
       "                                  within_aperture   xlow   xhigh    ylow  \\\n",
       "run    lumiblock event     slice                                           \n",
       "297101 216       302550016 0                 True  1.995  24.334 -10.098   \n",
       "                           0                 True  2.422  24.620  -9.698   \n",
       "297292 87        162028576 0                 True  1.995  24.334 -10.098   \n",
       "                           0                 True  2.422  24.620  -9.698   \n",
       "297050 23        23347598  0                 True  1.995  24.334 -10.098   \n",
       "                           0                 True  2.422  24.620  -9.698   \n",
       "       65        78788008  0                 True  1.995  24.334 -10.098   \n",
       "                           0                 True  2.422  24.620  -9.698   \n",
       "       499       605989248 0                 True  1.995  24.334 -10.098   \n",
       "                           0                 True  2.422  24.620  -9.698   \n",
       "297411 315       507722016 0                 True  1.995  24.334 -10.098   \n",
       "                           0                 True  2.422  24.620  -9.698   \n",
       "       377       608603776 0                 True  1.995  24.334 -10.098   \n",
       "                           0                 True  2.422  24.620  -9.698   \n",
       "297177 103       155861312 0                 True  1.995  24.334 -10.098   \n",
       "                           0                 True  2.422  24.620  -9.698   \n",
       "       112       168879968 0                 True  1.995  24.334 -10.098   \n",
       "                           0                 True  2.422  24.620  -9.698   \n",
       "297057 359       451236608 0                 True  1.995  24.334 -10.098   \n",
       "                           0                 True  2.422  24.620  -9.698   \n",
       "\n",
       "                                  yhigh  \n",
       "run    lumiblock event     slice         \n",
       "297101 216       302550016 0      4.298  \n",
       "                           0      4.698  \n",
       "297292 87        162028576 0      4.298  \n",
       "                           0      4.698  \n",
       "297050 23        23347598  0      4.298  \n",
       "                           0      4.698  \n",
       "       65        78788008  0      4.298  \n",
       "                           0      4.698  \n",
       "       499       605989248 0      4.298  \n",
       "                           0      4.698  \n",
       "297411 315       507722016 0      4.298  \n",
       "                           0      4.698  \n",
       "       377       608603776 0      4.298  \n",
       "                           0      4.698  \n",
       "297177 103       155861312 0      4.298  \n",
       "                           0      4.698  \n",
       "       112       168879968 0      4.298  \n",
       "                           0      4.698  \n",
       "297057 359       451236608 0      4.298  \n",
       "                           0      4.698  \n",
       "\n",
       "[20 rows x 59 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_protons_multiRP_bkg_index[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>jet0_pt</th>\n",
       "      <th>jet0_eta</th>\n",
       "      <th>jet0_phi</th>\n",
       "      <th>jet0_energy</th>\n",
       "      <th>jet0_mass</th>\n",
       "      <th>jet0_corrmass</th>\n",
       "      <th>jet0_tau1</th>\n",
       "      <th>jet0_tau2</th>\n",
       "      <th>jet0_vertexz</th>\n",
       "      <th>muon0_pt</th>\n",
       "      <th>...</th>\n",
       "      <th>xlow</th>\n",
       "      <th>xhigh</th>\n",
       "      <th>ylow</th>\n",
       "      <th>yhigh</th>\n",
       "      <th>MX</th>\n",
       "      <th>YX</th>\n",
       "      <th>diffMWW_MX</th>\n",
       "      <th>ratioMWW_MX</th>\n",
       "      <th>shiftedRatioMWW_MX</th>\n",
       "      <th>diffYWW_YX</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>run</th>\n",
       "      <th>lumiblock</th>\n",
       "      <th>event</th>\n",
       "      <th>slice</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>297101</th>\n",
       "      <th>216</th>\n",
       "      <th>302550016</th>\n",
       "      <th>0</th>\n",
       "      <td>253.968201</td>\n",
       "      <td>-0.707324</td>\n",
       "      <td>2.950644</td>\n",
       "      <td>333.493073</td>\n",
       "      <td>79.504433</td>\n",
       "      <td>85.812309</td>\n",
       "      <td>0.362731</td>\n",
       "      <td>0.066945</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.329308</td>\n",
       "      <td>...</td>\n",
       "      <td>1.995</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "      <td>991.082153</td>\n",
       "      <td>-0.354882</td>\n",
       "      <td>-539.447571</td>\n",
       "      <td>0.455698</td>\n",
       "      <td>-0.544302</td>\n",
       "      <td>-0.057944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297292</th>\n",
       "      <th>87</th>\n",
       "      <th>162028576</th>\n",
       "      <th>0</th>\n",
       "      <td>211.335602</td>\n",
       "      <td>0.778516</td>\n",
       "      <td>-2.783820</td>\n",
       "      <td>282.520508</td>\n",
       "      <td>32.382271</td>\n",
       "      <td>34.919926</td>\n",
       "      <td>0.129498</td>\n",
       "      <td>0.093096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>118.123466</td>\n",
       "      <td>...</td>\n",
       "      <td>1.995</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "      <td>740.213928</td>\n",
       "      <td>-0.314870</td>\n",
       "      <td>-198.417114</td>\n",
       "      <td>0.731946</td>\n",
       "      <td>-0.268054</td>\n",
       "      <td>0.441119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">297050</th>\n",
       "      <th>23</th>\n",
       "      <th>23347598</th>\n",
       "      <th>0</th>\n",
       "      <td>261.334717</td>\n",
       "      <td>-1.010473</td>\n",
       "      <td>1.135294</td>\n",
       "      <td>412.250397</td>\n",
       "      <td>26.409565</td>\n",
       "      <td>28.783287</td>\n",
       "      <td>0.201263</td>\n",
       "      <td>0.162387</td>\n",
       "      <td>0.0</td>\n",
       "      <td>114.449654</td>\n",
       "      <td>...</td>\n",
       "      <td>1.995</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "      <td>686.472717</td>\n",
       "      <td>-0.290764</td>\n",
       "      <td>-130.869202</td>\n",
       "      <td>0.809360</td>\n",
       "      <td>-0.190640</td>\n",
       "      <td>-0.851664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <th>78788008</th>\n",
       "      <th>0</th>\n",
       "      <td>223.571411</td>\n",
       "      <td>-0.640955</td>\n",
       "      <td>-1.560675</td>\n",
       "      <td>276.403442</td>\n",
       "      <td>11.500848</td>\n",
       "      <td>12.381974</td>\n",
       "      <td>0.134743</td>\n",
       "      <td>0.108764</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.997532</td>\n",
       "      <td>...</td>\n",
       "      <td>1.995</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "      <td>1188.718994</td>\n",
       "      <td>0.036870</td>\n",
       "      <td>-732.761475</td>\n",
       "      <td>0.383570</td>\n",
       "      <td>-0.616430</td>\n",
       "      <td>-0.824225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <th>605989248</th>\n",
       "      <th>0</th>\n",
       "      <td>247.993744</td>\n",
       "      <td>-0.750684</td>\n",
       "      <td>-1.880714</td>\n",
       "      <td>327.492065</td>\n",
       "      <td>37.604755</td>\n",
       "      <td>40.593559</td>\n",
       "      <td>0.186229</td>\n",
       "      <td>0.143760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.856094</td>\n",
       "      <td>...</td>\n",
       "      <td>1.995</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "      <td>585.438477</td>\n",
       "      <td>0.021673</td>\n",
       "      <td>-10.685974</td>\n",
       "      <td>0.981747</td>\n",
       "      <td>-0.018253</td>\n",
       "      <td>-0.049548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">297411</th>\n",
       "      <th>315</th>\n",
       "      <th>507722016</th>\n",
       "      <th>0</th>\n",
       "      <td>202.485367</td>\n",
       "      <td>-1.909996</td>\n",
       "      <td>-2.974470</td>\n",
       "      <td>701.777527</td>\n",
       "      <td>37.559608</td>\n",
       "      <td>42.359814</td>\n",
       "      <td>0.238960</td>\n",
       "      <td>0.180407</td>\n",
       "      <td>0.0</td>\n",
       "      <td>145.918137</td>\n",
       "      <td>...</td>\n",
       "      <td>1.995</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "      <td>1281.273071</td>\n",
       "      <td>0.157078</td>\n",
       "      <td>-894.971375</td>\n",
       "      <td>0.301498</td>\n",
       "      <td>-0.698502</td>\n",
       "      <td>-2.032602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <th>608603776</th>\n",
       "      <th>0</th>\n",
       "      <td>241.069107</td>\n",
       "      <td>-0.283513</td>\n",
       "      <td>2.425223</td>\n",
       "      <td>259.671875</td>\n",
       "      <td>16.288521</td>\n",
       "      <td>17.486908</td>\n",
       "      <td>0.193646</td>\n",
       "      <td>0.163281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.912529</td>\n",
       "      <td>...</td>\n",
       "      <td>1.995</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "      <td>533.259521</td>\n",
       "      <td>-0.114459</td>\n",
       "      <td>-5.137268</td>\n",
       "      <td>0.990366</td>\n",
       "      <td>-0.009634</td>\n",
       "      <td>-0.776025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">297177</th>\n",
       "      <th>103</th>\n",
       "      <th>155861312</th>\n",
       "      <th>0</th>\n",
       "      <td>205.575821</td>\n",
       "      <td>1.169397</td>\n",
       "      <td>-2.365454</td>\n",
       "      <td>367.820312</td>\n",
       "      <td>34.211136</td>\n",
       "      <td>38.303947</td>\n",
       "      <td>0.272831</td>\n",
       "      <td>0.143435</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.736053</td>\n",
       "      <td>...</td>\n",
       "      <td>1.995</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "      <td>835.258789</td>\n",
       "      <td>0.426335</td>\n",
       "      <td>-464.775391</td>\n",
       "      <td>0.443555</td>\n",
       "      <td>-0.556445</td>\n",
       "      <td>0.356864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <th>168879968</th>\n",
       "      <th>0</th>\n",
       "      <td>249.287399</td>\n",
       "      <td>-0.463456</td>\n",
       "      <td>0.465937</td>\n",
       "      <td>284.730316</td>\n",
       "      <td>20.713552</td>\n",
       "      <td>22.339268</td>\n",
       "      <td>0.206549</td>\n",
       "      <td>0.140903</td>\n",
       "      <td>0.0</td>\n",
       "      <td>184.405487</td>\n",
       "      <td>...</td>\n",
       "      <td>1.995</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "      <td>662.516785</td>\n",
       "      <td>-0.480477</td>\n",
       "      <td>-61.676575</td>\n",
       "      <td>0.906906</td>\n",
       "      <td>-0.093094</td>\n",
       "      <td>-0.452691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297057</th>\n",
       "      <th>359</th>\n",
       "      <th>451236608</th>\n",
       "      <th>0</th>\n",
       "      <td>394.285278</td>\n",
       "      <td>0.398574</td>\n",
       "      <td>-1.529445</td>\n",
       "      <td>453.403381</td>\n",
       "      <td>138.037384</td>\n",
       "      <td>148.165756</td>\n",
       "      <td>0.378855</td>\n",
       "      <td>0.201349</td>\n",
       "      <td>0.0</td>\n",
       "      <td>146.408371</td>\n",
       "      <td>...</td>\n",
       "      <td>1.995</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "      <td>1477.063232</td>\n",
       "      <td>-0.068353</td>\n",
       "      <td>-562.615295</td>\n",
       "      <td>0.619099</td>\n",
       "      <td>-0.380901</td>\n",
       "      <td>-0.027711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">297219</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">2173</th>\n",
       "      <th>3099827968</th>\n",
       "      <th>0</th>\n",
       "      <td>288.619812</td>\n",
       "      <td>0.455739</td>\n",
       "      <td>0.454244</td>\n",
       "      <td>328.551727</td>\n",
       "      <td>69.199577</td>\n",
       "      <td>74.361023</td>\n",
       "      <td>0.192689</td>\n",
       "      <td>0.060436</td>\n",
       "      <td>0.0</td>\n",
       "      <td>116.402565</td>\n",
       "      <td>...</td>\n",
       "      <td>1.995</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "      <td>1283.559326</td>\n",
       "      <td>0.109039</td>\n",
       "      <td>-653.152527</td>\n",
       "      <td>0.491140</td>\n",
       "      <td>-0.508860</td>\n",
       "      <td>0.765708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3099778304</th>\n",
       "      <th>0</th>\n",
       "      <td>375.724274</td>\n",
       "      <td>0.520944</td>\n",
       "      <td>-0.466679</td>\n",
       "      <td>431.490173</td>\n",
       "      <td>6.786267</td>\n",
       "      <td>7.290971</td>\n",
       "      <td>0.061769</td>\n",
       "      <td>0.052032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>116.836121</td>\n",
       "      <td>...</td>\n",
       "      <td>1.995</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "      <td>1190.692749</td>\n",
       "      <td>-0.067335</td>\n",
       "      <td>-425.383972</td>\n",
       "      <td>0.642742</td>\n",
       "      <td>-0.357258</td>\n",
       "      <td>0.714580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3100205568</th>\n",
       "      <th>0</th>\n",
       "      <td>247.402695</td>\n",
       "      <td>-1.569887</td>\n",
       "      <td>0.242396</td>\n",
       "      <td>622.208618</td>\n",
       "      <td>11.449527</td>\n",
       "      <td>12.662539</td>\n",
       "      <td>0.129074</td>\n",
       "      <td>0.105484</td>\n",
       "      <td>0.0</td>\n",
       "      <td>191.388321</td>\n",
       "      <td>...</td>\n",
       "      <td>1.995</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "      <td>857.619751</td>\n",
       "      <td>-0.030244</td>\n",
       "      <td>-326.385620</td>\n",
       "      <td>0.619429</td>\n",
       "      <td>-0.380571</td>\n",
       "      <td>-1.324861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297425</th>\n",
       "      <th>58</th>\n",
       "      <th>92710800</th>\n",
       "      <th>0</th>\n",
       "      <td>456.756805</td>\n",
       "      <td>-0.825246</td>\n",
       "      <td>1.224345</td>\n",
       "      <td>627.350525</td>\n",
       "      <td>17.468006</td>\n",
       "      <td>18.844810</td>\n",
       "      <td>0.106869</td>\n",
       "      <td>0.081346</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74.711189</td>\n",
       "      <td>...</td>\n",
       "      <td>1.995</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "      <td>1039.189087</td>\n",
       "      <td>-0.384506</td>\n",
       "      <td>-378.624207</td>\n",
       "      <td>0.635654</td>\n",
       "      <td>-0.364346</td>\n",
       "      <td>-0.130105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">297050</th>\n",
       "      <th>640</th>\n",
       "      <th>759692672</th>\n",
       "      <th>0</th>\n",
       "      <td>356.842926</td>\n",
       "      <td>-1.857062</td>\n",
       "      <td>1.034243</td>\n",
       "      <td>1173.785034</td>\n",
       "      <td>22.226070</td>\n",
       "      <td>24.627333</td>\n",
       "      <td>0.151161</td>\n",
       "      <td>0.117192</td>\n",
       "      <td>0.0</td>\n",
       "      <td>323.216003</td>\n",
       "      <td>...</td>\n",
       "      <td>1.995</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "      <td>733.031860</td>\n",
       "      <td>0.553902</td>\n",
       "      <td>159.882874</td>\n",
       "      <td>1.218112</td>\n",
       "      <td>0.218112</td>\n",
       "      <td>-1.763480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <th>778318912</th>\n",
       "      <th>0</th>\n",
       "      <td>323.938354</td>\n",
       "      <td>-2.283609</td>\n",
       "      <td>2.840562</td>\n",
       "      <td>1607.517578</td>\n",
       "      <td>16.241558</td>\n",
       "      <td>18.127111</td>\n",
       "      <td>0.124860</td>\n",
       "      <td>0.098927</td>\n",
       "      <td>0.0</td>\n",
       "      <td>118.342453</td>\n",
       "      <td>...</td>\n",
       "      <td>1.995</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "      <td>897.606995</td>\n",
       "      <td>0.386342</td>\n",
       "      <td>-55.687317</td>\n",
       "      <td>0.937960</td>\n",
       "      <td>-0.062040</td>\n",
       "      <td>-1.731446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">633</th>\n",
       "      <th>752294720</th>\n",
       "      <th>0</th>\n",
       "      <td>281.741394</td>\n",
       "      <td>1.049507</td>\n",
       "      <td>3.116731</td>\n",
       "      <td>461.690247</td>\n",
       "      <td>82.215797</td>\n",
       "      <td>90.832924</td>\n",
       "      <td>0.317704</td>\n",
       "      <td>0.144190</td>\n",
       "      <td>0.0</td>\n",
       "      <td>136.802490</td>\n",
       "      <td>...</td>\n",
       "      <td>1.995</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "      <td>834.609375</td>\n",
       "      <td>-0.459594</td>\n",
       "      <td>-289.815002</td>\n",
       "      <td>0.652754</td>\n",
       "      <td>-0.347246</td>\n",
       "      <td>1.814427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752405312</th>\n",
       "      <th>0</th>\n",
       "      <td>203.592255</td>\n",
       "      <td>-1.403587</td>\n",
       "      <td>1.493702</td>\n",
       "      <td>447.180359</td>\n",
       "      <td>62.394794</td>\n",
       "      <td>70.382645</td>\n",
       "      <td>0.424542</td>\n",
       "      <td>0.222673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76.544220</td>\n",
       "      <td>...</td>\n",
       "      <td>1.995</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "      <td>832.814880</td>\n",
       "      <td>-0.137661</td>\n",
       "      <td>-285.878296</td>\n",
       "      <td>0.656732</td>\n",
       "      <td>-0.343268</td>\n",
       "      <td>-0.518805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <th>754300416</th>\n",
       "      <th>0</th>\n",
       "      <td>278.962280</td>\n",
       "      <td>2.154710</td>\n",
       "      <td>-2.295970</td>\n",
       "      <td>1224.405396</td>\n",
       "      <td>78.443649</td>\n",
       "      <td>88.524498</td>\n",
       "      <td>0.450680</td>\n",
       "      <td>0.133851</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.994087</td>\n",
       "      <td>...</td>\n",
       "      <td>1.995</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "      <td>830.026733</td>\n",
       "      <td>-0.218986</td>\n",
       "      <td>-442.425659</td>\n",
       "      <td>0.466974</td>\n",
       "      <td>-0.533026</td>\n",
       "      <td>1.997271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <th>753442816</th>\n",
       "      <th>0</th>\n",
       "      <td>460.727081</td>\n",
       "      <td>1.591670</td>\n",
       "      <td>1.678488</td>\n",
       "      <td>1189.134033</td>\n",
       "      <td>133.705704</td>\n",
       "      <td>146.878067</td>\n",
       "      <td>0.386564</td>\n",
       "      <td>0.159941</td>\n",
       "      <td>0.0</td>\n",
       "      <td>251.566620</td>\n",
       "      <td>...</td>\n",
       "      <td>1.995</td>\n",
       "      <td>24.334</td>\n",
       "      <td>-10.098</td>\n",
       "      <td>4.298</td>\n",
       "      <td>883.677795</td>\n",
       "      <td>0.323264</td>\n",
       "      <td>88.513611</td>\n",
       "      <td>1.100165</td>\n",
       "      <td>0.100165</td>\n",
       "      <td>0.612963</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      jet0_pt  jet0_eta  jet0_phi  \\\n",
       "run    lumiblock event      slice                                   \n",
       "297101 216       302550016  0      253.968201 -0.707324  2.950644   \n",
       "297292 87        162028576  0      211.335602  0.778516 -2.783820   \n",
       "297050 23        23347598   0      261.334717 -1.010473  1.135294   \n",
       "       65        78788008   0      223.571411 -0.640955 -1.560675   \n",
       "       499       605989248  0      247.993744 -0.750684 -1.880714   \n",
       "297411 315       507722016  0      202.485367 -1.909996 -2.974470   \n",
       "       377       608603776  0      241.069107 -0.283513  2.425223   \n",
       "297177 103       155861312  0      205.575821  1.169397 -2.365454   \n",
       "       112       168879968  0      249.287399 -0.463456  0.465937   \n",
       "297057 359       451236608  0      394.285278  0.398574 -1.529445   \n",
       "297219 2173      3099827968 0      288.619812  0.455739  0.454244   \n",
       "                 3099778304 0      375.724274  0.520944 -0.466679   \n",
       "                 3100205568 0      247.402695 -1.569887  0.242396   \n",
       "297425 58        92710800   0      456.756805 -0.825246  1.224345   \n",
       "297050 640       759692672  0      356.842926 -1.857062  1.034243   \n",
       "       658       778318912  0      323.938354 -2.283609  2.840562   \n",
       "       633       752294720  0      281.741394  1.049507  3.116731   \n",
       "                 752405312  0      203.592255 -1.403587  1.493702   \n",
       "       635       754300416  0      278.962280  2.154710 -2.295970   \n",
       "       634       753442816  0      460.727081  1.591670  1.678488   \n",
       "\n",
       "                                   jet0_energy   jet0_mass  jet0_corrmass  \\\n",
       "run    lumiblock event      slice                                           \n",
       "297101 216       302550016  0       333.493073   79.504433      85.812309   \n",
       "297292 87        162028576  0       282.520508   32.382271      34.919926   \n",
       "297050 23        23347598   0       412.250397   26.409565      28.783287   \n",
       "       65        78788008   0       276.403442   11.500848      12.381974   \n",
       "       499       605989248  0       327.492065   37.604755      40.593559   \n",
       "297411 315       507722016  0       701.777527   37.559608      42.359814   \n",
       "       377       608603776  0       259.671875   16.288521      17.486908   \n",
       "297177 103       155861312  0       367.820312   34.211136      38.303947   \n",
       "       112       168879968  0       284.730316   20.713552      22.339268   \n",
       "297057 359       451236608  0       453.403381  138.037384     148.165756   \n",
       "297219 2173      3099827968 0       328.551727   69.199577      74.361023   \n",
       "                 3099778304 0       431.490173    6.786267       7.290971   \n",
       "                 3100205568 0       622.208618   11.449527      12.662539   \n",
       "297425 58        92710800   0       627.350525   17.468006      18.844810   \n",
       "297050 640       759692672  0      1173.785034   22.226070      24.627333   \n",
       "       658       778318912  0      1607.517578   16.241558      18.127111   \n",
       "       633       752294720  0       461.690247   82.215797      90.832924   \n",
       "                 752405312  0       447.180359   62.394794      70.382645   \n",
       "       635       754300416  0      1224.405396   78.443649      88.524498   \n",
       "       634       753442816  0      1189.134033  133.705704     146.878067   \n",
       "\n",
       "                                   jet0_tau1  jet0_tau2  jet0_vertexz  \\\n",
       "run    lumiblock event      slice                                       \n",
       "297101 216       302550016  0       0.362731   0.066945           0.0   \n",
       "297292 87        162028576  0       0.129498   0.093096           0.0   \n",
       "297050 23        23347598   0       0.201263   0.162387           0.0   \n",
       "       65        78788008   0       0.134743   0.108764           0.0   \n",
       "       499       605989248  0       0.186229   0.143760           0.0   \n",
       "297411 315       507722016  0       0.238960   0.180407           0.0   \n",
       "       377       608603776  0       0.193646   0.163281           0.0   \n",
       "297177 103       155861312  0       0.272831   0.143435           0.0   \n",
       "       112       168879968  0       0.206549   0.140903           0.0   \n",
       "297057 359       451236608  0       0.378855   0.201349           0.0   \n",
       "297219 2173      3099827968 0       0.192689   0.060436           0.0   \n",
       "                 3099778304 0       0.061769   0.052032           0.0   \n",
       "                 3100205568 0       0.129074   0.105484           0.0   \n",
       "297425 58        92710800   0       0.106869   0.081346           0.0   \n",
       "297050 640       759692672  0       0.151161   0.117192           0.0   \n",
       "       658       778318912  0       0.124860   0.098927           0.0   \n",
       "       633       752294720  0       0.317704   0.144190           0.0   \n",
       "                 752405312  0       0.424542   0.222673           0.0   \n",
       "       635       754300416  0       0.450680   0.133851           0.0   \n",
       "       634       753442816  0       0.386564   0.159941           0.0   \n",
       "\n",
       "                                     muon0_pt  ...   xlow   xhigh    ylow  \\\n",
       "run    lumiblock event      slice              ...                          \n",
       "297101 216       302550016  0       89.329308  ...  1.995  24.334 -10.098   \n",
       "297292 87        162028576  0      118.123466  ...  1.995  24.334 -10.098   \n",
       "297050 23        23347598   0      114.449654  ...  1.995  24.334 -10.098   \n",
       "       65        78788008   0       50.997532  ...  1.995  24.334 -10.098   \n",
       "       499       605989248  0       80.856094  ...  1.995  24.334 -10.098   \n",
       "297411 315       507722016  0      145.918137  ...  1.995  24.334 -10.098   \n",
       "       377       608603776  0       78.912529  ...  1.995  24.334 -10.098   \n",
       "297177 103       155861312  0      102.736053  ...  1.995  24.334 -10.098   \n",
       "       112       168879968  0      184.405487  ...  1.995  24.334 -10.098   \n",
       "297057 359       451236608  0      146.408371  ...  1.995  24.334 -10.098   \n",
       "297219 2173      3099827968 0      116.402565  ...  1.995  24.334 -10.098   \n",
       "                 3099778304 0      116.836121  ...  1.995  24.334 -10.098   \n",
       "                 3100205568 0      191.388321  ...  1.995  24.334 -10.098   \n",
       "297425 58        92710800   0       74.711189  ...  1.995  24.334 -10.098   \n",
       "297050 640       759692672  0      323.216003  ...  1.995  24.334 -10.098   \n",
       "       658       778318912  0      118.342453  ...  1.995  24.334 -10.098   \n",
       "       633       752294720  0      136.802490  ...  1.995  24.334 -10.098   \n",
       "                 752405312  0       76.544220  ...  1.995  24.334 -10.098   \n",
       "       635       754300416  0       55.994087  ...  1.995  24.334 -10.098   \n",
       "       634       753442816  0      251.566620  ...  1.995  24.334 -10.098   \n",
       "\n",
       "                                   yhigh           MX        YX  diffMWW_MX  \\\n",
       "run    lumiblock event      slice                                             \n",
       "297101 216       302550016  0      4.298   991.082153 -0.354882 -539.447571   \n",
       "297292 87        162028576  0      4.298   740.213928 -0.314870 -198.417114   \n",
       "297050 23        23347598   0      4.298   686.472717 -0.290764 -130.869202   \n",
       "       65        78788008   0      4.298  1188.718994  0.036870 -732.761475   \n",
       "       499       605989248  0      4.298   585.438477  0.021673  -10.685974   \n",
       "297411 315       507722016  0      4.298  1281.273071  0.157078 -894.971375   \n",
       "       377       608603776  0      4.298   533.259521 -0.114459   -5.137268   \n",
       "297177 103       155861312  0      4.298   835.258789  0.426335 -464.775391   \n",
       "       112       168879968  0      4.298   662.516785 -0.480477  -61.676575   \n",
       "297057 359       451236608  0      4.298  1477.063232 -0.068353 -562.615295   \n",
       "297219 2173      3099827968 0      4.298  1283.559326  0.109039 -653.152527   \n",
       "                 3099778304 0      4.298  1190.692749 -0.067335 -425.383972   \n",
       "                 3100205568 0      4.298   857.619751 -0.030244 -326.385620   \n",
       "297425 58        92710800   0      4.298  1039.189087 -0.384506 -378.624207   \n",
       "297050 640       759692672  0      4.298   733.031860  0.553902  159.882874   \n",
       "       658       778318912  0      4.298   897.606995  0.386342  -55.687317   \n",
       "       633       752294720  0      4.298   834.609375 -0.459594 -289.815002   \n",
       "                 752405312  0      4.298   832.814880 -0.137661 -285.878296   \n",
       "       635       754300416  0      4.298   830.026733 -0.218986 -442.425659   \n",
       "       634       753442816  0      4.298   883.677795  0.323264   88.513611   \n",
       "\n",
       "                                   ratioMWW_MX  shiftedRatioMWW_MX  diffYWW_YX  \n",
       "run    lumiblock event      slice                                               \n",
       "297101 216       302550016  0         0.455698           -0.544302   -0.057944  \n",
       "297292 87        162028576  0         0.731946           -0.268054    0.441119  \n",
       "297050 23        23347598   0         0.809360           -0.190640   -0.851664  \n",
       "       65        78788008   0         0.383570           -0.616430   -0.824225  \n",
       "       499       605989248  0         0.981747           -0.018253   -0.049548  \n",
       "297411 315       507722016  0         0.301498           -0.698502   -2.032602  \n",
       "       377       608603776  0         0.990366           -0.009634   -0.776025  \n",
       "297177 103       155861312  0         0.443555           -0.556445    0.356864  \n",
       "       112       168879968  0         0.906906           -0.093094   -0.452691  \n",
       "297057 359       451236608  0         0.619099           -0.380901   -0.027711  \n",
       "297219 2173      3099827968 0         0.491140           -0.508860    0.765708  \n",
       "                 3099778304 0         0.642742           -0.357258    0.714580  \n",
       "                 3100205568 0         0.619429           -0.380571   -1.324861  \n",
       "297425 58        92710800   0         0.635654           -0.364346   -0.130105  \n",
       "297050 640       759692672  0         1.218112            0.218112   -1.763480  \n",
       "       658       778318912  0         0.937960           -0.062040   -1.731446  \n",
       "       633       752294720  0         0.652754           -0.347246    1.814427  \n",
       "                 752405312  0         0.656732           -0.343268   -0.518805  \n",
       "       635       754300416  0         0.466974           -0.533026    1.997271  \n",
       "       634       753442816  0         1.100165            0.100165    0.612963  \n",
       "\n",
       "[20 rows x 50 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_protons_multiRP_bkg_events[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2651, 15)\n",
      "                               jet0_pt  jet0_phi  jet0_tau1  jet0_tau2  \\\n",
      "run lumiblock event  slice                                               \n",
      "1   2273      227227 0      335.731934 -3.108820   0.267451   0.157171   \n",
      "    2275      227457 0      345.850189  3.099098   0.233271   0.122797   \n",
      "    2276      227546 0      608.518616 -0.481351   0.172939   0.069660   \n",
      "              227579 0      819.906433  1.723913   0.110508   0.028728   \n",
      "    2281      228042 0      764.408875  2.295349   0.132495   0.041123   \n",
      "    2282      228117 0      321.463196 -2.196248   0.315415   0.074349   \n",
      "    2286      228503 0      649.005920  1.066668   0.136293   0.045175   \n",
      "              228565 0      360.785767 -1.373860   0.313472   0.088111   \n",
      "    2297      229605 0      314.089844  2.804024   0.238683   0.113802   \n",
      "    2298      229715 0      303.292603  1.378168   0.361345   0.191824   \n",
      "    2299      229845 0      477.458221  2.630056   0.146203   0.045197   \n",
      "              229840 0      320.155548  1.047925   0.261954   0.129281   \n",
      "    2301      230059 0      441.006378 -0.766519   0.216342   0.052447   \n",
      "    2302      230119 0      798.193726 -1.469288   0.066868   0.028144   \n",
      "              230195 0      544.700195  1.596644   0.107923   0.052825   \n",
      "    2305      230479 0      711.671204 -2.544181   0.122053   0.059448   \n",
      "    2316      231571 0      304.978241  1.442453   0.318945   0.116181   \n",
      "    2320      231972 0      776.558167  1.715490   0.117076   0.036474   \n",
      "    2325      232498 0      251.262024  0.202985   0.455797   0.160066   \n",
      "    2326      232588 0      219.488464  0.155746   0.462321   0.072898   \n",
      "\n",
      "                              muon0_pt  muon0_phi         met   met_phi  \\\n",
      "run lumiblock event  slice                                                \n",
      "1   2273      227227 0       59.688713  -0.185175  279.726990  0.073903   \n",
      "    2275      227457 0      299.653168  -0.157973   46.698696  0.576530   \n",
      "    2276      227546 0      563.565247   2.698961   35.266457  0.594330   \n",
      "              227579 0      263.500122  -1.364434  554.802795 -1.440646   \n",
      "    2281      228042 0      171.430862  -0.646621  569.630676 -0.919760   \n",
      "    2282      228117 0      182.239685   0.833361   65.530228  1.270933   \n",
      "    2286      228503 0      253.108688  -2.239687  355.973389 -1.992965   \n",
      "              228565 0       82.222229   1.472144  274.096283  1.767121   \n",
      "    2297      229605 0      114.872162   0.018725  194.401230 -0.627070   \n",
      "    2298      229715 0      140.965668  -2.075449  154.517334 -1.532069   \n",
      "    2299      229845 0      261.667572  -0.400389  212.046051 -0.745950   \n",
      "              229840 0      310.928772  -2.139195   18.785320 -1.125676   \n",
      "    2301      230059 0      376.423645   2.340909   16.786642 -2.830832   \n",
      "    2302      230119 0      371.175262   1.684883  390.387268  1.647014   \n",
      "              230195 0      212.969421  -1.542189  336.686005 -1.592190   \n",
      "    2305      230479 0       62.106911   0.492302  570.847595  0.615912   \n",
      "    2316      231571 0      199.382690  -1.520774   99.450745 -2.009108   \n",
      "    2320      231972 0      704.859009  -1.409935   28.737747 -2.129198   \n",
      "    2325      232498 0      152.786087  -3.025021   93.958359 -2.729038   \n",
      "    2326      232588 0      147.309311  -2.869980   80.226387  3.120911   \n",
      "\n",
      "                            pfcand_nextracks  WLeptonicPt  WLeptonicPhi  \\\n",
      "run lumiblock event  slice                                                \n",
      "1   2273      227227 0                    12   337.770020      0.028615   \n",
      "    2275      227457 0                     0   335.773132     -0.064625   \n",
      "    2276      227546 0                     0   546.464294      2.643376   \n",
      "              227579 0                     0   817.784180     -1.416111   \n",
      "    2281      228042 0                     3   736.160339     -0.856900   \n",
      "    2282      228117 0                     2   243.186386      0.947794   \n",
      "    2286      228503 0                     1   604.586548     -2.095389   \n",
      "              228565 0                     2   353.576324      1.699464   \n",
      "    2297      229605 0                     0   294.373840     -0.390005   \n",
      "    2298      229715 0                     0   284.667511     -1.790984   \n",
      "    2299      229845 0                     2   466.738281     -0.554890   \n",
      "              229840 0                     4   321.259766     -2.089548   \n",
      "    2301      230059 0                     0   384.161072      2.380086   \n",
      "    2302      230119 0                     2   761.425476      1.665471   \n",
      "              230195 0                     8   549.492310     -1.572818   \n",
      "    2305      230479 0                     0   632.527954      0.603805   \n",
      "    2316      231571 0                     1   290.974365     -1.681820   \n",
      "    2320      231972 0                     0   726.724854     -1.435990   \n",
      "    2325      232498 0                    19   244.201447     -2.912558   \n",
      "    2326      232588 0                    11   225.321930     -2.972757   \n",
      "\n",
      "                                recoMWW  recoRapidityWW           MX        YX  \n",
      "run lumiblock event  slice                                                      \n",
      "1   2273      227227 0       719.006958        0.010485   713.363708  0.032141  \n",
      "    2275      227457 0      1238.092041       -0.276731  1157.675781 -0.256271  \n",
      "    2276      227546 0      1294.787231       -0.159461  1384.800293 -0.162342  \n",
      "              227579 0      1881.556519       -0.178598  1843.537598 -0.179152  \n",
      "    2281      228042 0      1732.266968       -0.106745  1513.853882 -0.097639  \n",
      "    2282      228117 0       623.450012       -0.205239   607.001648 -0.221897  \n",
      "    2286      228503 0      1270.726807       -0.058441  1325.282715 -0.043387  \n",
      "              228565 0       861.616760        0.352069   802.749634  0.354960  \n",
      "    2297      229605 0       711.417664       -0.338958   675.655457 -0.298634  \n",
      "    2298      229715 0       770.068420        0.090720   706.302795  0.137830  \n",
      "    2299      229845 0      1364.477173       -0.515801  1247.290771 -0.492779  \n",
      "              229840 0      1144.093628       -0.448797  1271.329590 -0.418647  \n",
      "    2301      230059 0      1376.853271       -0.070998  1236.970337 -0.106248  \n",
      "    2302      230119 0      1580.546387        0.076291  1440.446411  0.063868  \n",
      "              230195 0      1528.207153        0.203880  1613.275391  0.092361  \n",
      "    2305      230479 0      1364.226318       -0.176050  1551.888184 -0.161361  \n",
      "    2316      231571 0       693.497925       -0.644795   656.987549 -0.716215  \n",
      "    2320      231972 0      1521.599365       -0.307415  1483.320068 -0.278201  \n",
      "    2325      232498 0       530.004333        0.041647   471.837036  0.032974  \n",
      "    2326      232588 0       910.025085        0.389043   826.616821  0.366834  \n",
      "(100000, 15)\n",
      "                                      jet0_pt  jet0_phi  jet0_tau1  jet0_tau2  \\\n",
      "run    lumiblock event      slice                                               \n",
      "297219 2173      3099827968 0      288.619812  0.454244   0.192689   0.060436   \n",
      "297050 633       752294720  0      281.741394  3.116731   0.317704   0.144190   \n",
      "                 752405312  0      203.592255  1.493702   0.424542   0.222673   \n",
      "       635       754300416  0      278.962280 -2.295970   0.450680   0.133851   \n",
      "297178 492       640713216  0      265.786072  2.916315   0.499268   0.165113   \n",
      "       18        25319148   0      294.096222 -0.131271   0.211378   0.144324   \n",
      "297050 86        108647864  0      256.000824 -1.053994   0.243876   0.161555   \n",
      "299062 265       435280032  0      244.149170 -0.558302   0.404284   0.164087   \n",
      "297429 13        21205478   0      231.507278 -2.024890   0.199734   0.129822   \n",
      "       55        95608760   0      250.853119  1.738213   0.482493   0.203398   \n",
      "297100 299       536040544  0      213.262833 -2.554901   0.288563   0.151412   \n",
      "297057 222       288992096  0      251.807938  2.359524   0.223934   0.098314   \n",
      "       136       184997920  0      231.314774 -3.119062   0.266010   0.173548   \n",
      "297430 6         11287939   0      253.492783 -1.922145   0.223665   0.153974   \n",
      "297057 833       964179584  0      557.916809 -2.884355   0.198749   0.126133   \n",
      "297050 25        25949614   0      281.364502 -0.263302   0.227486   0.154717   \n",
      "299184 312       512313504  0      204.175751  2.458998   0.334451   0.087232   \n",
      "297100 301       539401344  0      237.537018  2.151508   0.242644   0.133341   \n",
      "       353       619094592  0      381.178009  0.788046   0.260924   0.098505   \n",
      "297308 11        9230215    0      219.985565 -1.968702   0.158578   0.073125   \n",
      "\n",
      "                                     muon0_pt  muon0_phi         met  \\\n",
      "run    lumiblock event      slice                                      \n",
      "297219 2173      3099827968 0      116.402565  -2.200928  156.650482   \n",
      "297050 633       752294720  0      136.802490   0.357347   50.260876   \n",
      "                 752405312  0       76.544220  -0.945078  146.062485   \n",
      "       635       754300416  0       55.994087  -0.171512   16.637386   \n",
      "297178 492       640713216  0      194.762405  -0.166104  107.078514   \n",
      "       18        25319148   0      219.955276   2.758443   23.653261   \n",
      "297050 86        108647864  0      272.063965   2.030100   26.717655   \n",
      "299062 265       435280032  0       58.741760   2.402935  127.405151   \n",
      "297429 13        21205478   0      172.335297   1.249367   50.216431   \n",
      "       55        95608760   0       50.997974  -2.612201   42.958923   \n",
      "297100 299       536040544  0       44.420254   0.957096   17.341503   \n",
      "297057 222       288992096  0       73.220367  -1.035420  204.410339   \n",
      "       136       184997920  0      177.677460  -0.432606   66.809860   \n",
      "297430 6         11287939   0      169.332321   1.086343  121.356842   \n",
      "297057 833       964179584  0       57.192329  -0.101035  493.430115   \n",
      "297050 25        25949614   0       85.921112   3.111260  186.055130   \n",
      "299184 312       512313504  0      128.496643  -0.746906   52.199570   \n",
      "297100 301       539401344  0      151.982437  -1.029728   59.460835   \n",
      "       353       619094592  0       71.062332  -2.469296   71.966049   \n",
      "297308 11        9230215    0      111.167412   1.235271  109.642021   \n",
      "\n",
      "                                    met_phi  pfcand_nextracks  WLeptonicPt  \\\n",
      "run    lumiblock event      slice                                            \n",
      "297219 2173      3099827968 0     -2.787736                18   261.643188   \n",
      "297050 633       752294720  0     -3.044703                90   218.592194   \n",
      "                 752405312  0     -1.812114                64   204.114319   \n",
      "       635       754300416  0      0.555821                59    69.309814   \n",
      "297178 492       640713216  0     -0.477826                60   298.492615   \n",
      "       18        25319148   0     -2.338818                33   263.504822   \n",
      "297050 86        108647864  0     -0.724373                37   247.529007   \n",
      "299062 265       435280032  0      2.818822                 5    99.050484   \n",
      "297429 13        21205478   0      0.511735                25   212.203308   \n",
      "       55        95608760   0     -0.654218                49    52.837715   \n",
      "297100 299       536040544  0      1.651533                18    82.815414   \n",
      "297057 222       288992096  0     -0.713653                28   274.850037   \n",
      "       136       184997920  0     -3.062608                51   123.819603   \n",
      "297430 6         11287939   0      1.775328                55   274.089600   \n",
      "297057 833       964179584  0     -0.000376                47   550.362793   \n",
      "297050 25        25949614   0      2.847505                15   269.935944   \n",
      "299184 312       512313504  0     -0.913633                25   168.675690   \n",
      "297100 301       539401344  0     -1.200316                15   210.822006   \n",
      "       353       619094592  0      0.131224                23    38.234097   \n",
      "297308 11        9230215    0      1.284861                23   220.741562   \n",
      "\n",
      "                                   WLeptonicPhi      recoMWW  recoRapidityWW  \\\n",
      "run    lumiblock event      slice                                              \n",
      "297219 2173      3099827968 0         -2.538836   630.406799        0.874747   \n",
      "297050 633       752294720  0          0.412133   544.794373        1.354833   \n",
      "                 752405312  0         -1.522156   546.936584       -0.656466   \n",
      "       635       754300416  0         -0.011227   387.601074        1.778285   \n",
      "297178 492       640713216  0         -0.276349   638.857849       -1.720904   \n",
      "       18        25319148   0          2.763414   751.345886        0.216361   \n",
      "297050 86        108647864  0          1.989340   778.246948       -0.506885   \n",
      "299062 265       435280032  0          2.867944   575.872498        0.188393   \n",
      "297429 13        21205478   0          1.089537   838.701721        0.296661   \n",
      "       55        95608760   0         -1.759819   331.372925       -0.568604   \n",
      "297100 299       536040544  0          1.109561   334.051971       -0.071973   \n",
      "297057 222       288992096  0         -0.798001   574.660095       -0.087020   \n",
      "       136       184997920  0         -0.699936   574.946899       -1.086981   \n",
      "297430 6         11287939   0          1.371688   689.824036        0.313667   \n",
      "297057 833       964179584  0         -0.010819  1398.230347        0.722681   \n",
      "297050 25        25949614   0          2.930584   769.940002        1.460537   \n",
      "299184 312       512313504  0         -0.627023   577.436707        0.933358   \n",
      "297100 301       539401344  0         -1.077626   501.222809        1.103781   \n",
      "       353       619094592  0         -1.146258   408.195160        0.651679   \n",
      "297308 11        9230215    0          1.259895   473.625305        0.474679   \n",
      "\n",
      "                                            MX        YX  \n",
      "run    lumiblock event      slice                         \n",
      "297219 2173      3099827968 0      1283.559326  0.109039  \n",
      "297050 633       752294720  0       834.609375 -0.459594  \n",
      "                 752405312  0       832.814880 -0.137661  \n",
      "       635       754300416  0       830.026733 -0.218986  \n",
      "297178 492       640713216  0      1061.609375  0.051072  \n",
      "       18        25319148   0       600.780884 -0.219225  \n",
      "297050 86        108647864  0      1283.559326  0.109039  \n",
      "299062 265       435280032  0       862.115295  0.017556  \n",
      "297429 13        21205478   0       628.371216 -0.088150  \n",
      "       55        95608760   0      1116.774170 -0.308799  \n",
      "297100 299       536040544  0      1116.470215 -0.000673  \n",
      "297057 222       288992096  0      1435.116333  0.026479  \n",
      "       136       184997920  0      1341.757935  0.021317  \n",
      "297430 6         11287939   0      1423.255371 -0.110249  \n",
      "297057 833       964179584  0       779.499084 -0.074154  \n",
      "297050 25        25949614   0       647.292480 -0.464040  \n",
      "299184 312       512313504  0       886.006287 -0.346447  \n",
      "297100 301       539401344  0      1458.896362  0.017780  \n",
      "       353       619094592  0      1255.012939 -0.012000  \n",
      "297308 11        9230215    0       464.179047  0.063929  \n"
     ]
    }
   ],
   "source": [
    "msk_bkg = ( ( df_protons_multiRP_bkg_events.loc[ :, \"jet0_corrmass\"] >= 50.0 ) &\n",
    "            ( df_protons_multiRP_bkg_events.loc[ :, \"jet0_corrmass\"] <= 110.0 ) & \n",
    "            ( df_protons_multiRP_bkg_events.loc[ :, \"num_bjets_ak4\"] == 0 ) )\n",
    "msk_signal = ( ( df_protons_multiRP_signal_events.loc[ :, \"jet0_corrmass\"] >= 50.0 ) &\n",
    "               ( df_protons_multiRP_signal_events.loc[ :, \"jet0_corrmass\"] <= 110.0 ) &\n",
    "               ( df_protons_multiRP_signal_events.loc[ :, \"num_bjets_ak4\"] == 0 ) )\n",
    "df_protons_multiRP_bkg_events = df_protons_multiRP_bkg_events.loc[ msk_bkg ]\n",
    "df_protons_multiRP_signal_events = df_protons_multiRP_signal_events.loc[ msk_signal ]\n",
    "\n",
    "# variables_ = [ 'jet0_pt', 'jet0_eta', 'jet0_phi', 'jet0_tau1', 'jet0_tau2', 'muon0_pt', 'muon0_eta', 'muon0_phi', 'met', 'met_phi',\n",
    "#                'pfcand_nextracks', 'WLeptonicPt', 'WLeptonicPhi', 'recoMWW', 'recoRapidityWW', 'MX', 'YX' ]\n",
    "variables_ = [ 'jet0_pt', 'jet0_phi', 'jet0_tau1', 'jet0_tau2', 'muon0_pt', 'muon0_phi', 'met', 'met_phi',\n",
    "               'pfcand_nextracks', 'WLeptonicPt', 'WLeptonicPhi', 'recoMWW', 'recoRapidityWW', 'MX', 'YX' ]\n",
    "\n",
    "\n",
    "n_events_signal = None\n",
    "if n_events_signal:\n",
    "    X_sig = df_protons_multiRP_signal_events[ variables_ ].iloc[:n_events_signal]\n",
    "else:\n",
    "    X_sig = df_protons_multiRP_signal_events[ variables_ ]\n",
    "print ( X_sig.shape )\n",
    "print ( X_sig[:20] )\n",
    "\n",
    "n_events_bkg = 100000\n",
    "if n_events_bkg:\n",
    "    X_bkg = df_protons_multiRP_bkg_events[ variables_ ].iloc[:n_events_bkg]\n",
    "else:\n",
    "    X_bkg = df_protons_multiRP_bkg_events[ variables_ ]\n",
    "print ( X_bkg.shape )\n",
    "print ( X_bkg[:20] )\n",
    "\n",
    "y_sig = np.ones( len(X_sig) )\n",
    "y_bkg = np.zeros( len(X_bkg) )\n",
    "\n",
    "X = pd.concat( [X_sig, X_bkg] ) \n",
    "y = np.concatenate( [y_sig, y_bkg] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split( X, y, test_size=0.20, shuffle=True, random_state=42 )\n",
    "X_train, X_valid, y_train, y_valid = train_test_split( X_train_full, y_train_full, test_size=0.20, shuffle=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler()\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = None\n",
    "\n",
    "if train_model:\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform( X_train )\n",
    "else:\n",
    "    scaler = load( scaler_path )\n",
    "    X_train_scaled = scaler.transform( X_train )\n",
    "    \n",
    "X_valid_scaled = scaler.transform( X_valid )\n",
    "X_test_scaled = scaler.transform( X_test )\n",
    "\n",
    "print ( scaler )\n",
    "\n",
    "if train_model and save_model:\n",
    "    dump( scaler, \"model/standard_scaler.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.50505789,  1.02582681,  0.94401592, -0.22675914,  0.57224713,\n",
       "        -1.04846914, -0.81600841, -0.78170755, -1.58025911, -0.07412654,\n",
       "        -1.01027347,  0.2161363 ,  0.48175676, -0.07535906,  0.76783978],\n",
       "       [-0.30499371, -0.10446489, -1.03228439, -0.61967524, -0.88195714,\n",
       "        -1.51440517, -0.85773221, -0.85010235,  0.85682177, -1.28173592,\n",
       "        -1.31869737, -1.03510702,  0.65900975, -1.93919022, -0.92683617],\n",
       "       [ 0.5209429 ,  0.72199448, -0.54137592, -0.04423102,  0.70352056,\n",
       "        -0.9326525 , -0.90743875, -1.03250309, -1.30436316, -0.01471699,\n",
       "        -0.95050115,  1.14519945, -0.19123966, -0.19248966,  1.54171388],\n",
       "       [-0.65248749, -1.50782425,  0.60338686, -1.06338358,  0.41803806,\n",
       "         0.36887113, -0.15950406, -0.01035741,  0.07511658,  0.21864605,\n",
       "         0.23802837, -0.50417025, -0.7615129 ,  2.44740981,  0.39770461],\n",
       "       [-0.70534166,  1.24186772,  0.32887541, -0.27462984, -0.16738362,\n",
       "         0.12710116, -0.40162327, -0.740298  , -0.29274468, -0.7419387 ,\n",
       "        -0.1564924 , -0.83113875,  1.0712862 , -1.67513182,  0.96747446],\n",
       "       [-0.14303643,  1.74404612, -1.27868529, -0.17027283, -0.64513269,\n",
       "         0.2539168 ,  0.85957224, -0.05019544, -1.12043253,  0.24435116,\n",
       "         0.05296885,  0.77323568,  0.41805248,  0.32033506, -0.02992195],\n",
       "       [ 0.39845013, -0.88033765, -0.97739725, -0.43833978, -0.37988541,\n",
       "         0.8563688 ,  1.60082355,  0.74122084, -1.16641519,  1.05476477,\n",
       "         0.78063198,  0.25949218, -0.46635869, -0.47382215, -0.63578561],\n",
       "       [-0.30680239, -0.23730519,  0.88758644,  0.3945193 ,  2.32886375,\n",
       "         1.51551193, -0.74155728,  1.18052467,  0.07511658,  1.24572027,\n",
       "         1.47027966,  0.02116846, -0.2415827 , -0.70993543,  0.17555921],\n",
       "       [ 0.12754508, -0.27136768, -1.32401048, -1.05035491, -0.05320452,\n",
       "        -0.45101295, -1.01104302, -1.38118899, -0.33872734, -0.83724457,\n",
       "        -0.5413446 ,  0.20172867,  0.56901846, -0.46722095,  0.27873374],\n",
       "       [ 0.4406869 , -0.59393619, -1.3482956 , -1.13273704, -0.7557122 ,\n",
       "         0.96709128, -0.28767463,  0.9428714 ,  0.21306455, -0.61719931,\n",
       "         0.95466821, -0.53014429,  0.60838696, -0.69374556, -1.56684798],\n",
       "       [-0.54760175,  0.96023669,  0.27497986,  1.43465567, -0.86220743,\n",
       "        -1.08395205, -0.81356077, -0.03530508,  0.12109924, -1.4556916 ,\n",
       "        -0.7990263 , -1.09589372,  1.20754049,  0.17563554,  1.73112562],\n",
       "       [ 0.34727935, -1.07695953, -1.12331993, -1.27895894,  2.38905761,\n",
       "         0.67853935, -0.46035351,  0.57718165, -0.20077937,  1.5490786 ,\n",
       "         0.65571497,  0.94104477,  1.09618232, -1.61056386,  0.213267  ],\n",
       "       [ 0.30486342, -1.50265743, -0.58141639, -1.11991042, -0.03963712,\n",
       "         0.13649954, -0.2012035 ,  0.47053485, -0.24676203, -0.12589543,\n",
       "         0.26219617, -0.03817238,  0.54583484,  1.35221808, -0.12880472],\n",
       "       [ 0.22449139, -1.63293569, -0.40828477, -0.77250061, -0.7980791 ,\n",
       "        -0.14024765, -0.54329008,  0.07630333,  0.90280443, -0.86526616,\n",
       "        -0.04833222, -0.77873423, -2.05195264,  0.12201755, -1.84081719],\n",
       "       [ 0.50756797, -1.41025385, -1.50200783, -0.93862633,  0.41980845,\n",
       "         0.43785148,  0.83065828,  0.13926936,  1.0407524 ,  0.97178881,\n",
       "         0.28885798,  0.21926178, -0.76223873, -0.50152684, -0.99228646],\n",
       "       [-0.65999095,  0.01077981,  1.00387078, -0.1888143 , -0.6498157 ,\n",
       "         0.40794532,  1.30338812, -1.66565343,  0.76485645, -0.74271016,\n",
       "         1.59072411, -0.25355719, -0.31077178, -0.18203863,  1.76900453],\n",
       "       [-0.1569609 , -1.25179227,  1.17633255,  1.07598705, -0.77105417,\n",
       "         0.76031932, -0.45696019, -0.02802819, -0.38471   , -1.06659086,\n",
       "         0.40692757,  0.64434488, -0.17736836, -0.04396235, -1.19068439],\n",
       "       [-0.74257739, -0.54000944,  0.70348619,  2.21705681,  1.04995188,\n",
       "         1.18097638, -1.16725595,  1.49534648, -1.12043253,  0.03567884,\n",
       "         1.18206404, -0.42281861, -0.98868462,  0.48873139, -0.05576521],\n",
       "       [-0.08008001, -0.53323932, -1.76543579, -1.0689228 ,  1.49884723,\n",
       "         1.13940537, -0.24051137,  1.3090201 , -0.9365019 ,  1.0335399 ,\n",
       "         1.17530598,  0.02533754, -0.16984166,  2.14031787, -0.24340902],\n",
       "       [ 0.32569268,  0.97110879, -1.46532438, -1.2505608 , -0.58066378,\n",
       "        -1.25506738, -0.25505527, -0.86151728,  1.0407524 , -0.57098443,\n",
       "        -1.07398472, -0.4537495 , -0.02609973,  0.12884914, -0.2065619 ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model build function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden=1, n_neurons=50, learning_rate=5e-4, dropout=0.20, input_shape=X_train_scaled.shape[1:]):\n",
    "    print( \"Building model with:\" )\n",
    "    print( \"Number of hidden layers: {}\".format(n_hidden) )\n",
    "    print( \"Number of neurons per layer: {}\".format(n_neurons) )\n",
    "    print( \"Learning rate: {}\".format(learning_rate) )\n",
    "    print( \"Input shape: {}\".format(input_shape) )\n",
    "    print( \"Dropout rate: {}\".format(dropout) )\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add( keras.layers.InputLayer(input_shape=input_shape) )\n",
    "    for layer in range(n_hidden):\n",
    "        if dropout > 0.:\n",
    "            model.add( keras.layers.Dropout(rate=dropout) )\n",
    "        model.add( keras.layers.Dense(n_neurons, activation=\"elu\", kernel_initializer=\"he_normal\") )\n",
    "    if dropout > 0.:\n",
    "        model.add( keras.layers.Dropout(rate=dropout) )    \n",
    "    model.add( keras.layers.Dense(1, activation=\"sigmoid\") )\n",
    "    \n",
    "    #optimizer = keras.optimizers.SGD(lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "    optimizer = keras.optimizers.Nadam(lr=learning_rate)\n",
    "    model.compile( loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_logdir(log_dir):\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(log_dir, run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callbacks(patience=10, log_dir=\"\"):\n",
    "    callbacks_ = []\n",
    "    # Early stopping\n",
    "    if patience > 0:\n",
    "        early_stopping_cb_ = keras.callbacks.EarlyStopping( patience=patience, restore_best_weights=True )\n",
    "        callbacks_.append( early_stopping_cb_ )\n",
    "        \n",
    "    # TensorBoard\n",
    "    if log_dir:\n",
    "        run_logdir = get_run_logdir(log_dir)\n",
    "        print ( \"Log dir: {}\".format(run_logdir) )\n",
    "        tensorboard_cb_ = keras.callbacks.TensorBoard( run_logdir )\n",
    "        callbacks_.append( tensorboard_cb_ )\n",
    "    \n",
    "    return callbacks_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log dir: keras_logs/run_2021_02_04-10_51_40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.callbacks.EarlyStopping at 0x7f5d24692a90>,\n",
       " <tensorflow.python.keras.callbacks.TensorBoard at 0x7f5d24692f50>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dir=\"keras_logs\"\n",
    "callbacks_ = callbacks(patience=5, log_dir=log_dir)\n",
    "callbacks_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model (example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with:\n",
      "Number of hidden layers: 1\n",
      "Number of neurons per layer: 50\n",
      "Learning rate: 0.0001\n",
      "Input shape: (15,)\n",
      "Dropout rate: 0.2\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout (Dropout)            (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50)                800       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 851\n",
      "Trainable params: 851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model( \n",
    "    n_hidden=1,\n",
    "    n_neurons=50,\n",
    "    learning_rate=1e-4,\n",
    "    dropout=0.20,\n",
    "    input_shape=X_train_scaled.shape[1:]\n",
    "    )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fit_example = False\n",
    "\n",
    "if fit_example:\n",
    "    history = model.fit( X_train_scaled, y_train, epochs=100, validation_data=(X_valid_scaled, y_valid), callbacks=callbacks_ )\n",
    "    history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fit_example:\n",
    "    pd.DataFrame( history.history ).plot( figsize=(12,10) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on training data (without dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2053/2053 [==============================] - 2s 880us/step - loss: 0.7845 - accuracy: 0.5760\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7844530344009399, 0.5760167837142944]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate( X_train_scaled, y_train )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "642/642 [==============================] - 1s 1ms/step - loss: 0.7849 - accuracy: 0.5771\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7849200367927551, 0.5771272778511047]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate( X_test_scaled, y_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20531, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.63994586],\n",
       "       [0.4709172 ],\n",
       "       [0.42279437],\n",
       "       ...,\n",
       "       [0.53103465],\n",
       "       [0.45071825],\n",
       "       [0.28720126]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_proba = model.predict( X_test_scaled )\n",
    "print ( y_test_proba.shape )\n",
    "y_test_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scan over different learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_learning_rate( lr_init=1e-4, lr_end=5e-2, steps=20, epochs=30, model_build_fn=build_model, *build_fn_args, **build_fn_kwargs ):\n",
    "    results_ = {}\n",
    "    results_['learning_rate'] = []\n",
    "    results_['loss'] = []\n",
    "    results_['accuracy'] = []\n",
    "    results_['val_loss'] = []\n",
    "    results_['val_accuracy'] = []\n",
    "    c_ = (lr_end/lr_init) ** (1/steps)\n",
    "    lr_ = lr_init\n",
    "    for i_it in range( steps + 1 ):\n",
    "        results_['learning_rate'].append( lr_ )\n",
    "        model_ = model_build_fn( *build_fn_args, **build_fn_kwargs, learning_rate=lr_ )\n",
    "        callbacks_ = callbacks(patience=5)\n",
    "        history_ = model_.fit( X_train_scaled, y_train, epochs=epochs, validation_data=(X_valid_scaled, y_valid), callbacks=callbacks_ )\n",
    "        results_['loss'].append( history_.history['loss'] )\n",
    "        results_['accuracy'].append( history_.history['accuracy'] )\n",
    "        results_['val_loss'].append( history_.history['val_loss'] )\n",
    "        results_['val_accuracy'].append( history_.history['val_accuracy'] )\n",
    "        # Update learning rate\n",
    "        lr_ = lr_ * c_\n",
    "        \n",
    "    return results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs_lr_scan=20\n",
    "results = None\n",
    "if learning_rate_scan:\n",
    "    results = find_max_learning_rate(\n",
    "                lr_init=1e-4,\n",
    "                lr_end=2e-2,\n",
    "                steps=10,\n",
    "                epochs=epochs_lr_scan,\n",
    "                model_build_fn=build_model,\n",
    "                n_hidden=1,\n",
    "                n_neurons=20,\n",
    "                dropout=0.20,\n",
    "                input_shape=X_train_scaled.shape[1:],\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if learning_rate_scan:\n",
    "    metrics_ = 'val_loss'\n",
    "    columns=[\"lr_{}\".format(lr_) for lr_ in np.round( results['learning_rate'], 4)] \n",
    "    df = pd.DataFrame( np.full((epochs_lr_scan,len(columns)),np.nan), columns=columns )\n",
    "    for i_lr_,col_ in enumerate(columns):\n",
    "        df[col_] = pd.Series( results[ metrics_ ][i_lr_] )\n",
    "    df.plot( figsize=(12,10) )\n",
    "    plt.yscale('log')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-4\n",
    "#dropout = 0.50\n",
    "grid_search = None\n",
    "n_iter_search = 3\n",
    "\n",
    "if train_model and run_grid_search:\n",
    "    import time\n",
    "    print( time.strftime(\"%Y/%m/%d %H:%M:%S\", time.localtime() ) )\n",
    "    time_s_ = time.time()\n",
    "        \n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    #from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    def build_fn_(n_hidden, n_neurons, dropout):\n",
    "        return build_model(n_hidden, n_neurons, learning_rate=learning_rate, input_shape=X_train_scaled.shape[1:], dropout=dropout)\n",
    "\n",
    "    keras_clf = keras.wrappers.scikit_learn.KerasClassifier( build_fn_ )\n",
    "\n",
    "#     #param_grid = [\n",
    "#     #    { \"n_hidden\": [2],\n",
    "#     #      \"n_neurons\": [50,100] }\n",
    "#     #    ]\n",
    "#     param_grid = [\n",
    "#         { \"n_hidden\": np.arange(1,3),\n",
    "#           \"n_neurons\": [20,50] }\n",
    "#         ]\n",
    "\n",
    "    param_distribs = {\n",
    "        \"n_hidden\": np.arange(2,6),\n",
    "        \"n_neurons\": 2 ** np.arange(4,8),\n",
    "        \"dropout\":  0.1 * np.arange(2,6),\n",
    "        \"batch_size\": 2 ** np.arange(5,8)\n",
    "        }\n",
    "\n",
    "    #grid_search = GridSearchCV( keras_clf, param_grid, cv=3, scoring='f1', refit=False )\n",
    "    \n",
    "    grid_search = RandomizedSearchCV(\n",
    "        keras_clf,\n",
    "        param_distribs,\n",
    "        n_iter=n_iter_search, cv=4, verbose=20, n_jobs=-1, scoring='f1', refit=False, random_state=42\n",
    "        )\n",
    "\n",
    "    callbacks_ = callbacks(patience=5)\n",
    "    print ( callbacks_ )\n",
    "    grid_search.fit( X_train_scaled, y_train, epochs=100, validation_data=(X_valid_scaled, y_valid), callbacks=callbacks_ )\n",
    "    \n",
    "    print ( grid_search.best_params_ )\n",
    "    print ( grid_search.best_score_ )\n",
    "    print ( grid_search.cv_results_ )\n",
    "    \n",
    "    time_e_ = time.time()\n",
    "    print ( \"Total time elapsed: {:.0f}\".format( time_e_ - time_s_ ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout (Dropout)            (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 68,225\n",
      "Trainable params: 68,225\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_final = None\n",
    "\n",
    "if train_model:\n",
    "    params = {'n_hidden': 1, 'n_neurons': 50, 'dropout': 0.20}\n",
    "    batch_size = 32\n",
    "    if run_grid_search: \n",
    "        params = grid_search.best_params_.copy()\n",
    "        batch_size = params[ 'batch_size' ]\n",
    "        params.pop( 'batch_size' )\n",
    "    print ( params, \"batch_size: {}\".format( batch_size ) )\n",
    "    \n",
    "    model_final = build_model(**params, learning_rate=8e-4, input_shape=X_train_scaled.shape[1:])\n",
    "    model_final.summary()\n",
    "    log_dir=\"keras_logs\"\n",
    "    callbacks_ = callbacks(patience=5, log_dir=log_dir)\n",
    "    print ( callbacks_ )\n",
    "    model_final.fit( X_train_scaled, y_train, epochs=100, batch_size=batch_size, validation_data=(X_valid_scaled, y_valid), callbacks=callbacks_ )\n",
    "else:\n",
    "    model_final = keras.models.load_model( model_path )\n",
    "    \n",
    "model_final.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'sequential',\n",
       " 'layers': [{'class_name': 'InputLayer',\n",
       "   'config': {'batch_input_shape': (None, 15),\n",
       "    'dtype': 'float32',\n",
       "    'sparse': False,\n",
       "    'ragged': False,\n",
       "    'name': 'input_1'}},\n",
       "  {'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'rate': 0.2,\n",
       "    'noise_shape': None,\n",
       "    'seed': None}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'dense',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 128,\n",
       "    'activation': 'elu',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'HeNormal', 'config': {'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}},\n",
       "  {'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'rate': 0.2,\n",
       "    'noise_shape': None,\n",
       "    'seed': None}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 128,\n",
       "    'activation': 'elu',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'HeNormal', 'config': {'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}},\n",
       "  {'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'rate': 0.2,\n",
       "    'noise_shape': None,\n",
       "    'seed': None}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 128,\n",
       "    'activation': 'elu',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'HeNormal', 'config': {'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}},\n",
       "  {'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'rate': 0.2,\n",
       "    'noise_shape': None,\n",
       "    'seed': None}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 128,\n",
       "    'activation': 'elu',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'HeNormal', 'config': {'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}},\n",
       "  {'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'rate': 0.2,\n",
       "    'noise_shape': None,\n",
       "    'seed': None}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 128,\n",
       "    'activation': 'elu',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'HeNormal', 'config': {'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}},\n",
       "  {'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'rate': 0.2,\n",
       "    'noise_shape': None,\n",
       "    'seed': None}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 1,\n",
       "    'activation': 'sigmoid',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
       "     'config': {'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}}]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_final.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on training data (without dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2053/2053 [==============================] - 2s 1ms/step - loss: 0.0152 - accuracy: 0.9955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.015210107900202274, 0.9954639673233032]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_final.evaluate( X_train_scaled, y_train )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-evaluate on validation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "514/514 [==============================] - 1s 2ms/step - loss: 0.0169 - accuracy: 0.9949\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.016910690814256668, 0.9949464201927185]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_final.evaluate( X_valid_scaled, y_valid )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "642/642 [==============================] - 1s 2ms/step - loss: 0.0153 - accuracy: 0.9953\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.4373660e-04],\n",
       "       [8.6508423e-01],\n",
       "       [2.5271900e-09],\n",
       "       ...,\n",
       "       [6.3006490e-02],\n",
       "       [1.2544990e-03],\n",
       "       [3.2055378e-04]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_final.evaluate( X_test_scaled, y_test )\n",
    "\n",
    "y_test_proba = model_final.predict( X_test_scaled )\n",
    "y_test_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure( figsize=(12,10) )\n",
    "plt.hist( y_test_proba[ y_test == 0 ], histtype='step', color='lightcoral', bins=30, range=(0.,1.), label=\"Background\" )\n",
    "plt.hist( y_test_proba[ y_test == 1 ], histtype='step', color='lightblue', bins=30, range=(0.,1.), label=r\"$\\gamma\\gamma \\to WW \\quad a_0^W=1e^{-6}$\" )\n",
    "plt.yscale('log')\n",
    "plt.xlabel( \"Classifier probability\", fontsize=22 )\n",
    "plt.ylabel( \"Events\", fontsize=22 )\n",
    "plt.legend( loc='best', fontsize=16 )\n",
    "if save_figures:\n",
    "    plt.savefig( \"plots/ANN-Keras_Classifier_Probability_test-multiRP_2021_01_22-17_46_10.pdf\", bbox_inches='tight' )\n",
    "    plt.savefig( \"plots/ANN-Keras_Classifier_Probability_test-multiRP_2021_01_22-17_46_10.png\", bbox_inches='tight' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAt0AAAJbCAYAAAA43ig3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nOzdebhdVX038O8iARKGXBCCQBiChkEKBSRSKYKKgAwGBEFBXotjihYkDtWiVAi0lBdURotERRwoVhSKsQJSLPZRtAUEfKGRMgmEAGHyUoiYgfX+cW9iEjLdm7Nzzj35fJ7nPOSsvffav7u5Sb5Zd521Sq01AABAc9ZodwEAANDthG4AAGiY0A0AAA0TugEAoGFCNwAANEzoBgCAhg1vdwGrwsYbb1zHjh3b7jIAAOhyt91221O11tGLt68WoXvs2LG59dZb210GAABdrpTy0JLaTS8BAICGCd0AANAwoRsAABomdAMAQMOEbgAAaJjQDQAADVstlgwEgNXFc889l5kzZ2bOnDntLgW6xvDhwzNixIiMHj06I0aMGFwfLa4JAGiT5557Lk888UTGjBmTkSNHppTS7pJgyKu1Zu7cuXn++efz8MMP55WvfGV6enoG3I/QDQBdYubMmRkzZkzWWWeddpcCXaOUkjXXXDMbbrhh1l577Tz++OODCt3mdANAl5gzZ05GjhzZ7jKga40cOTJ/+MMfBnWt0A0AXcSUEmjOyvz+EroBAKBhQjcAADTMBykBYDXw3Hnnpfb2tu3+pacnoyZNGvB1l112Wd73vvcteL/GGmtk0003zV577ZUzzjgj22+/fSvLTJKMHTs2b3jDG/Ltb3+75X13stNOOy2TJ09OrbXdpXQloRsAVgO1tzc9p57atvv3Tp68UtdfeeWV2WKLLTJv3rzcf//9OeOMM/KWt7wld99996BWkoBVTegGADrerrvumnHjxiVJ9tprr2y++ebZf//9c/PNN+eggw5qc3UD84c//CFrr712u8tgFTOnGwAYckaNGpUkC3bevO+++/Ke97wn22yzTUaOHJlXvepV+fCHP5xnn332Zdf+9Kc/zf7775+enp6su+662WWXXfK1r31tqfeaN29eJk6cmFGjRuXGG29c0H7FFVdkhx12yIgRI7LzzjvnBz/4Qd70pjflTW9604JzbrrpppRSctVVV+VDH/pQRo8enVe+8pULjl933XXZc889M3LkyPT09OTtb3977rnnnkXuP3bs2Lz3ve99WV2llJx22mkL3p922mkppeTee+/NIYcckvXWWy9bb711Tj/99Lz00kuLXHv77bdn7733zogRIzJmzJicccYZppU0zEg3ANDx5s2bl7lz52bevHl54IEH8pnPfCabbLLJgoA7Y8aMbLHFFjnvvPOy4YYb5oEHHsiZZ56Zgw8+OL/4xS8W9HPNNdfkHe94R/baa69ccskl2XjjjXP33XfnoYceWuJ9f//73+eYY47JL37xi9x000157WtfmyS54YYbcuyxx+bQQw/NF77whTz11FOZNGlSXnzxxWy33XYv6+fEE0/MQQcdlG9961t58cUXk/QF7kMOOST77rtv/vmf/znPP/98Pve5z+UNb3hD7rjjjowZM2ZQz+rwww/P+973vnzsYx/L1KlTc+qpp2bLLbdcMDf+qaeeyr777ptNN9003/jGN7L22mvnnHPOycMPPzyo+7FihG4AoOPtsMMOi7zffPPN88Mf/nDBiPc+++yTffbZZ8HxP//zP8+4ceOy99575/bbb89uu+2WWmtOOumk7Lrrrvn3f//3rLFG3w/899tvvyXe89lnn82hhx6aGTNm5Oc///mC6S1Jcuqpp2bHHXfM1VdfvWDt5p133jm77777EkP3Hnvska9+9auLtJ1yyil51atelWuvvTbDh/dFsj333DPbbbddvvCFL+SLX/ziQB9TkuQTn/jEgoC933775Sc/+UmuuOKKBW3nnntuXnjhhVx//fXZaqutkiT7779/tt5660HdjxVjegkA0PGuvvrq3HLLLfmv//qv/Mu//Et23HHHHHzwwZk2bVqSZPbs2TnzzDOzww47ZOTIkVlzzTWz9957J8mC6Rr33HNPHnrooXzwgx9cELiXZsaMGdl7773z/PPPvyxwz5s3L7feemve8Y53LLJZymtf+9pss802S+zv8MMPX+T9Cy+8kF/96ld517vetSBwJ8k222yTvfbaKz/96U8H8HQWdcghhyzyfqeddlpkFPsXv/hFXv/61y8I3Emy7rrrZsKECYO+J8tnpBsA6Hg77bTTIsH3gAMOyJZbbpnTTjst//zP/5yTTz45F154YT73uc/lz//8z7P++utn+vTpOeKIIxZM53j66aeTJFtsscVy7/frX/86Tz/9dM4666xsuummixx76qmnMmfOnGyyySYvu27h+doL22yzzRZ5/+yzz6bW+rL2JNl0002XOt1lRbziFa9Y5P3aa6+94BkkyWOPPZaddtrpZdctrXZaQ+gGAIac+R+W/PWvf50k+c53vpO/+Iu/yCmnnLLgnOeff36RazbeeOMkyaOPPrrc/g888MDssssu+dSnPpURI0bkpJNOWqSfNddcMzNnznzZdU888cQiI8jzLb59+IYbbphSSh5//PGXnfv4449no402WvB+xIgRmT179iLnPPPMM8v9GpZms802yxNPPPGy9iW10TqmlwAAQ86sWbNy//33Z/To0Qver7nmmouc8/Wvf32R99ttt13Gjh2br371qyu0Usdf//Vf54tf/GImTZqUc889d0H7sGHDMn78+Hz/+99fpJ/bbrstDz744ArVv+6662b33XfPlVdemXnz5i1of+ihh3LzzTfnjW9844K2rbfeOnfdddci1//whz9cofssyZ577plf/vKXeeSRRxa0vfDCC5k6deqg+2T5jHQDtEETuwMOdsc/GAruuOOOPPXUU6m15rHHHstFF12UZ555JieeeGKSvpHpb3zjG9l5550zbty4XHXVVbn55psX6aOUkvPOOy9HHHFE9t133xx//PEZPXp0pk2blpkzZ2byEjbw+djHPpZhw4Zl0qRJmTdvXj75yU8mSSZPnpwDDjgghx9+eCZOnJinnnoqp512WjbddNPlzhef74wzzsghhxySt73tbfnIRz6S559/Pqeeemp6enryiU98YsF5Rx99dN7//vfnYx/7WN72trflzjvvzGWXXTbIJ9n3Nf3jP/5jDjjggJx22mkLVi8ZOXLkoPtk+YRugDZoYnfAld3xDzrZUUcdteDXo0ePzk477ZTrrrsub33rW5MkF154YWqt+exnP5skOfjgg3PFFVdkjz32WKSfww47LDfccEPOOOOMfOADH0iSvPrVr86kZfyD9aMf/WiGDRuWE088MS+99FI+9alPZf/998/ll1+eyZMn5/DDD8+4cePyhS98IaeffvoK75B54IEH5l//9V8zefLkvPOd78xaa62VN73pTTn77LOz+eabLzjvuOOOyyOPPJKvfe1rueSSS7L33nvn6quvXmSO+0BsvPHGufHGG3PSSSfluOOOy0YbbZTjjz8+c+fOzemnnz6oPlm+sjoshD5+/Ph66623trsMYAhr9ch0E6PSvZMnt3Wbb9pv2rRpec1rXrPEY038dGUgVoefxEyfPj3jxo3LZz/72fzt3/5tu8uhIcv6fZYkpZTbaq3jF28fciPdpZTXJDkpycZJbqy1XtzmkoDVQBMj07AqdXvgXdV+//vf5+Mf/3j222+/bLzxxnnggQdy9tlnZ5111skHP/jBdpdHB+qI0F1KuTTJ25LMrLXutFD7gUnOTzIsyVdrrWfVWqclOb6UskaSr7SlYABgtTZs2LA8/vjjOeGEE/L0009n3XXXzd57750rr7xyicsAQkeE7iSXJbkoyTfnN5RShiX5UpL9k0xPcksp5Qe11v8upRya5G/6rwEAWKXWWmutXH311e0ugyGkI0J3rfU/SiljF2veI8l9tdYHkqSU8p0khyX571rrD5L8oJTyr0n+aVXWCtCpSk9PR3+YcnWY0wuwNB0RupdiTJJHFno/PcmflVLelOSIJGsn+dHSLi6lTEwyMckSF6kH6DadHmg7+R8EAE3r5NBdltBWa603JblpeRfXWqckmZL0rV7S0soAAGAAOjl0T0+y5ULvt0gyo021AENME0v8sXJaPf3FdBVgKOnk0H1Lkm1LKdskeTTJ0Une3d6SgKHCEn+dp4l1yQGGio4I3aWUK5K8KcnGpZTpSU6ttX6tlHJCkuvTt2TgpbXWu9tYJtCv3ZtsrAgj0wB0ko4I3bXWY5bS/qMs48OSQHsYRaYTNLFaiykrQFM6InQDwEA1EY5NWQGaska7C2hSKWVCKWVKb4f/GBwAoJ3GjBmTu+/+4yze97///dlmm20WvJ89e3Y233zz/PrXv25HeY2YPXt2Jk2alG233TZ/8id/koMPPrjR+3X1SHetdWqSqePHj/9Qu2sBAOhUG2ywQeYPUj7++OP5yU9+khdffHHB8X/6p3/KzjvvnD/90z9tV4kt95nPfCazZ8/OPffckzXWWCOPPfZYo/fr6pFuAICVceGFF6aUkhkzFl21+M4770wpZYkhdPLkyRk+fHgeeuihlb5+VVk4dF900UWZOHHiIsfPPffcfPKTn2zk3tOnT8+JJ56YPffcM+uss05KKfntb3/byL3mmzVrVqZMmZKzzjora6zRF4c322yzRu/Z1SPdQB9rVgMMzgYbbJAk6e3tzeabb76g/fzzz1/QvrA5c+bkkksuyWGHHZatt956pa9fVTbYYIP87ne/y6xZs/Ltb387t99+e772ta/lf//3f/PLX/4ySbL//vs3cu/77rsv3/3ud7P77rtn7733zo9//ONG7rP4PTfccMOcddZZueGGG7Lmmmvm05/+dA477LDG7il0w2rAaiPAdffPzKy589p2/3WGD8uBr96kbfcfrJ7+QYbnnntuQdtTTz2VK664Ivvss0/uvPPORc7//ve/n8ceeywnnXRSS65fVeaPdF922WV5+9vfng033DCjRo1Kb29vvvjFLzY2yp0k++yzT5544okkyVe/+tWWhO7Xvva1efjhh5d47Pbbb8+cOXPy8MMP59WvfnXOPPPM/OY3v8nee++dnXbaKa9+9atX+v5LInQDwGpg1tx5OWL7Zn98vixX3TO4+bLveMc78rOf/SyXXnppDjnkkEWOvetd78qdd96Z17zmNbn55puXec6vf/3rrLXWWku8x9y5c3PmmWfm4x//eNZbb71Fji0pNE+ZMiXrrbdePvzhD+fYY49NrTWllCR901F22WWX7LPPPi25fmXdcMMNOf/883P77bfn6aefzujRo/Oud70r//AP/5A111xzwXkbbLBBnn322XzrW9/Ktddeu6D2m2++OXfddVeOPvroltSzJPOnd6yIWmsuvfTSXHLJJbnrrrvyile8IkcddVTOPPPMjBw5csF5v/rVr5bZz8iRI1NKyXve854kyQ477JBdd901t99+e2Oh25xuAKBjffrTn86GG26YL3zhC4u033jjjfnud7+biy66KCeffPJyz1la4E6SRx99NBdffHHe+ta3LhKOkz9OL5nfPnfu3Fx88cX5y7/8y2yyySZ56aWX8vzzzyfpC3o333zzIqPUK3v9yrrzzjvzlre8JV/5yldy/fXX5+Mf/3i+/OUv55xzznnZ13n55Zdn1113XTCtZdSoUTn99NPz0Y9+dJGAvrBaa+bOnbvc17x5rfkpywc/+MH81V/9Vfbbb79cc801Ofnkk3PppZfm4x//+ID62XjjjfPWt7411113XZLksccey1133ZWdd965JXUuiZFu6EDmYAP02WOPPXLsscfmwgsvXNA2Z86cnHDCCTnqqKOy3377JckKnbM0W2+9dX7yk59k3333zQEHHJDrr79+wQj14iPVV111VZ544ol85CMfWfDhyOeeey7rr79+LrzwwowePTrHHPPHPf9W9vqVtfC0kHnz5mWvvfbKTTfdlJ///OeLnLfhhhvm7rvvzje/+c1Fan/44Ydf9qHKhf30pz/Nm9/85uXW8cY3vjE33XTTwL+AhXzzm9/MpZdemu9///s54ogjkvTNM3/xxRfzmc98JhdddFGGDRu2wv1dfPHF+cAHPpDPfvazWWONNfL5z38+22+//UrVuCxCN3Qgc7ChPVq9y6UdLltjxx13zJNPPpmnn346G220Ub74xS9m+vTpueGGG1bonKeeeiqjR49eoXs9/vjj+cxnPpMvfelLSV4emi+44IIcddRR2XzzzReMUPf29mbttdfOd77znXziE5/IiBEjFvS3stevjLlz5+bb3/52Lrnkktx77715+umnFxx75zvfuci5n/zkJ182b/tb3/rWcu+x++6755Zbblnueeuvv/4KVr10f/d3f5d99tknhx56aObOnbugfccdd8zs2bMzY8aMbLnllivc39ixY3PjjTeudF0rqqtDdyllQpIJ48aNa3cpAAwBrQ7IdrhsjR122CFJMm3atIwdOzZnnHFGTj311GyxxRYrdM68efMybdq0Zd7jqaeeylFHHZW11157kfDZ09OTUkqee+65/OpXv8rPf/7znHvuuUn6pl8kfYH6mmuuybx58/KRj3xkkX5X9vqV8e53vzvXXnttTjjhhJxyyinZeOON8+KLL+bNb35zdtppp5bcY7311suuu+663PPmz1kfrAcffDD33ntv7r333qVOdenp8J/qdnXotjkOAO1k5Lw1tt122wwfPjzTpk3L+eefn6222iqTFnsOyzpn2LBhC0L5kjz99NN517velREjRuTf//3fM3bs2AXH1lhjjay33np57rnncv7552fPPffM6173uiR/DM3PPPNMLr744hx55JGLLAvYiusH64477siVV16Zyy+/PO9+97sXtH/ve99LrTW77bZbS+6zqqaXPProo0mSr3/960v8B8Maa6yx4Hl2qq4O3QDQTkbOW2OttdbKq171qkyZMiW33nprfvKTn7xstHNFzlmaYcOGZbvttsvnP//5Ja6N3dPTk3vvvTfXXXfdInOe11lnnQwfPjzf+MY38sgjj+SjH/3oEvtf2esHY/5yeQvPUX7hhRdyyimnJOlbUm+++++/P8cdd1xmzpyZddddN1/5ylcyfvz4FbrPqppeMmbMmCTJ2muvvcK1dRqhGwDoeK95zWtyzTXX5Oijj17qyOqKnLMkG2ywQa688splHp86dWrGjBmz4AN8862//vr53ve+lz322COvf/3rG7l+MHbbbbestdZa+eu//uucfPLJefzxx3P22Wdn9uzZ2WSTTRYZUT/++OPz3ve+Nx/84Adzww035Nhjj81vfvObFZoSsv7667ckBH/ve99Lktx2221JkmuvvTajR4/O6NGj88Y3vjFjx47Nm9/85px00kmZOXNmdtlll8yaNSsPPvhgbrjhhlx11VUDWnqwHYRuAKDjjRs3LiNGjHjZsoADPWcwenp68tJLL+Wv/uqvMnz48Jcde/bZZ5c5Sr2y1w/Glltumcsvvzyf/vSnc+ihh2aXXXbJ2WefnXPPPXeRcPrkk0/ml7/8ZX70ox8l+eOuk7fddtsqHVE+6qijFnk/f277/GkppZRceeWVmTx5cs4///zMmDEjPT092WGHHXLkkUd2fOBOhG4AYAh45JFHsttuuy1zzvOKnDMYP/vZz5Z67MEHH2z8+sE68sgjc+SRRy7SdtBBBy3y/uGHH87mm2++yFScrbfeOg8//PAqDd211uWes9FGG+WCCy7IBRdcsAoqaj2hGwBWA+sMHzboXSFbdf+Vcdttt70sMA7mHJZvRQIwAyd0A8AQsbzVUF464IDM7d9wZXH7jVxic8qwYRn2yle2orzG9Pb25oEHHljkw3+DOYeX22qrrTJjxozMmTNnwWj3Qw89lK222qrNlXUfoRsAhojlrYYyY9q0DB/g1IqlhfROMn9O9Mqew8uNHj06e+yxRy677LJ86EMfyg033JBaa3bfffd2l9Z1ujp02xyHVaHVW7Yntm0HYNX58pe/nOOOOy7nnHNO1llnnVx++eUrvZkNL9fVodvmOKwKtmwHYCjbdtttc/PNN7e7jK7X+eurAADAENfVI90AwLKVYcNaOq97KHwwE9pB6AaA1VirA/JQ+GAmtIPQzWqn1R989KFHAGB5hG5WOz74CACsaj5ICQBdxG6C0JyV+f0ldANAl1hzzTXz+9//vt1lQNf6/e9/n7XXXntQ1wrdANAlNtlkkzz66KOZNWuWEW9okVpr5syZk2eeeSbTp0/PRhttNKh+zOkGgC4xatSoJMmMGTMyZ86cttTw0u9+lzVavEsvtNvw4cMzYsSIbLXVVhkxYsTg+mhxTR3FNvAArG5GjRq1IHy3Q+/kyT6sDkvQ1dNLaq1Ta60TeyzpBgBAG3V16AYAgE4gdAMAQMOEbgAAaJjQDQAADevq1UsAgFWr9PSkd/LklvY3atKklvUH7SJ001LPnXdeaoevz1qsZgPQmFYH5FYGeGgnoZuWqr291mcFAFiMOd0AANAwoRsAABomdAMAQMPM6QYAOpbVUOgWXR26SykTkkwYN25cu0sBAAbBaih0i66eXlJrnVprndhjiTgAANqoq0M3AAB0AqEbAAAaJnQDAEDDhG4AAGiY0A0AAA0TugEAoGFCNwAANKyrN8dh+Z4777zU3t6W9VesiQ4ADNB198/MrLnzWtbfOsOH5cBXb9Ky/lpB6F7N1d7e9Jx6arvLAIBVotXbys/v09byK2fW3Hk5YvvNWtbfVfc81rK+WkXoBgBWG02EY1vLsyLM6QYAgIYJ3QAA0DChGwAAGtbVobuUMqGUMqW3hatzAADAQHV16K61Tq21TuyxjB0AAG3U1aEbAAA6gdANAAANE7oBAKBhQjcAADRM6AYAgIYJ3QAA0DChGwAAGiZ0AwBAw4RuAABomNANAAANE7oBAKBhQjcAADRseLsLYGCeO++81N7elvVXenpa1hcAAEsmdA8xtbc3Paee2u4yAAAYANNLAACgYUI3AAA0TOgGAICGCd0AANAwoRsAABrW1aG7lDKhlDKlt4VL7AEAwEB1deiutU6ttU7ssRY1AABtZJ3uBrV6I5vEZjYAAEOR0N0gG9kAAJB0+fQSAADoBEI3AAA0zPQSAICVUHp60jt5ckv7GzVpUsv6ozMI3QAAK6HVAbmVAZ7OYXoJAAA0TOgGAICGCd0AANAwoRsAABomdAMAQMOEbgAAaJjQDQAADRO6AQCgYUI3AAA0TOgGAICGCd0AANAwoRsAABomdAMAQMOEbgAAaJjQDQAADRO6AQCgYUI3AAA0TOgGAICGCd0AANAwoRsAABrW1aG7lDKhlDKlt7e33aUAALAa6+rQXWudWmud2NPT0+5SAABYjXV16AYAgE4gdAMAQMOEbgAAaJjQDQAADRO6AQCgYUI3AAA0TOgGAICGCd0AANAwoRsAABomdAMAQMOEbgAAaJjQDQAADRO6AQCgYUI3AAA0TOgGAICGCd0AANAwoRsAABomdAMAQMOEbgAAaJjQDQAADRO6AQCgYUI3AAA0TOgGAICGCd0AANAwoRsAABomdAMAQMOEbgAAaJjQDQAADRO6AQCgYUI3AAA0TOgGAICGCd0AANAwoRsAABomdAMAQMOEbgAAaJjQDQAADRO6AQCgYUI3AAA0TOgGAICGCd0AANAwoRsAABomdAMAQMOEbgAAaNiQDN2llLeXUr5SSrmmlHJAu+sBAIBlGd7uAuYrpVya5G1JZtZad1qo/cAk5ycZluSrtdazaq3/kuRfSikbJvl8kh+3o2YAgFYrPT3pnTy5pf2NmjSpZf0xOB0TupNcluSiJN+c31BKGZbkS0n2TzI9yS2llB/UWv+7/5RT+o8DAHSFVgfkVgZ4Bq9jppfUWv8jyTOLNe+R5L5a6wO11tlJvpPksNLn/ya5ttb6q1VdKwAADETHhO6lGJPkkYXeT+9vOzHJfkmOLKUcv6QLSykTSym3llJuffLJJ5uvFAAAlqKTppcsSVlCW621XpDkgmVdWGudkmRKkowfP742UBsAAKyQTh/pnp5ky4Xeb5FkRptqAQCAQen00H1Lkm1LKduUUtZKcnSSH7S5JgAAGJCOCd2llCuS/CLJ9qWU6aWUD9Ra5yY5Icn1SaYl+W6t9e521gkAAAPVMXO6a63HLKX9R0l+tIrLAQCAlumYkW4AAOhWQjcAADSsq0N3KWVCKWVKb29vu0sBAGA11tWhu9Y6tdY6saenp92lAACwGuvq0A0AAJ1A6AYAgIYJ3QAA0DChGwAAGiZ0AwBAw4RuAABoWFeHbut0AwDQCbo6dFunGwCATtDVoRsAADqB0A0AAA0TugEAoGFCNwAANEzoBgCAhgndAADQMKEbAAAa1tWh2+Y4AAB0gq4O3TbHAQCgE3R16AYAgE4gdAMAQMOEbgAAaJjQDQAADRO6AQCgYUI3AAA0TOgGAICGCd0AANAwoRsAABrW1aHbNvAAAHSCrg7dtoEHAKATdHXoBgCATiB0AwBAw4RuAABomNANAAANE7oBAKBhQjcAADRM6AYAgIYNb2VnpZRtk/xpkodqrbe2sm8AABiqBjzSXUo5opTyo1LKny3WfkqSaUm+m+Q/SynfblGNAAAwpA1mesn/SbJPkv83v6GUslOS05O8lOTnSX6X5JhSyhGtKBIAAIaywYTu3ZLcWWudtVDb/0lSk3yw1rpPktclmZPkQytfIgAADG2DCd0bJXl0sbY3Jnk+yT8lSa31gSQ/S/KalapuJZVSJpRSpvT29razDAAAVnODCd1rJynz35RS1kqya5Jf1FrnLnTe40leuXLlrZxa69Ra68Senp52lgEAwGpuMKH7sSQ7LvR+n/QF8Z8vdt56SZ4bZF0AANA1BrNk4E+T/J9SyqeSXJfkjPTN575usfN2SjJ95coDAKDTXHf/zMyaO69l/a0zfFjL+upUgwndf5/k7Un+of9VkvxbrfWW+SeUUrZL8qokX25FkQAAdI5Zc+fliO03a3cZQ8qAQ3et9X9KKXsl+XiSTZL8V5SER5UAAB3DSURBVJJzFjvtLUnuTPLDla4QAACGuEHtSFlrvSvJ+5dx/OIkFw+2KAAA6CaD2ZHyc6WUQ1fgvLeVUj43uLIAAKB7DGb1ktPSN6d7eQ5Lcuog+gcAgK4ymNC9ooalb1UTAABYrTUZul8d63QDAMCKfZByCXOzd13GfO3h6dv+/Q1Jbhp8aQAA0B1WdPWS09I3VWT+9u+79r+WZVaS0wdXFgAAdI8VDd2n54+h+3NJ7khyzVLOnZ3k0STX11qfWOkKAQBgiFuh0F1rPW3+r/unldxRa53cVFEAANBNBrMjZZMfvgQAgK7T1QG6lDKhlDKlt7e33aUAALAaG9Q28ElSShmT5M1JNk8yYimn1VrrGYO9x8qqtU5NMnX8+PEfalcNAAAw4NBdSilJzkvykfxxpLwsdtr8D13WJG0L3QAA0AkGM9L910lOTPJSkuuS/CY2wQEAgKUaTOh+X5I5Sd5Sa/1Zi+sBAICuM5gPUm6T5GcCNwAArJjBhO7fJbHpDQAArKDBhO6fJHldqwsBAIBuNZjQ/bdJRpdS/rbVxQAAQDcazAcp90ry9SSnlVIOTnJtkofTt5rJy9Ravzn48gAAYOgbTOi+LH9ch/vPkuyxnPOFbgAAVmuDCd3fTF/oBgAAVsCAQ3et9b0N1AEAAF1rMB+kBAAABmAw00sWUUoZl2R0kqdrrf+z8iUBAEB3GdRIdylleCnlc6WUJ5Lck+RnSf5moePvLaXcXErZqUV1AgDAkDXgke5SyvAkP0ryliRzk0xLsuNip92a5NIk70hy10rWCADAIJWenvROntzaTo+e2Nr+VgODmV5yQpL9kvxbkuNqrY+VUhZZo7vWelcp5bdJDkjS4v/LAACsqFGTJrW+03sea32fXW4wofs9SZ5O8s5a6++Wcd6DSbYdVFUAANBFBjOne/sk/7mcwJ0kT6TvA5YAALBaG0zorlnKlu+L2TTJi4Pov2VKKRNKKVN6e3vbWQYAAKu5wYTuB5PsUkpZ6rWllJFJ/jR9H7Jsm1rr1FrrxJ6ennaWAQDAam4wofsHSbZI8sllnPPpJBsmuWYwRQEAQDcZzAcpv5jkfUn+oZSya5Lv9bdvXEo5KMlRSY5L8nCSf2xJlQAAMIQNOHTXWp8ppRyYvlHso5O8K33zvA/pf5UkjySZUGv93xbWCgAAQ9KgtoGvtf6/UsqO6RvxPijJq5IMS1/YvjbJlFrrCy2rEgAAhrBBhe4kqbW+mOTi/hcAALAUA/4gZSllgyYKAQCAbjWY1UseK6V8p5RyYCmltLwiAADoMoMJ3WskeWeSf00yvZRyVinlNa0tCwAAusdgQvemSU5McluSzZJ8KsldpZRfllKON/0EAAAWNeDQXWt9ttb6pVrrHkn+JMk5SR5LskeSL+WP008OMv0EAAAGN9K9QK11Wq3100m2TN/Sgd9N8lL6pp/8MH1LCAIAwGptpUL3fLXP9bXWY9I35eSC9G2Ss1kr+gcAgKFs0Ot0L66UsmGSd6dvC/jdW9UvAAAMdSsVukspayQ5OH1B+21J1soft4H/VpLLVrI+AAAY8gYVukspO6cvaB+bZJP0Be3fJ/lO+oL2v9Vaa4tqBACAIW3AobuUcluSXdMXtJPkl+kL2t+ptT7XutIAAKA7DGake7ckjyb5dpKv11r/p7UlAQBAdxlM6D4oyY9NHwEAgBWz3CUDSyn7lFK2m/++f2nA5QbuUsp+pZSPrmyBAAAw1K3IOt03Jfn0kg6UUp4ppVy4lOuOTXLuIOsCAICusaLTS5a2nfsGSdZtUS0AALTYdffPzKy581ra54gX/jf2QByYlm2OAwBA55k1d16O2L61Abl38pTktae2tM9u15Jt4AEAgKUTugEAoGFCNwAANEzoBgCAhq3oByk3LaXsM8Bjmw6yJgAA6CorGrrf2v9aXF3GsbYrpUxIMmHcuHHtLgUAYIW0eom/dYYPa1lfDN6KhO6H0xeuh5xa69QkU8ePH/+hdtcCALAimljij/ZbbuiutY5dBXUAAEDX8kFKAABomNANAAANE7oBAKBhQjcAADRM6AYAgIYJ3QAA0DChGwAAGiZ0AwBAw4RuAABomNANAAANE7oBAKBhQjcAADRM6AYAgIYJ3QAA0DChGwAAGiZ0AwBAw4RuAABomNANAAANE7oBAKBhQjcAADRM6AYAgIYJ3QAA0DChGwAAGiZ0AwBAw4RuAABomNANAAANE7oBAKBhQjcAADRM6AYAgIYJ3QAA0DChGwAAGiZ0AwBAw4RuAABomNANAAANE7oBAKBhQjcAADRM6AYAgIYJ3QAA0DChGwAAGiZ0AwBAw4RuAABomNANAAANG3Khu5TyqlLK10op32t3LQAAsCI6InSXUi4tpcwspdy1WPuBpZR7Sin3lVL+JklqrQ/UWj/QnkoBAGDgOiJ0J7ksyYELN5RShiX5UpKDkuyY5JhSyo6rvjQAAFg5HRG6a63/keSZxZr3SHJf/8j27CTfSXLYKi8OAABWUkeE7qUYk+SRhd5PTzKmlLJRKeXLSXYrpZy8tItLKRNLKbeWUm598sknm64VAACWani7C1iGsoS2Wmt9Osnxy7u41jolyZQkGT9+fG1xbQAAsMI6eaR7epItF3q/RZIZbaoFAAAGrZND9y1Jti2lbFNKWSvJ0Ul+0OaaAABgwDoidJdSrkjyiyTbl1Kml1I+UGudm+SEJNcnmZbku7XWu9tZJwAADEZHzOmutR6zlPYfJfnRKi4HAABaqiNGugEAoJt1degupUwopUzp7e1tdykAAKzGujp011qn1lon9vT0tLsUAABWY10dugEAoBMI3QAA0DChGwAAGiZ0AwBAw4RuAABomNANAAAN64gdKZtSSpmQZMK4cePaXQoA0KWuu39mZs2d17L+1hk+rGV9NaX09KR38uSW9jdq0qSW9deJujp011qnJpk6fvz4D7W7FgCgO82aOy9HbL9Zu8tYpVodkFsZ4DuV6SUAANAwoRsAABomdAMAQMOEbgAAaJjQDQAADRO6AQCgYUI3AAA0TOgGAICGdXXoLqVMKKVM6e3tbXcpAACsxro6dNdap9ZaJ/b09LS7FAAAVmNdHboBAKATCN0AANAwoRsAABomdAMAQMOEbgAAaJjQDQAADRO6AQCgYUI3AAA0TOgGAICGdXXotg08AACdoKtDt23gAQDoBF0dugEAoBMI3QAA0DChGwAAGiZ0AwBAw4RuAABomNANAAANE7oBAKBhQjcAADRM6AYAgIYJ3QAA0DChGwAAGtbVobuUMqGUMqW3t7fdpQAAsBrr6tBda51aa53Y09PT7lIAAFiNdXXoBgCATiB0AwBAw4RuAABomNANAAANE7oBAKBhQjcAADRM6AYAgIYJ3QAA0DChGwAAGiZ0AwBAw4RuAABomNANAAANE7oBAKBhw9tdQJNKKROSTBg3bly7SwEAOsB198/MrLnzWtrnOsOHtbQ/ulNXh+5a69QkU8ePH/+hdtcCALTfrLnzcsT2m7W7DFZDppcAAEDDhG4AAGiY0A0AAA0TugEAoGFCNwAANEzoBgCAhgndAADQMKEbAAAaJnQDAEDDhG4AAGiY0A0AAA0TugEAoGFCNwAANEzoBgCAhgndAADQMKEbAAAaJnQDAEDDhG4AAGiY0A0AAA3r6tBdSplQSpnS29vb7lIAAFiNdXXorrVOrbVO7OnpaXcpAACsxro6dAMAQCcQugEAoGFCNwAANEzoBgCAhgndAADQMKEbAAAaJnQDAEDDhG4AAGiY0A0AAA0TugEAoGFCNwAANEzoBgCAhgndAADQMKEbAAAaJnQDAEDDhG4AAGiY0A0AAA0TugEAoGFCNwAANEzoBgCAhgndAADQMKEbAAAaJnQDAEDDhG4AAGiY0A0AAA0TugEAoGFdHbpLKRNKKVN6e3vbXQoAAKuxrg7dtdaptdaJPT097S4FAIDVWFeHbgAA6ARCNwAANEzoBgCAhgndAADQMKEbAAAaJnQDAEDDhG4AAGiY0A0AAA0TugEAoGFCNwAANEzoBgCAhgndAADQMKEbAAAaJnQDAEDDhG4AAGiY0A0AAA0TugEAoGFCNwAANEzoBgCAhgndAADQMKEbAAAaJnQDAEDDhG4AAGiY0A0AAA0TugEAoGFCNwAANEzoBgCAhgndAADQMKEbAAAaJnQDAEDDhG4AAGiY0A0AAA0TugEAoGFCNwAANEzoBgCAhgndAADQMKEbAAAaJnQDAEDDhG4AAGiY0A0AAA0TugEAoGFCNwAANEzoBgCAhg1vdwEDVUpZN8k/Jpmd5KZa6+VtLgkAAJapI0a6SymXllJmllLuWqz9wFLKPaWU+0opf9PffESS79VaP5Tk0FVeLAAADFBHhO4klyU5cOGGUsqwJF9KclCSHZMcU0rZMckWSR7pP23eKqwRAAAGpSNCd631P5I8s1jzHknuq7U+UGudneQ7SQ5LMj19wTvpkPoBAGBZOnlO95j8cUQ76Qvbf5bkgiQXlVIOSTJ1aReXUiYmmZgkW221VYNlAsCSXXf/zMya27ofyq4zfFgOfPUmLetvKGjiGdJ5Sk9PeidPbl2HR09sXV8t0smhuyyhrdZaX0jyvuVdXGudkmRKkowfP762uDYAWK5Zc+fliO03a1l/V93zWMv6Gipa/QzpTKMmTWpthx34e6WTp2dMT7LlQu+3SDKjTbUAAMCgdXLoviXJtqWUbUopayU5OskP2lwTAAAMWEeE7lLKFUl+kWT7Usr0UsoHaq1zk5yQ5Pok05J8t9Z6dzvrBACAweiIOd211mOW0v6jJD9axeUAAEBLdcRINwAAdDOhGwAAGtbVobuUMqGUMqW3t7fdpQAAsBrr6tBda51aa53Y09PT7lIAAFiNdXXoBgCATiB0AwBAw4RuAABomNANAAANE7oBAKBhQjcAADSsq0O3dboBAOgEXR26rdMNAEAn6OrQDQAAnUDoBgCAhgndAADQMKEbAAAaJnQDAEDDhG4AAGiY0A0AAA0rtdZ219C4UsqTSR5qw603TvJUG+47lHlmA+N5DYznNTCe18B4XgPjeQ2M5zUw7XxeW9daRy/euFqE7nYppdxaax3f7jqGEs9sYDyvgfG8BsbzGhjPa2A8r4HxvAamE5+X6SUAANAwoRsAABomdDdrSrsLGII8s4HxvAbG8xoYz2tgPK+B8bwGxvMamI57XuZ0AwBAw4x0AwBAw4TuhpVSdi2l/LKUckcp5dZSyh7trqmTlVL+uf9Z3VFK+W0p5Y5219TpSiknllLuKaXcXUo5u931dLJSymmllEcX+h47uN01DRWllE+WUmopZeN219LJSilnlFJ+3f/99eNSyubtrqmTlVLOKaX8pv+ZXV1K2aDdNXWyUspR/X/Wv1RK6aiVOTpJKeXA/r8X7yul/E2765nP9JKGlVJ+nOTcWuu1/X/Bf6rW+qY2lzUklFK+kKS31np6u2vpVKWUNyf5bJJDaq1/KKVsUmud2e66OlUp5bQkz9daP9/uWoaSUsqWSb6aZIcku9darRW8FKWUUbXW5/p//dEkO9Zaj29zWR2rlHJAkp/UWueWUv5vktRaP93msjpWKeU1SV5KckmST9Zab21zSR2nlDIsyf8k2T/J9CS3JDmm1vrfbS0sRrpXhZpkVP+ve5LMaGMtQ0YppSR5Z5Ir2l1Lh/twkrNqrX9IEoGbhpyb5FPp+/OMZZgfuPutG89smWqtP661zu1/+8skW7Sznk5Xa51Wa72n3XV0uD2S3FdrfaDWOjvJd5Ic1uaakgjdq8KkJOeUUh5J8vkkJ7e5nqFi7yRP1FrvbXchHW67JHuXUv6zlPLTUsrr2l3QEHBC/4+yLy2lbNjuYjpdKeXQJI/WWu9sdy1DRSnl7/v/zD82yefaXc8Q8v4k17a7CIa8MUkeWej99P62thve7gK6QSnl35JsuoRDn03yliQfq7V+v5TyziRfS7Lfqqyv0yzredVar+n/9TExyp1kud9fw5NsmOT1SV6X5LullFfV1Xje2HKe18VJzkjf6OMZSb6Qvr/oV2vLeWafSXLAqq2osy3vz7Ba62eTfLaUcnKSE5KcukoL7DAr8md+KeWzSeYmuXxV1taJVvDvSJauLKGtI/5ONKe7YaWU3iQb1Fpr/5SJ3lrrqOVdtzorpQxP8mj65o5Ob3c9nayUcl36ppfc1P/+/iSvr7U+2dbChoBSytgkP6y17tTmUjpWKWXnJDcmmdXftEX6psjtUWt9vG2FDRGllK2T/KvvsWUrpRyX5Pgkb6m1zlre+SSllJtiTvcSlVL2THJarfWt/e9PTpJa6z+0tbCYXrIqzEjyxv5f75vEdInl2y/JbwTuFfIv6fu+SilluyRrJfEht6UopWy20NvDk9zVrlqGglrr/6u1blJrHVtrHZu+H9O+VuBeulLKtgu9PTTJb9pVy1BQSjkwyaeTHCpw0yK3JNm2lLJNKWWtJEcn+UGba0piesmq8KEk5/eP3r6YZGKb6xkKjo6pJSvq0iSXllLuSjI7yXGr89SSFXB2KWXX9P2o8bdJ/rK95dCFziqlbJ++FSYeSt8ILkt3UZK1k9zQ98Pg/NJqL0tXSjk8yYVJRif511LKHfNHdOnTvxLOCUmuTzIsyaW11rvbXFYS00sAAKBxppcAAEDDhG4AAGiY0A0AAA0TugEAoGFCNwAANEzoBgCAhgndAADQMKEbGFJKKb8tpdT+bdy7Sinl9aWUl0opZy3h2Pyve+HXi6WUB0sp3+zf9GdV1lpLKatko4fB3mtp3ysDbW9CKWWdUspjpZRbSv+uMEB3E7oBOkB/8LogyXNJ/u8yTr0+yTf6Xz9OMiLJe5LcUko5uuk6V1etDuT9W57/fZLxSf6iFX0CnU3oBugMxyR5XZILaq3PLuO8s2qt7+1/HZrkVUkuTzI8yZRSyitWQa1DxVuSvCbJow2dv7KmJHk8yZmllLVW0T2BNhG6ATrDpCQ1yaUDuajW+vskH07yQpL1k7y19aUNTbXW+2utv6m1zmni/JVVa52d5NtJNk9y1Kq4J9A+QjfQNUopW5dS/rGU8kAp5Q+llGdLKf9eSnn3Mq7ZtZRyTSnlmVLKC6WU20op7+8/tkrmLZdSXpe+Ue6f1lp/O9Dra63/m+R/+t9uvVC/C+ovpXyglPKfpZTn+ts3WOi8AT+3xeqfWEq5vZQyq5TydCnlqlLKTks5989KKeeUUm4tpTxRSpldSplRSvleKeX1Lb7XgKaELOn8Usp7+5/h/Of64GLz6seWUub1f/+MXEq/a/bP366llB0XO/yN/v9+ZEVqBIYuoRvoCqWUP0tyR/pGfZPk6iS3JtkryeX9HzYsi12zb5JfJDk0yRNJfpC+OdVTSinnrKrak7y9/7//thJ9jOr/7x8WP1BKuTB9Uxn+kOSHSW5L36j6oJ7bYn2fm+TiJL1JrknyVJLDk/xnKeUNS7jk75N8LMmaSf4rfc/86STvSPKzUspSR3wHca9WuC99wfiF/vffzx/n1H8jyfNJpibZMH1ThJbkHUk2TXJTrfW/Fz5Qa70rfd97e5ZSRre8eqBz1Fq9vLy8hswryW/TFxjHLtQ2IsnD/e3nJhm20LGd0hdqapK/XKh9nSQz+tsnJykLHfvzJP/bf6yugq/p5/332ncFvu43LeHYrknm9R9/80Lttf/1uyR7LOG6AT+3JfT9QpJ9FmovSf6h/9jDSUYsdt2BSV65hP4mJJmdvgC+Tovu9bLvlcG0r8Cxt/Qfu3Up/+/+o//4kUs5fnX/8Xe2+/eXl5dXcy8j3UA3OCrJlkkeSvKpWuu8+Qdq30jiaf1vP7nQNUcm2Sx90zIm11rrQtfcnOQfl3SjUsq4UsqXSyl3lFLmllLuWlpRpZRtSynXlVKeL6U8WUq5sJSyzhJOnb/c37TlfqWL9r9hKeXQJFel7yeXdyT56RJOPbvW+l9LaB/Mc1vcxbXW/1jouprklCQP9Pf9joVPrrVeV2t9YvFOaq1Tk1yZ5BVJ3tyKe60qtdYbk/x3kt1LKXssfKx/6sve6fsH3r8spYv5o9+7NVYk0HZCN9AN3tj/38vrkj8E9/X0jSSOK6WMWeya/9/e/YVYVUVxHP8urRRLMyztj5BmWGJBhKW9RA+F9A8kxAozqSAj8cUKQyswoR6SHtSgTGpQ0yRSxKhMyqIHpQcryMrIQjKjfxqK/8JaPax9msOZc+6duXPv6Iy/Dwzbc87e9/wZkXW366y9zt3/LRmzpuJc44HbibSDryv6kHKmtxIvN04FHiPSD14r9DubmHWHmOWtZ2suV3s/kWYxGtgBTKm4l/UVn9XIcytaXdyRgve1afOm4nEzOz/lSi82sxVm1mZmbcTsOsDYZp2rBy1LbTE3e3Zql7v7iYqx+1M7oulXJSKnjDNO9gWIiDRBFhD+WHbQ3Y+Z2b7U7xKiJFw2Zk/FZ1bt3+TuGwFSoDihot8sIs/3Gnf/I/U/QeRJL3L3nanfuak97lHNop7NRJk5iBztfcCnwNb8bH0n76WR51ZUOpZIxwAYmd9pZrOAF2n/olFmSMX+Lp2rh60kUl3uNrO57r7fzAYD9wEniJz6KgdTO7RGHxHp5TTTLSJ9QfaiX61KI1UvA1aNKZsxpmImucxtwIdZwJ28TQTKt+b2/ZXaAWY2oBOfm6/TPcvdF7r7RzUCbjzKCpbpznPrrP8/28wmEC9Cngk8AVwJnAP0c/csP7s75+yRFTJLT+x+mPhfjIHAg2n3TOL+Nrj7LzWGZ18yatVnF5FeTkG3iPQFe1N7WdlBMxtI5G9D+2ztvtRe2nEEAKO6eU3jKKSfuPtxYDcRbGb7jtBeGaOnF7Zp5LkVjaqzf19u31QioF7i7ovdfZe7H859Ybi8zvV25Vwnw0vEl7VHzKwf7RVhXqozblhqf2vVhYnIyaegW0T6guzlwXvNrCxtbiYR7H3v7lnwmL2QNy0FSEVV5d866zzaZ7HzDtAxuN6R2mIN51Zr5LkVTS/uMLP+wN1p8+Pcoey+fyoZcwFwS53r7cq5mi1L/alMy3T33cB7wBjgOeL3udPdy15uzct+7ztq9hKRXk1Bt4j0BW8Rgdxo4Pl8EJ0WI1mYNhcXxvxKzDovyNeiTrWrZ9N9ZekOVrJ/a2pvaMI5u6KR51b0aL5GdnqOC4lZ65+JlJrMt6m938zOyY0ZTKRm1Mtp7sq5mi370jGuTr+lqZ2X2tIqOAWTiL8TH3f9skSkt1DQLSK9nrsfA6YRM8uPA9+Z2Voz2wx8TlSFWEXuZbaUgzuDyLF+FthpZmvM7COibvaK1LXRJcEPELPdRUPpmLublZK7ucFzNaSR51biVeCTtILlGqLs4QLgKDC9kE/+OhHkXwv8kFaT3EC8CDmBQmWXbp6r2Tak9o20euaK9DOs0O8DYFf68yHi+VUys6uJ57zN3X9v6hWLyClFQbeI9Anuvp2od/0y0B+4C5gIbCcqSMwsvmzo7luIhXA2EbnLU4hAeTaxWAzEqoeN+IbCrGh6UXIM7TO+2XV8nq7zxs4uWd4sjTy3grnAHCJ1ZAownPgSMbGYVuHuB4jgejmxkuPtaXs9EYh3SDtp9FwtsAx4mpjxvgN4KP0MzndKzypbWXSlux+q87kzU9uZGXER6cWs9r+lIiKnJzObQZSBe8fd76zo0wZMcPerSo7NI4K0S939z7TvHqKm9HgvLAeeO7bI3Z9p5r1IzzGzs4jVMUdQ8nsu6buHePlydCdLRopIL6WZbhE5bZnZcDPrUL3EzCYBL6TNtsKxQWY21cymEpVPhmTbhc96hUjb2Ghmk1MQv5RYjKcsEFsHfAbMMbOytBTpHWYTAff7tQLu5GHgQmC+Am6Rvk8z3SJy2jKzm4EtwFfEwit/E+XzsuW4V7n7/YUxo6hepOUBd2/L9R0LLCGWAT8KvEkst36k4nomAtuIZdufbOimpMeZ2RVE3fGLgcnAP8B17v5ljTGDiPKRe4Hr66TwiEgfoKBbRE5bZjYSmE8sh34RkZ97EPiCmOFerWBI6jGzm4gKNMeJ2uxPufu7J/WiROSUo6BbRERERKTFlNMtIiIiItJiCrpFRERERFpMQbeIiIiISIsp6BYRERERaTEF3SIiIiIiLaagW0RERESkxRR0i4iIiIi0mIJuEREREZEW+w8cv6lgZy8F0gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure( figsize=(12,10) )\n",
    "plt.hist( np.log10( y_test_proba[ y_test == 0 ] ), histtype='step', color='lightcoral', bins=30, range=(-8.,0.), label=\"Background\" )\n",
    "plt.hist( np.log10( y_test_proba[ y_test == 1 ] ), histtype='step', color='lightblue', bins=30, range=(-8.,0.), label=r\"$\\gamma\\gamma \\to WW \\quad a_0^W=1e^{-6}$\" )\n",
    "plt.yscale('log')\n",
    "plt.xlabel( r\"$\\log_{10}(\\rm{Probability})$\", fontsize=22 )\n",
    "plt.ylabel( \"Events\", fontsize=22 )\n",
    "plt.legend( loc='best', fontsize=16 )\n",
    "if save_figures:\n",
    "    plt.savefig( \"plots/ANN-Keras_Classifier_Probability_log_test-multiRP_2021_01_22-17_46_10.pdf\", bbox_inches='tight' )\n",
    "    plt.savefig( \"plots/ANN-Keras_Classifier_Probability_log_test-multiRP_2021_01_22-17_46_10.png\", bbox_inches='tight' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00000000e+00 0.00000000e+00 0.00000000e+00 5.00200080e-05\n",
      " 5.00200080e-05 1.00040016e-04 1.00040016e-04 1.50060024e-04\n",
      " 1.50060024e-04 2.50100040e-04 2.50100040e-04 3.00120048e-04\n",
      " 3.00120048e-04 3.50140056e-04 3.50140056e-04 4.00160064e-04\n",
      " 4.00160064e-04 4.50180072e-04 4.50180072e-04 5.00200080e-04\n",
      " 5.00200080e-04 6.50260104e-04 6.50260104e-04 7.50300120e-04\n",
      " 7.50300120e-04 8.50340136e-04 8.50340136e-04 1.00040016e-03\n",
      " 1.00040016e-03 1.10044018e-03 1.10044018e-03 1.20048019e-03\n",
      " 1.20048019e-03 1.25050020e-03 1.25050020e-03 1.40056022e-03\n",
      " 1.40056022e-03 1.45058023e-03 1.45058023e-03 1.60064026e-03\n",
      " 1.60064026e-03 1.65066026e-03 1.65066026e-03 1.75070028e-03\n",
      " 1.75070028e-03 1.95078031e-03 1.95078031e-03 2.00080032e-03\n",
      " 2.00080032e-03 2.05082033e-03 2.05082033e-03 2.35094038e-03\n",
      " 2.35094038e-03 2.65106042e-03 2.65106042e-03 2.70108043e-03\n",
      " 2.70108043e-03 2.85114046e-03 2.85114046e-03 2.95118047e-03\n",
      " 2.95118047e-03 3.00120048e-03 3.00120048e-03 3.05122049e-03\n",
      " 3.05122049e-03 3.20128051e-03 3.20128051e-03 3.30132053e-03\n",
      " 3.30132053e-03 3.70148059e-03 3.70148059e-03 4.35174070e-03\n",
      " 4.35174070e-03 5.15206082e-03 5.15206082e-03 6.25250100e-03\n",
      " 6.25250100e-03 6.90276110e-03 6.90276110e-03 7.20288115e-03\n",
      " 7.20288115e-03 8.30332133e-03 8.30332133e-03 9.35374150e-03\n",
      " 9.35374150e-03 9.40376150e-03 9.40376150e-03 1.05042017e-02\n",
      " 1.05042017e-02 1.10544218e-02 1.10544218e-02 1.15046018e-02\n",
      " 1.15046018e-02 1.56562625e-02 1.56562625e-02 3.38135254e-02\n",
      " 3.38135254e-02 3.59143657e-02 3.59143657e-02 3.65646259e-02\n",
      " 3.65646259e-02 3.95158063e-02 3.95158063e-02 4.93697479e-02\n",
      " 4.93697479e-02 1.03741497e-01 1.03941577e-01 1.18947579e-01\n",
      " 1.19047619e-01 1.21698679e-01 1.21798719e-01 1.29101641e-01\n",
      " 1.29201681e-01 1.30252101e-01 1.30352141e-01 1.33853541e-01\n",
      " 1.33953581e-01 1.47008804e-01 1.47108844e-01 1.49209684e-01\n",
      " 1.49309724e-01 1.54611845e-01 1.54711885e-01 1.59463786e-01\n",
      " 1.59563826e-01 1.69817927e-01 1.69917967e-01 1.75470188e-01\n",
      " 1.75570228e-01 1.78021208e-01 1.78121248e-01 1.80172069e-01\n",
      " 1.80272109e-01 1.81322529e-01 1.81322529e-01 1.82923169e-01\n",
      " 1.83023209e-01 1.89625850e-01 1.89725890e-01 1.97979192e-01\n",
      " 1.98079232e-01 1.98129252e-01 1.98229292e-01 2.01780712e-01\n",
      " 2.01880752e-01 2.05632253e-01 2.05732293e-01 2.06832733e-01\n",
      " 2.06932773e-01 2.08133253e-01 2.08233293e-01 2.09333733e-01\n",
      " 2.09433774e-01 2.10384154e-01 2.10484194e-01 2.15136054e-01\n",
      " 2.15236094e-01 2.16386555e-01 2.16486595e-01 2.16936775e-01\n",
      " 2.17086835e-01 2.19737895e-01 2.19837935e-01 2.23189276e-01\n",
      " 2.23289316e-01 2.24139656e-01 2.24239696e-01 2.24539816e-01\n",
      " 2.24639856e-01 2.28591437e-01 2.28691477e-01 2.33593437e-01\n",
      " 2.33693477e-01 2.34643858e-01 2.34743898e-01 2.35244098e-01\n",
      " 2.35344138e-01 2.36394558e-01 2.36494598e-01 2.37595038e-01\n",
      " 2.37695078e-01 2.39745898e-01 2.39845938e-01 2.41496599e-01\n",
      " 2.41596639e-01 2.46698679e-01 2.46798719e-01 2.46948780e-01\n",
      " 2.47148860e-01 2.47248900e-01 2.49749900e-01 2.49849940e-01\n",
      " 2.51200480e-01 2.51300520e-01 2.51700680e-01 2.51900760e-01\n",
      " 2.53301321e-01 2.53501401e-01 2.55652261e-01 2.55752301e-01\n",
      " 2.57653061e-01 2.57753101e-01 2.58753501e-01 2.58853541e-01\n",
      " 2.59103641e-01 2.59203681e-01 2.59453782e-01 2.59553822e-01\n",
      " 2.59653862e-01 2.59753902e-01 2.60904362e-01 2.61004402e-01\n",
      " 2.62454982e-01 2.62555022e-01 2.62605042e-01 2.62705082e-01\n",
      " 2.62955182e-01 2.63055222e-01 2.64555822e-01 2.64655862e-01\n",
      " 2.65656263e-01 2.65756303e-01 2.66706683e-01 2.66806723e-01\n",
      " 2.67306923e-01 2.67507003e-01 2.69357743e-01 2.69457783e-01\n",
      " 2.69957983e-01 2.70058023e-01 2.70508203e-01 2.70608243e-01\n",
      " 2.71258503e-01 2.71458583e-01 2.72108844e-01 2.72208884e-01\n",
      " 2.72909164e-01 2.73009204e-01 2.75010004e-01 2.75110044e-01\n",
      " 2.75660264e-01 2.75760304e-01 2.75860344e-01 2.75960384e-01\n",
      " 2.76860744e-01 2.76960784e-01 2.77761104e-01 2.77861144e-01\n",
      " 2.79111645e-01 2.79211685e-01 2.79261705e-01 2.79361745e-01\n",
      " 2.80512205e-01 2.80712285e-01 2.81112445e-01 2.81212485e-01\n",
      " 2.81812725e-01 2.81912765e-01 2.83013205e-01 2.83113245e-01\n",
      " 2.84413766e-01 2.84613846e-01 2.85014006e-01 2.85114046e-01\n",
      " 2.85664266e-01 2.85764306e-01 2.87164866e-01 2.87264906e-01\n",
      " 2.88865546e-01 2.88965586e-01 2.89465786e-01 2.89565826e-01\n",
      " 2.90016006e-01 2.90116046e-01 2.90366146e-01 2.90466186e-01\n",
      " 2.90816327e-01 2.91016407e-01 2.92266907e-01 2.92466987e-01\n",
      " 2.93467387e-01 2.93617447e-01 2.93667467e-01 2.93767507e-01\n",
      " 2.94317727e-01 2.94417767e-01 2.94467787e-01 2.94567827e-01\n",
      " 2.96018407e-01 2.96118447e-01 2.96318527e-01 2.96418567e-01\n",
      " 2.98319328e-01 2.98419368e-01 2.98519408e-01 2.98619448e-01\n",
      " 2.98669468e-01 2.98869548e-01 3.03121248e-01 3.03221289e-01\n",
      " 3.03271309e-01 3.03471389e-01 3.03571429e-01 3.03671469e-01\n",
      " 3.05822329e-01 3.05922369e-01 3.05972389e-01 3.06072429e-01\n",
      " 3.06322529e-01 3.06422569e-01 3.06822729e-01 3.06922769e-01\n",
      " 3.07072829e-01 3.07222889e-01 3.07523009e-01 3.07623049e-01\n",
      " 3.07773109e-01 3.07973189e-01 3.08073229e-01 3.08373349e-01\n",
      " 3.08873549e-01 3.08973589e-01 3.11124450e-01 3.11224490e-01\n",
      " 3.12124850e-01 3.12224890e-01 3.12575030e-01 3.12675070e-01\n",
      " 3.13625450e-01 3.13725490e-01 3.13825530e-01 3.13925570e-01\n",
      " 3.14175670e-01 3.14375750e-01 3.14775910e-01 3.14875950e-01\n",
      " 3.14925970e-01 3.15076030e-01 3.16776711e-01 3.16876751e-01\n",
      " 3.17426971e-01 3.17577031e-01 3.17777111e-01 3.17877151e-01\n",
      " 3.17977191e-01 3.18077231e-01 3.18677471e-01 3.18777511e-01\n",
      " 3.19927971e-01 3.20028011e-01 3.20278111e-01 3.20378151e-01\n",
      " 3.20628251e-01 3.20728291e-01 3.21128451e-01 3.21228491e-01\n",
      " 3.22178872e-01 3.22278912e-01 3.22629052e-01 3.22729092e-01\n",
      " 3.22879152e-01 3.23379352e-01 3.23479392e-01 3.23579432e-01\n",
      " 3.23729492e-01 3.23829532e-01 3.24529812e-01 3.24629852e-01\n",
      " 3.24779912e-01 3.24879952e-01 3.24979992e-01 3.25080032e-01\n",
      " 3.25230092e-01 3.25330132e-01 3.25630252e-01 3.25730292e-01\n",
      " 3.25980392e-01 3.26080432e-01 3.26580632e-01 3.26680672e-01\n",
      " 3.27030812e-01 3.27130852e-01 3.27531012e-01 3.27631052e-01\n",
      " 3.27881152e-01 3.27981192e-01 3.28181273e-01 3.28281313e-01\n",
      " 3.28631453e-01 3.28731493e-01 3.30632253e-01 3.30782313e-01\n",
      " 3.30882353e-01 3.30982393e-01 3.31282513e-01 3.31382553e-01\n",
      " 3.32482993e-01 3.32583033e-01 3.33733493e-01 3.33833533e-01\n",
      " 3.33983593e-01 3.34083633e-01 3.34383754e-01 3.34583834e-01\n",
      " 3.34633854e-01 3.34733894e-01 3.35434174e-01 3.35534214e-01\n",
      " 3.35634254e-01 3.35734294e-01 3.35784314e-01 3.35884354e-01\n",
      " 3.36034414e-01 3.36134454e-01 3.36984794e-01 3.37084834e-01\n",
      " 3.37434974e-01 3.37535014e-01 3.38935574e-01 3.39035614e-01\n",
      " 3.39585834e-01 3.39785914e-01 3.40586234e-01 3.40686275e-01\n",
      " 3.40986395e-01 3.41086435e-01 3.41436575e-01 3.41536615e-01\n",
      " 3.41636655e-01 3.41736695e-01 3.42286915e-01 3.42386955e-01\n",
      " 3.42486995e-01 3.42687075e-01 3.42737095e-01 3.42837135e-01\n",
      " 3.43337335e-01 3.43437375e-01 3.43537415e-01 3.43737495e-01\n",
      " 3.43787515e-01 3.43887555e-01 3.43987595e-01 3.44137655e-01\n",
      " 3.44287715e-01 3.44387755e-01 3.44637855e-01 3.44737895e-01\n",
      " 3.44887955e-01 3.44987995e-01 3.45188075e-01 3.45388155e-01\n",
      " 3.45488195e-01 3.45638255e-01 3.45838335e-01 3.45988395e-01\n",
      " 3.46238495e-01 3.46338535e-01 3.46488595e-01 3.46688675e-01\n",
      " 3.46938776e-01 3.47038816e-01 3.47238896e-01 3.47338936e-01\n",
      " 3.47488996e-01 3.47589036e-01 3.47739096e-01 3.47839136e-01\n",
      " 3.47939176e-01 3.48039216e-01 3.48189276e-01 3.48389356e-01\n",
      " 3.48539416e-01 3.48739496e-01 3.50940376e-01 3.51040416e-01\n",
      " 3.51140456e-01 3.51290516e-01 3.51890756e-01 3.51990796e-01\n",
      " 3.52140856e-01 3.52240896e-01 3.52290916e-01 3.52390956e-01\n",
      " 3.52591036e-01 3.52691076e-01 3.53341337e-01 3.53441377e-01\n",
      " 3.53791517e-01 3.53891557e-01 3.54391757e-01 3.54491797e-01\n",
      " 3.55692277e-01 3.55792317e-01 3.56092437e-01 3.56192477e-01\n",
      " 3.56292517e-01 3.56392557e-01 3.56692677e-01 3.56792717e-01\n",
      " 3.57192877e-01 3.57292917e-01 3.57442977e-01 3.57543017e-01\n",
      " 3.57593037e-01 3.57793117e-01 3.57943177e-01 3.58043217e-01\n",
      " 3.58093237e-01 3.58293317e-01 3.58543417e-01 3.58643457e-01\n",
      " 3.58843537e-01 3.58943577e-01 3.59243697e-01 3.59343737e-01\n",
      " 3.59393758e-01 3.59493798e-01 3.59593838e-01 3.59793918e-01\n",
      " 3.60294118e-01 3.60394158e-01 3.60494198e-01 3.60594238e-01\n",
      " 3.60794318e-01 3.60994398e-01 3.61144458e-01 3.61244498e-01\n",
      " 3.61444578e-01 3.61544618e-01 3.61644658e-01 3.61744698e-01\n",
      " 3.61794718e-01 3.61894758e-01 3.61944778e-01 3.62044818e-01\n",
      " 3.62094838e-01 3.62294918e-01 3.62344938e-01 3.62545018e-01\n",
      " 3.62695078e-01 3.62795118e-01 3.63145258e-01 3.63245298e-01\n",
      " 3.63345338e-01 3.63445378e-01 3.63595438e-01 3.63845538e-01\n",
      " 3.63945578e-01 3.64595838e-01 3.64695878e-01 3.65446178e-01\n",
      " 3.65546218e-01 3.65696279e-01 3.65896359e-01 3.66046419e-01\n",
      " 3.66146459e-01 3.66746699e-01 3.66846739e-01 3.66996799e-01\n",
      " 3.67096839e-01 3.67346939e-01 3.67446979e-01 3.67997199e-01\n",
      " 3.68097239e-01 3.68197279e-01 3.68347339e-01 3.68847539e-01\n",
      " 3.69047619e-01 3.69097639e-01 3.69197679e-01 3.69247699e-01\n",
      " 3.69347739e-01 3.69447779e-01 3.69597839e-01 3.69697879e-01\n",
      " 3.69847939e-01 3.69947979e-01 3.70148059e-01 3.70248099e-01\n",
      " 3.70298119e-01 3.70398159e-01 3.70498199e-01 3.70598239e-01\n",
      " 3.70948379e-01 3.71098439e-01 3.71198479e-01 3.71448579e-01\n",
      " 3.71548619e-01 3.71898760e-01 3.71998800e-01 3.72198880e-01\n",
      " 3.72398960e-01 3.72448980e-01 3.72549020e-01 3.77651060e-01\n",
      " 3.77751100e-01 3.93057223e-01 3.93157263e-01 4.54681873e-01\n",
      " 4.54781913e-01 5.88635454e-01 5.88735494e-01 6.05292117e-01\n",
      " 6.05392157e-01 6.09443778e-01 6.09543818e-01 7.69707883e-01\n",
      " 7.69807923e-01 8.38935574e-01 8.39035614e-01 8.52541016e-01\n",
      " 8.52641056e-01 8.89955982e-01 8.90056022e-01 9.09263705e-01\n",
      " 9.09363745e-01 1.00000000e+00] [0.         0.00185529 0.74953618 0.74953618 0.82560297 0.82560297\n",
      " 0.85157699 0.85157699 0.86085343 0.86085343 0.86827458 0.86827458\n",
      " 0.87198516 0.87198516 0.87384045 0.87384045 0.89239332 0.89239332\n",
      " 0.89795918 0.89795918 0.89981447 0.89981447 0.90723562 0.90723562\n",
      " 0.90909091 0.90909091 0.9109462  0.9109462  0.91280148 0.91280148\n",
      " 0.91651206 0.91651206 0.91836735 0.91836735 0.92022263 0.92022263\n",
      " 0.92207792 0.92207792 0.9257885  0.9257885  0.92764378 0.92764378\n",
      " 0.93135436 0.93135436 0.93320965 0.93320965 0.93506494 0.93506494\n",
      " 0.93692022 0.93692022 0.94434137 0.94434137 0.94619666 0.94619666\n",
      " 0.94805195 0.94805195 0.94990724 0.94990724 0.95176252 0.95176252\n",
      " 0.95361781 0.95361781 0.9554731  0.9554731  0.95732839 0.95732839\n",
      " 0.95918367 0.95918367 0.96103896 0.96103896 0.96474954 0.96474954\n",
      " 0.96660482 0.96660482 0.96846011 0.96846011 0.9703154  0.9703154\n",
      " 0.97217069 0.97217069 0.97402597 0.97402597 0.97588126 0.97588126\n",
      " 0.97773655 0.97773655 0.97959184 0.97959184 0.98144712 0.98144712\n",
      " 0.98330241 0.98330241 0.9851577  0.9851577  0.98886827 0.98886827\n",
      " 0.99072356 0.99072356 0.99257885 0.99257885 0.99443414 0.99443414\n",
      " 0.99628942 0.99628942 0.99814471 0.99814471 0.99814471 0.99814471\n",
      " 0.99814471 0.99814471 0.99814471 0.99814471 0.99814471 0.99814471\n",
      " 0.99814471 0.99814471 0.99814471 0.99814471 0.99814471 0.99814471\n",
      " 0.99814471 0.99814471 0.99814471 0.99814471 0.99814471 0.99814471\n",
      " 0.99814471 0.99814471 0.99814471 0.99814471 0.99814471 0.99814471\n",
      " 0.99814471 0.99814471 1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.        ] [1.98956215e+00 9.89562154e-01 6.44508898e-01 6.42891109e-01\n",
      " 4.98307973e-01 4.98197109e-01 4.25961703e-01 4.21736598e-01\n",
      " 4.10294503e-01 4.04356062e-01 3.78195822e-01 3.71420622e-01\n",
      " 3.65776777e-01 3.52375150e-01 3.51180255e-01 3.49862754e-01\n",
      " 3.03632140e-01 3.03238571e-01 2.92652488e-01 2.86844492e-01\n",
      " 2.81522393e-01 2.72797406e-01 2.63667762e-01 2.60093391e-01\n",
      " 2.57380903e-01 2.45305896e-01 2.39879459e-01 2.30364978e-01\n",
      " 2.28251040e-01 2.25509226e-01 2.24996895e-01 2.23433733e-01\n",
      " 2.19945043e-01 2.16111869e-01 2.15871245e-01 2.12739378e-01\n",
      " 2.10648209e-01 2.07881838e-01 2.05853164e-01 2.04840958e-01\n",
      " 2.02551812e-01 1.99337900e-01 1.97947025e-01 1.83461368e-01\n",
      " 1.81271821e-01 1.77955478e-01 1.77069753e-01 1.73845887e-01\n",
      " 1.72165841e-01 1.71713710e-01 1.50753886e-01 1.44297332e-01\n",
      " 1.43841326e-01 1.37876511e-01 1.37512982e-01 1.37152344e-01\n",
      " 1.36661053e-01 1.35169327e-01 1.33928210e-01 1.32697135e-01\n",
      " 1.32321805e-01 1.31131947e-01 1.30363673e-01 1.29278898e-01\n",
      " 1.28870904e-01 1.27693146e-01 1.26087457e-01 1.26008302e-01\n",
      " 1.25942618e-01 1.18212193e-01 1.18031710e-01 1.07008696e-01\n",
      " 1.05751187e-01 9.87034738e-02 9.87001359e-02 9.22007263e-02\n",
      " 9.13804471e-02 8.84291232e-02 8.81958604e-02 8.60047340e-02\n",
      " 8.59115720e-02 8.09730291e-02 8.05194974e-02 7.46249557e-02\n",
      " 7.43615329e-02 7.43023753e-02 7.41578341e-02 7.04114735e-02\n",
      " 7.03813732e-02 6.93522096e-02 6.91642761e-02 6.77539408e-02\n",
      " 6.73841834e-02 5.69815934e-02 5.68237603e-02 3.26823294e-02\n",
      " 3.26660275e-02 3.08616757e-02 3.08527350e-02 3.03092599e-02\n",
      " 3.01955342e-02 2.77920663e-02 2.77682543e-02 2.10363269e-02\n",
      " 2.10308433e-02 6.26143813e-03 6.24686480e-03 4.72995639e-03\n",
      " 4.72986698e-03 4.49240208e-03 4.49135900e-03 3.95056605e-03\n",
      " 3.94585729e-03 3.86977196e-03 3.86860967e-03 3.62196565e-03\n",
      " 3.62038612e-03 2.94631720e-03 2.94440985e-03 2.81593204e-03\n",
      " 2.81307101e-03 2.56636739e-03 2.55477428e-03 2.37283111e-03\n",
      " 2.37065554e-03 1.99529529e-03 1.99493766e-03 1.82169676e-03\n",
      " 1.81967020e-03 1.74269080e-03 1.74105167e-03 1.66812539e-03\n",
      " 1.66535378e-03 1.63179636e-03 1.63027644e-03 1.58709288e-03\n",
      " 1.58241391e-03 1.41802430e-03 1.41745806e-03 1.27145648e-03\n",
      " 1.27017498e-03 1.26874447e-03 1.26779079e-03 1.20341778e-03\n",
      " 1.20237470e-03 1.13970041e-03 1.13701820e-03 1.10346079e-03\n",
      " 1.10331178e-03 1.08492374e-03 1.08444691e-03 1.06531382e-03\n",
      " 1.06433034e-03 1.04779005e-03 1.04776025e-03 9.74088907e-04\n",
      " 9.73880291e-04 9.56594944e-04 9.56177711e-04 9.46670771e-04\n",
      " 9.44912434e-04 9.08762217e-04 9.08553600e-04 8.61197710e-04\n",
      " 8.60989094e-04 8.48889351e-04 8.48799944e-04 8.47011805e-04\n",
      " 8.43971968e-04 7.87705183e-04 7.86781311e-04 7.34120607e-04\n",
      " 7.33852386e-04 7.21424818e-04 7.20530748e-04 7.17490911e-04\n",
      " 7.17073679e-04 7.06434250e-04 7.05987215e-04 6.94930553e-04\n",
      " 6.94900751e-04 6.74098730e-04 6.73562288e-04 6.64055347e-04\n",
      " 6.63816929e-04 6.13033772e-04 6.12705946e-04 6.12467527e-04\n",
      " 6.12020493e-04 6.11990690e-04 5.92559576e-04 5.90950251e-04\n",
      " 5.81711531e-04 5.81115484e-04 5.77390194e-04 5.76943159e-04\n",
      " 5.61267138e-04 5.60939312e-04 5.44995070e-04 5.44756651e-04\n",
      " 5.31882048e-04 5.31375408e-04 5.23418188e-04 5.22792339e-04\n",
      " 5.19216061e-04 5.18977642e-04 5.17815351e-04 5.16384840e-04\n",
      " 5.15311956e-04 5.14656305e-04 5.07086515e-04 5.06877899e-04\n",
      " 4.96596098e-04 4.96119261e-04 4.95880842e-04 4.95433807e-04\n",
      " 4.94629145e-04 4.94480133e-04 4.81009483e-04 4.80949879e-04\n",
      " 4.75466251e-04 4.74482775e-04 4.70489264e-04 4.70131636e-04\n",
      " 4.65452671e-04 4.65333462e-04 4.53203917e-04 4.53084707e-04\n",
      " 4.47630882e-04 4.47064638e-04 4.45187092e-04 4.44650650e-04\n",
      " 4.41223383e-04 4.40806150e-04 4.36156988e-04 4.36097383e-04\n",
      " 4.31090593e-04 4.30852175e-04 4.20153141e-04 4.20033932e-04\n",
      " 4.15652990e-04 4.15325165e-04 4.14997339e-04 4.14520502e-04\n",
      " 4.09722328e-04 4.09305096e-04 4.03732061e-04 4.03255224e-04\n",
      " 3.96698713e-04 3.95953655e-04 3.95923853e-04 3.95655632e-04\n",
      " 3.89724970e-04 3.89248133e-04 3.86953354e-04 3.85731459e-04\n",
      " 3.82065773e-04 3.81946564e-04 3.77357006e-04 3.77178192e-04\n",
      " 3.70830297e-04 3.70532274e-04 3.68565321e-04 3.68535519e-04\n",
      " 3.64899635e-04 3.64482403e-04 3.58879566e-04 3.58521938e-04\n",
      " 3.51071358e-04 3.50981951e-04 3.48299742e-04 3.48031521e-04\n",
      " 3.45379114e-04 3.45349312e-04 3.43501568e-04 3.43382359e-04\n",
      " 3.41475010e-04 3.41296196e-04 3.36378813e-04 3.35812569e-04\n",
      " 3.31848860e-04 3.31640244e-04 3.31431627e-04 3.31163406e-04\n",
      " 3.26663256e-04 3.26603651e-04 3.26097012e-04 3.25888395e-04\n",
      " 3.19957733e-04 3.19540501e-04 3.18616629e-04 3.18378210e-04\n",
      " 3.09675932e-04 3.09258699e-04 3.08841467e-04 3.08215618e-04\n",
      " 3.08156013e-04 3.07857990e-04 2.89201736e-04 2.89022923e-04\n",
      " 2.88963318e-04 2.88456678e-04 2.88277864e-04 2.88128853e-04\n",
      " 2.79873610e-04 2.79784203e-04 2.79664993e-04 2.79635191e-04\n",
      " 2.78979540e-04 2.78949738e-04 2.77161598e-04 2.76982784e-04\n",
      " 2.76505947e-04 2.76446342e-04 2.75015831e-04 2.74658203e-04\n",
      " 2.74270773e-04 2.73734331e-04 2.73495913e-04 2.72661448e-04\n",
      " 2.70456076e-04 2.70247459e-04 2.61783600e-04 2.61485577e-04\n",
      " 2.58743763e-04 2.58713961e-04 2.57611275e-04 2.57581472e-04\n",
      " 2.54839659e-04 2.54780054e-04 2.54601240e-04 2.54541636e-04\n",
      " 2.53707170e-04 2.53617764e-04 2.52634287e-04 2.52604485e-04\n",
      " 2.52276659e-04 2.52127647e-04 2.47299671e-04 2.47210264e-04\n",
      " 2.45004892e-04 2.44766474e-04 2.44528055e-04 2.44349241e-04\n",
      " 2.43753195e-04 2.43455172e-04 2.41219997e-04 2.41070986e-04\n",
      " 2.36034393e-04 2.35736370e-04 2.34812498e-04 2.34782696e-04\n",
      " 2.33948231e-04 2.33680010e-04 2.32815742e-04 2.32785940e-04\n",
      " 2.30282545e-04 2.30133533e-04 2.29179859e-04 2.29090452e-04\n",
      " 2.29030848e-04 2.26944685e-04 2.26706266e-04 2.26587057e-04\n",
      " 2.25603580e-04 2.25454569e-04 2.22951174e-04 2.22742558e-04\n",
      " 2.22116709e-04 2.21431255e-04 2.21103430e-04 2.20745802e-04\n",
      " 2.20388174e-04 2.20298767e-04 2.19285488e-04 2.18927860e-04\n",
      " 2.18153000e-04 2.18063593e-04 2.17139721e-04 2.16931105e-04\n",
      " 2.15828419e-04 2.15768814e-04 2.14874744e-04 2.14844942e-04\n",
      " 2.14397907e-04 2.14368105e-04 2.13652849e-04 2.13563442e-04\n",
      " 2.12401152e-04 2.12252140e-04 2.06202269e-04 2.06053257e-04\n",
      " 2.05546618e-04 2.05516815e-04 2.04294920e-04 2.04235315e-04\n",
      " 2.00748444e-04 2.00539827e-04 1.97678804e-04 1.97589397e-04\n",
      " 1.97052956e-04 1.96963549e-04 1.96188688e-04 1.95920467e-04\n",
      " 1.95831060e-04 1.95592642e-04 1.94042921e-04 1.94013119e-04\n",
      " 1.93744898e-04 1.93268061e-04 1.93059444e-04 1.92850828e-04\n",
      " 1.92761421e-04 1.92701817e-04 1.90436840e-04 1.90377235e-04\n",
      " 1.89214945e-04 1.88946724e-04 1.85161829e-04 1.84953213e-04\n",
      " 1.83254480e-04 1.83016062e-04 1.81019306e-04 1.80780888e-04\n",
      " 1.79499388e-04 1.79469585e-04 1.78754330e-04 1.78605318e-04\n",
      " 1.78247690e-04 1.78188086e-04 1.76578760e-04 1.76399946e-04\n",
      " 1.76072121e-04 1.75684690e-04 1.75654888e-04 1.75505877e-04\n",
      " 1.74045563e-04 1.73956156e-04 1.73598528e-04 1.73330307e-04\n",
      " 1.73240900e-04 1.73211098e-04 1.73151493e-04 1.73062086e-04\n",
      " 1.72853470e-04 1.72793865e-04 1.71840191e-04 1.71810389e-04\n",
      " 1.71244144e-04 1.71154737e-04 1.70916319e-04 1.70677900e-04\n",
      " 1.70350075e-04 1.70111656e-04 1.69873238e-04 1.69694424e-04\n",
      " 1.68889761e-04 1.68710947e-04 1.68085098e-04 1.67489052e-04\n",
      " 1.66863203e-04 1.66803598e-04 1.66594982e-04 1.66535378e-04\n",
      " 1.66118145e-04 1.66028738e-04 1.65760517e-04 1.65611506e-04\n",
      " 1.65402889e-04 1.65253878e-04 1.65015459e-04 1.64866447e-04\n",
      " 1.64449215e-04 1.64270401e-04 1.58399343e-04 1.58339739e-04\n",
      " 1.58250332e-04 1.58101320e-04 1.57415867e-04 1.57386065e-04\n",
      " 1.56462193e-04 1.56372786e-04 1.56342983e-04 1.56193972e-04\n",
      " 1.55866146e-04 1.55687332e-04 1.53958797e-04 1.53869390e-04\n",
      " 1.53541565e-04 1.53511763e-04 1.52856112e-04 1.52587891e-04\n",
      " 1.50114298e-04 1.50084496e-04 1.49369240e-04 1.49190426e-04\n",
      " 1.49130821e-04 1.48981810e-04 1.48236752e-04 1.48028135e-04\n",
      " 1.47402287e-04 1.47312880e-04 1.47014856e-04 1.46687031e-04\n",
      " 1.46657228e-04 1.46448612e-04 1.46180391e-04 1.46090984e-04\n",
      " 1.46031380e-04 1.45941973e-04 1.45405531e-04 1.45345926e-04\n",
      " 1.44988298e-04 1.44958496e-04 1.44392252e-04 1.44362450e-04\n",
      " 1.44332647e-04 1.44302845e-04 1.43736601e-04 1.43438578e-04\n",
      " 1.42395496e-04 1.42306089e-04 1.41948462e-04 1.41829252e-04\n",
      " 1.41292810e-04 1.41203403e-04 1.41113997e-04 1.40935183e-04\n",
      " 1.40488148e-04 1.40249729e-04 1.39981508e-04 1.39802694e-04\n",
      " 1.39772892e-04 1.39653683e-04 1.39564276e-04 1.39504671e-04\n",
      " 1.39474869e-04 1.39206648e-04 1.39087439e-04 1.38998032e-04\n",
      " 1.38700008e-04 1.38372183e-04 1.37954950e-04 1.37925148e-04\n",
      " 1.37835741e-04 1.37805939e-04 1.37627125e-04 1.36941671e-04\n",
      " 1.36911869e-04 1.35302544e-04 1.35272741e-04 1.33275986e-04\n",
      " 1.33216381e-04 1.32977962e-04 1.32918358e-04 1.32650137e-04\n",
      " 1.32381916e-04 1.30981207e-04 1.30891800e-04 1.30593777e-04\n",
      " 1.30534172e-04 1.30087137e-04 1.30027533e-04 1.29282475e-04\n",
      " 1.29222870e-04 1.28865242e-04 1.28805637e-04 1.27941370e-04\n",
      " 1.27762556e-04 1.27643347e-04 1.27613544e-04 1.27583742e-04\n",
      " 1.27285719e-04 1.27196312e-04 1.27136707e-04 1.27047300e-04\n",
      " 1.26659870e-04 1.26600266e-04 1.25974417e-04 1.25944614e-04\n",
      " 1.25914812e-04 1.25855207e-04 1.25676394e-04 1.25616789e-04\n",
      " 1.25139952e-04 1.25110149e-04 1.24841928e-04 1.24514103e-04\n",
      " 1.24424696e-04 1.23947859e-04 1.23918056e-04 1.23649836e-04\n",
      " 1.23590231e-04 1.23530626e-04 1.23441219e-04 1.16003786e-04\n",
      " 1.15975468e-04 9.58160454e-05 9.58109304e-05 4.61630116e-05\n",
      " 4.61280688e-05 9.74322393e-06 9.74191335e-06 7.83832820e-06\n",
      " 7.83332143e-06 7.46206524e-06 7.45719171e-06 8.17835655e-07\n",
      " 8.17608736e-07 2.32750182e-07 2.32143222e-07 1.73603794e-07\n",
      " 1.73514422e-07 7.43105915e-08 7.43077564e-08 4.58577212e-08\n",
      " 4.57213005e-08 6.83485793e-14]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve( y_test, y_test_proba )\n",
    "auc = roc_auc_score(y_test, y_test_proba)\n",
    "\n",
    "print ( fpr, tpr, thresholds )\n",
    "\n",
    "fig, axes = plt.subplots( 1, 2, figsize=(20,10) )\n",
    "\n",
    "axes[0].plot(fpr, tpr, label=\"ROC AUC={:.5f}\".format(auc))\n",
    "axes[0].legend( loc='best', fontsize=18 )\n",
    "axes[0].set_xlabel( 'False positive rate', fontsize=22 )\n",
    "axes[0].set_ylabel( 'True positive rate', fontsize=22 )\n",
    "axes[0].set_ylim(0.80,1.01)\n",
    "axes[0].grid()\n",
    "\n",
    "#axes[0].plot(tpr,\n",
    "#             np.divide(np.ones_like(fpr), fpr, out=np.zeros_like(fpr), where=fpr!=0), \n",
    "#             label=\"ROC AUC={:.5f}\".format(auc))\n",
    "#axes[0].legend(loc='best')\n",
    "#axes[0].set_ylabel('Background rejection (1/false positive rate)')\n",
    "#axes[0].set_xlabel('Signal efficiency (true positive rate)')\n",
    "#axes[0].set_xlim(0.88,1.0)\n",
    "#axes[0].grid()\n",
    "\n",
    "axes[1].plot(thresholds,fpr,label='Background', color='lightcoral')\n",
    "axes[1].plot(thresholds,tpr,label='Signal', color='lightblue')\n",
    "axes[1].legend( loc='best', fontsize=18 )\n",
    "axes[1].set_xlabel( 'Score', fontsize=22 )\n",
    "axes[1].set_ylabel( 'Efficiency', fontsize=22 )\n",
    "axes[1].grid()\n",
    "#axes[1].set_xscale('log')\n",
    "#axes[1].set_yscale('log')\n",
    "\n",
    "if save_figures:\n",
    "    plt.savefig( \"plots/ANN-Keras_ROC_Efficiency_test-multiRP_2021_01_22-17_46_10.pdf\", bbox_inches='tight' )\n",
    "    plt.savefig( \"plots/ANN-Keras_ROC_Efficiency_test-multiRP_2021_01_22-17_46_10.png\", bbox_inches='tight' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26366776\n",
      "[0 1 0 ... 0 0 0]\n",
      "0.9969314694851688\n",
      "0.9072356215213359\n",
      "0.9993497398959584\n",
      "0.0006502601040415668\n",
      "$\\rm{Prob.} \\geq 0.26 \\;\\rm{-}\\; \\rm{FPR} = 0.00065 \\;\\rm{-}\\; \\rm{TPR} = 0.9072$\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#prob_cut = 0.10\n",
    "\n",
    "#tpr_target = 0.95\n",
    "tpr_target = 0.90\n",
    "#tpr_target = 0.85\n",
    "#tpr_target = 0.70\n",
    "prob_cut = thresholds[ np.argmax( tpr >= tpr_target ) ]\n",
    "#fpr_target = 5.5e-04\n",
    "#prob_cut = thresholds[ np.argmin( fpr <= fpr_target ) - 1 ]\n",
    "print( prob_cut )\n",
    "\n",
    "y_test_pred = ( y_test_proba >= prob_cut ).astype( \"int32\" )\n",
    "y_test_pred = y_test_pred.ravel()\n",
    "print ( y_test_pred )\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print ( accuracy_score( y_test, y_test_pred ) )\n",
    "#tpr_cut = np.sum( ( y_test == 1 ) & ( y_test_pred == 1 ) ) / np.sum( ( y_test == 1 ) )\n",
    "tpr_cut = accuracy_score( y_test[ y_test == 1 ], y_test_pred[ y_test == 1 ] )\n",
    "print ( tpr_cut )\n",
    "#tnr_cut = np.sum( ( y_test == 0 ) & ( y_test_pred == 0 ) ) / np.sum( ( y_test == 0 ) )\n",
    "tnr_cut = accuracy_score( y_test[ y_test == 0 ], y_test_pred[ y_test == 0 ] )\n",
    "print ( tnr_cut )\n",
    "fpr_cut = ( 1. - tnr_cut )\n",
    "print ( fpr_cut )\n",
    "\n",
    "fig = plt.figure( figsize=(12,10) )\n",
    "plt.plot(fpr, tpr, label=\"ROC AUC = {:.5f}\".format(auc))\n",
    "label_str = \"$\\\\rm{{Prob.}} \\geq {:.2f} \\;\\\\rm{{-}}\\; \\\\rm{{FPR}} = {:.5f} \\;\\\\rm{{-}}\\; \\\\rm{{TPR}} = {:.4f}$\".format( prob_cut, fpr_cut, tpr_cut )\n",
    "print ( label_str )\n",
    "plt.plot( fpr_cut, tpr_cut, marker='o', markersize=10, linestyle='', color='darkblue',\n",
    "          label=r\"{}\".format( label_str ) )\n",
    "plt.legend( loc='best', fontsize=18 )\n",
    "plt.xlabel( 'False positive rate', fontsize=22 )\n",
    "plt.ylabel( 'True positive rate', fontsize=22 )\n",
    "plt.ylim(0.60,1.01)\n",
    "plt.grid()\n",
    "\n",
    "if save_figures:\n",
    "    plt.savefig( \"plots/ANN-Keras_ROC_90pct_Point_test-multiRP_2021_01_22-17_46_10.pdf\", bbox_inches='tight' )\n",
    "    plt.savefig( \"plots/ANN-Keras_ROC_90pct_Point_test-multiRP_2021_01_22-17_46_10.png\", bbox_inches='tight' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model and save_model:\n",
    "    model_final.save( \"model/keras_model.h5\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Sequential in module tensorflow.python.keras.engine.sequential:\n",
      "\n",
      "class Sequential(tensorflow.python.keras.engine.functional.Functional)\n",
      " |  Sequential(*args, **kwargs)\n",
      " |  \n",
      " |  `Sequential` groups a linear stack of layers into a `tf.keras.Model`.\n",
      " |  \n",
      " |  `Sequential` provides training and inference features on this model.\n",
      " |  \n",
      " |  Examples:\n",
      " |  \n",
      " |  >>> # Optionally, the first layer can receive an `input_shape` argument:\n",
      " |  >>> model = tf.keras.Sequential()\n",
      " |  >>> model.add(tf.keras.layers.Dense(8, input_shape=(16,)))\n",
      " |  >>> # Afterwards, we do automatic shape inference:\n",
      " |  >>> model.add(tf.keras.layers.Dense(4))\n",
      " |  \n",
      " |  >>> # This is identical to the following:\n",
      " |  >>> model = tf.keras.Sequential()\n",
      " |  >>> model.add(tf.keras.Input(shape=(16,)))\n",
      " |  >>> model.add(tf.keras.layers.Dense(8))\n",
      " |  \n",
      " |  >>> # Note that you can also omit the `input_shape` argument.\n",
      " |  >>> # In that case the model doesn't have any weights until the first call\n",
      " |  >>> # to a training/evaluation method (since it isn't yet built):\n",
      " |  >>> model = tf.keras.Sequential()\n",
      " |  >>> model.add(tf.keras.layers.Dense(8))\n",
      " |  >>> model.add(tf.keras.layers.Dense(4))\n",
      " |  >>> # model.weights not created yet\n",
      " |  \n",
      " |  >>> # Whereas if you specify the input shape, the model gets built\n",
      " |  >>> # continuously as you are adding layers:\n",
      " |  >>> model = tf.keras.Sequential()\n",
      " |  >>> model.add(tf.keras.layers.Dense(8, input_shape=(16,)))\n",
      " |  >>> model.add(tf.keras.layers.Dense(4))\n",
      " |  >>> len(model.weights)\n",
      " |  4\n",
      " |  \n",
      " |  >>> # When using the delayed-build pattern (no input shape specified), you can\n",
      " |  >>> # choose to manually build your model by calling\n",
      " |  >>> # `build(batch_input_shape)`:\n",
      " |  >>> model = tf.keras.Sequential()\n",
      " |  >>> model.add(tf.keras.layers.Dense(8))\n",
      " |  >>> model.add(tf.keras.layers.Dense(4))\n",
      " |  >>> model.build((None, 16))\n",
      " |  >>> len(model.weights)\n",
      " |  4\n",
      " |  \n",
      " |  ```python\n",
      " |  # Note that when using the delayed-build pattern (no input shape specified),\n",
      " |  # the model gets built the first time you call `fit`, `eval`, or `predict`,\n",
      " |  # or the first time you call the model on some input data.\n",
      " |  model = tf.keras.Sequential()\n",
      " |  model.add(tf.keras.layers.Dense(8))\n",
      " |  model.add(tf.keras.layers.Dense(1))\n",
      " |  model.compile(optimizer='sgd', loss='mse')\n",
      " |  # This builds the model for the first time:\n",
      " |  model.fit(x, y, batch_size=32, epochs=10)\n",
      " |  ```\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Sequential\n",
      " |      tensorflow.python.keras.engine.functional.Functional\n",
      " |      tensorflow.python.keras.engine.training.Model\n",
      " |      tensorflow.python.keras.engine.base_layer.Layer\n",
      " |      tensorflow.python.module.module.Module\n",
      " |      tensorflow.python.training.tracking.tracking.AutoTrackable\n",
      " |      tensorflow.python.training.tracking.base.Trackable\n",
      " |      tensorflow.python.keras.utils.version_utils.LayerVersionSelector\n",
      " |      tensorflow.python.keras.utils.version_utils.ModelVersionSelector\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, layers=None, name=None)\n",
      " |      Creates a `Sequential` model instance.\n",
      " |      \n",
      " |      Args:\n",
      " |        layers: Optional list of layers to add to the model.\n",
      " |        name: Optional name for the model.\n",
      " |  \n",
      " |  add(self, layer)\n",
      " |      Adds a layer instance on top of the layer stack.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          layer: layer instance.\n",
      " |      \n",
      " |      Raises:\n",
      " |          TypeError: If `layer` is not a layer instance.\n",
      " |          ValueError: In case the `layer` argument does not\n",
      " |              know its input shape.\n",
      " |          ValueError: In case the `layer` argument has\n",
      " |              multiple output tensors, or is already connected\n",
      " |              somewhere else (forbidden in `Sequential` models).\n",
      " |  \n",
      " |  build(self, input_shape=None)\n",
      " |      Builds the model based on input shapes received.\n",
      " |      \n",
      " |      This is to be used for subclassed models, which do not know at instantiation\n",
      " |      time what their inputs look like.\n",
      " |      \n",
      " |      This method only exists for users who want to call `model.build()` in a\n",
      " |      standalone way (as a substitute for calling the model on real data to\n",
      " |      build it). It will never be called by the framework (and thus it will\n",
      " |      never throw unexpected errors in an unrelated workflow).\n",
      " |      \n",
      " |      Args:\n",
      " |       input_shape: Single tuple, TensorShape, or list of shapes, where shapes\n",
      " |           are tuples, integers, or TensorShapes.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError:\n",
      " |          1. In case of invalid user-provided data (not of type tuple,\n",
      " |             list, or TensorShape).\n",
      " |          2. If the model requires call arguments that are agnostic\n",
      " |             to the input shapes (positional or kwarg in call signature).\n",
      " |          3. If not all layers were properly built.\n",
      " |          4. If float type inputs are not supported within the layers.\n",
      " |      \n",
      " |        In each of these cases, the user should build their model by calling it\n",
      " |        on real tensor data.\n",
      " |  \n",
      " |  call(self, inputs, training=None, mask=None)\n",
      " |      Calls the model on new inputs.\n",
      " |      \n",
      " |      In this case `call` just reapplies\n",
      " |      all ops in the graph to the new inputs\n",
      " |      (e.g. build a new computational graph from the provided inputs).\n",
      " |      \n",
      " |      Arguments:\n",
      " |          inputs: A tensor or list of tensors.\n",
      " |          training: Boolean or boolean scalar tensor, indicating whether to run\n",
      " |            the `Network` in training mode or inference mode.\n",
      " |          mask: A mask or list of masks. A mask can be\n",
      " |              either a tensor or None (no mask).\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor if there is a single output, or\n",
      " |          a list of tensors if there are more than one outputs.\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer.\n",
      " |      \n",
      " |      If the layer has not been built, this method will call `build` on the\n",
      " |      layer. This assumes that the layer will later be used with inputs that\n",
      " |      match the input shape provided here.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          input_shape: Shape tuple (tuple of integers)\n",
      " |              or list of shape tuples (one per output tensor of the layer).\n",
      " |              Shape tuples can include None for free dimensions,\n",
      " |              instead of an integer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An input shape tuple.\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the layer.\n",
      " |      \n",
      " |      A layer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of a layer.\n",
      " |      The same layer can be reinstantiated later\n",
      " |      (without its trained weights) from this configuration.\n",
      " |      \n",
      " |      The config of a layer does not include connectivity\n",
      " |      information, nor the layer class name. These are handled\n",
      " |      by `Network` (one layer of abstraction above).\n",
      " |      \n",
      " |      Returns:\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  pop(self)\n",
      " |      Removes the last layer in the model.\n",
      " |      \n",
      " |      Raises:\n",
      " |          TypeError: if there are no layers in the model.\n",
      " |  \n",
      " |  predict_classes(self, x, batch_size=32, verbose=0)\n",
      " |      Generate class predictions for the input samples. (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed after 2021-01-01.\n",
      " |      Instructions for updating:\n",
      " |      Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      " |      \n",
      " |      The input samples are processed batch by batch.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          x: input data, as a Numpy array or list of Numpy arrays\n",
      " |              (if the model has multiple inputs).\n",
      " |          batch_size: integer.\n",
      " |          verbose: verbosity mode, 0 or 1.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A numpy array of class predictions.\n",
      " |  \n",
      " |  predict_proba(self, x, batch_size=32, verbose=0)\n",
      " |      Generates class probability predictions for the input samples. (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed after 2021-01-01.\n",
      " |      Instructions for updating:\n",
      " |      Please use `model.predict()` instead.\n",
      " |      \n",
      " |      The input samples are processed batch by batch.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          x: input data, as a Numpy array or list of Numpy arrays\n",
      " |              (if the model has multiple inputs).\n",
      " |          batch_size: integer.\n",
      " |          verbose: verbosity mode, 0 or 1.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A Numpy array of probability predictions.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  from_config(config, custom_objects=None) from builtins.type\n",
      " |      Instantiates a Model from its config (output of `get_config()`).\n",
      " |      \n",
      " |      Arguments:\n",
      " |          config: Model config dictionary.\n",
      " |          custom_objects: Optional dictionary mapping names\n",
      " |              (strings) to custom classes or functions to be\n",
      " |              considered during deserialization.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A model instance.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: In case of improperly formatted config dict.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  input_spec\n",
      " |      `InputSpec` instance(s) describing the input format for this layer.\n",
      " |      \n",
      " |      When you create a layer subclass, you can set `self.input_spec` to enable\n",
      " |      the layer to run input compatibility checks when it is called.\n",
      " |      Consider a `Conv2D` layer: it can only be called on a single input tensor\n",
      " |      of rank 4. As such, you can set, in `__init__()`:\n",
      " |      \n",
      " |      ```python\n",
      " |      self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n",
      " |      ```\n",
      " |      \n",
      " |      Now, if you try to call the layer on an input that isn't rank 4\n",
      " |      (for instance, an input of shape `(2,)`, it will raise a nicely-formatted\n",
      " |      error:\n",
      " |      \n",
      " |      ```\n",
      " |      ValueError: Input 0 of layer conv2d is incompatible with the layer:\n",
      " |      expected ndim=4, found ndim=1. Full shape received: [2]\n",
      " |      ```\n",
      " |      \n",
      " |      Input checks that can be specified via `input_spec` include:\n",
      " |      - Structure (e.g. a single input, a list of 2 inputs, etc)\n",
      " |      - Shape\n",
      " |      - Rank (ndim)\n",
      " |      - Dtype\n",
      " |      \n",
      " |      For more information, see `tf.keras.layers.InputSpec`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.keras.layers.InputSpec` instance, or nested structure thereof.\n",
      " |  \n",
      " |  layers\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.keras.engine.functional.Functional:\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |        AttributeError: If no inbound nodes are found.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
      " |      have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined input_shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one output,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor or list of output tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        AttributeError: if the layer is connected to more than one incoming\n",
      " |          layers.\n",
      " |        RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one output,\n",
      " |      or if all outputs have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined output shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.keras.engine.training.Model:\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Support self.foo = trackable syntax.\n",
      " |  \n",
      " |  compile(self, optimizer='rmsprop', loss=None, metrics=None, loss_weights=None, weighted_metrics=None, run_eagerly=None, **kwargs)\n",
      " |      Configures the model for training.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          optimizer: String (name of optimizer) or optimizer instance. See\n",
      " |            `tf.keras.optimizers`.\n",
      " |          loss: String (name of objective function), objective function or\n",
      " |            `tf.keras.losses.Loss` instance. See `tf.keras.losses`. An objective\n",
      " |            function is any callable with the signature `loss = fn(y_true,\n",
      " |            y_pred)`, where y_true = ground truth values with shape =\n",
      " |            `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse\n",
      " |            categorical crossentropy where shape = `[batch_size, d0, .. dN-1]`.\n",
      " |            y_pred = predicted values with shape = `[batch_size, d0, .. dN]`. It\n",
      " |            returns a weighted loss float tensor. If a custom `Loss` instance is\n",
      " |            used and reduction is set to NONE, return value has the shape\n",
      " |            [batch_size, d0, .. dN-1] ie. per-sample or per-timestep loss values;\n",
      " |            otherwise, it is a scalar. If the model has multiple outputs, you can\n",
      " |            use a different loss on each output by passing a dictionary or a list\n",
      " |            of losses. The loss value that will be minimized by the model will\n",
      " |            then be the sum of all individual losses.\n",
      " |          metrics: List of metrics to be evaluated by the model during training\n",
      " |            and testing. Each of this can be a string (name of a built-in\n",
      " |            function), function or a `tf.keras.metrics.Metric` instance. See\n",
      " |            `tf.keras.metrics`. Typically you will use `metrics=['accuracy']`. A\n",
      " |            function is any callable with the signature `result = fn(y_true,\n",
      " |            y_pred)`. To specify different metrics for different outputs of a\n",
      " |            multi-output model, you could also pass a dictionary, such as\n",
      " |              `metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']}`.\n",
      " |                You can also pass a list (len = len(outputs)) of lists of metrics\n",
      " |                such as `metrics=[['accuracy'], ['accuracy', 'mse']]` or\n",
      " |                `metrics=['accuracy', ['accuracy', 'mse']]`. When you pass the\n",
      " |                strings 'accuracy' or 'acc', we convert this to one of\n",
      " |                `tf.keras.metrics.BinaryAccuracy`,\n",
      " |                `tf.keras.metrics.CategoricalAccuracy`,\n",
      " |                `tf.keras.metrics.SparseCategoricalAccuracy` based on the loss\n",
      " |                function used and the model output shape. We do a similar\n",
      " |                conversion for the strings 'crossentropy' and 'ce' as well.\n",
      " |          loss_weights: Optional list or dictionary specifying scalar coefficients\n",
      " |            (Python floats) to weight the loss contributions of different model\n",
      " |            outputs. The loss value that will be minimized by the model will then\n",
      " |            be the *weighted sum* of all individual losses, weighted by the\n",
      " |            `loss_weights` coefficients.\n",
      " |              If a list, it is expected to have a 1:1 mapping to the model's\n",
      " |                outputs. If a dict, it is expected to map output names (strings)\n",
      " |                to scalar coefficients.\n",
      " |          weighted_metrics: List of metrics to be evaluated and weighted by\n",
      " |            sample_weight or class_weight during training and testing.\n",
      " |          run_eagerly: Bool. Defaults to `False`. If `True`, this `Model`'s\n",
      " |            logic will not be wrapped in a `tf.function`. Recommended to leave\n",
      " |            this as `None` unless your `Model` cannot be run inside a\n",
      " |            `tf.function`.\n",
      " |          **kwargs: Any additional arguments. Supported arguments:\n",
      " |              - `experimental_steps_per_execution`: Int. The number of batches to\n",
      " |                run during each `tf.function` call. Running multiple batches\n",
      " |                inside a single `tf.function` call can greatly improve performance\n",
      " |                on TPUs or small models with a large Python overhead. Note that if\n",
      " |                this value is set to `N`, `Callback.on_batch` methods will only be\n",
      " |                called every `N` batches. This currently defaults to `1`. At most,\n",
      " |                one full epoch will be run each execution. If a number larger than\n",
      " |                the size of the epoch is passed, the execution will be truncated\n",
      " |                to the size of the epoch.\n",
      " |              - `sample_weight_mode` for backward compatibility.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: In case of invalid arguments for\n",
      " |              `optimizer`, `loss` or `metrics`.\n",
      " |  \n",
      " |  evaluate(self, x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, return_dict=False)\n",
      " |      Returns the loss value & metrics values for the model in test mode.\n",
      " |      \n",
      " |      Computation is done in batches (see the `batch_size` arg.)\n",
      " |      \n",
      " |      Arguments:\n",
      " |          x: Input data. It could be:\n",
      " |            - A Numpy array (or array-like), or a list of arrays\n",
      " |              (in case the model has multiple inputs).\n",
      " |            - A TensorFlow tensor, or a list of tensors\n",
      " |              (in case the model has multiple inputs).\n",
      " |            - A dict mapping input names to the corresponding array/tensors,\n",
      " |              if the model has named inputs.\n",
      " |            - A `tf.data` dataset. Should return a tuple\n",
      " |              of either `(inputs, targets)` or\n",
      " |              `(inputs, targets, sample_weights)`.\n",
      " |            - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n",
      " |              or `(inputs, targets, sample_weights)`.\n",
      " |            A more detailed description of unpacking behavior for iterator types\n",
      " |            (Dataset, generator, Sequence) is given in the `Unpacking behavior\n",
      " |            for iterator-like inputs` section of `Model.fit`.\n",
      " |          y: Target data. Like the input data `x`, it could be either Numpy\n",
      " |            array(s) or TensorFlow tensor(s). It should be consistent with `x`\n",
      " |            (you cannot have Numpy inputs and tensor targets, or inversely). If\n",
      " |            `x` is a dataset, generator or `keras.utils.Sequence` instance, `y`\n",
      " |            should not be specified (since targets will be obtained from the\n",
      " |            iterator/dataset).\n",
      " |          batch_size: Integer or `None`. Number of samples per batch of\n",
      " |            computation. If unspecified, `batch_size` will default to 32. Do not\n",
      " |            specify the `batch_size` if your data is in the form of a dataset,\n",
      " |            generators, or `keras.utils.Sequence` instances (since they generate\n",
      " |            batches).\n",
      " |          verbose: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar.\n",
      " |          sample_weight: Optional Numpy array of weights for the test samples,\n",
      " |            used for weighting the loss function. You can either pass a flat (1D)\n",
      " |            Numpy array with the same length as the input samples\n",
      " |              (1:1 mapping between weights and samples), or in the case of\n",
      " |                temporal data, you can pass a 2D array with shape `(samples,\n",
      " |                sequence_length)`, to apply a different weight to every timestep\n",
      " |                of every sample. This argument is not supported when `x` is a\n",
      " |                dataset, instead pass sample weights as the third element of `x`.\n",
      " |          steps: Integer or `None`. Total number of steps (batches of samples)\n",
      " |            before declaring the evaluation round finished. Ignored with the\n",
      " |            default value of `None`. If x is a `tf.data` dataset and `steps` is\n",
      " |            None, 'evaluate' will run until the dataset is exhausted. This\n",
      " |            argument is not supported with array inputs.\n",
      " |          callbacks: List of `keras.callbacks.Callback` instances. List of\n",
      " |            callbacks to apply during evaluation. See\n",
      " |            [callbacks](/api_docs/python/tf/keras/callbacks).\n",
      " |          max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
      " |            input only. Maximum size for the generator queue. If unspecified,\n",
      " |            `max_queue_size` will default to 10.\n",
      " |          workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
      " |            only. Maximum number of processes to spin up when using process-based\n",
      " |            threading. If unspecified, `workers` will default to 1. If 0, will\n",
      " |            execute the generator on the main thread.\n",
      " |          use_multiprocessing: Boolean. Used for generator or\n",
      " |            `keras.utils.Sequence` input only. If `True`, use process-based\n",
      " |            threading. If unspecified, `use_multiprocessing` will default to\n",
      " |            `False`. Note that because this implementation relies on\n",
      " |            multiprocessing, you should not pass non-picklable arguments to the\n",
      " |            generator as they can't be passed easily to children processes.\n",
      " |          return_dict: If `True`, loss and metric results are returned as a dict,\n",
      " |            with each key being the name of the metric. If `False`, they are\n",
      " |            returned as a list.\n",
      " |      \n",
      " |      See the discussion of `Unpacking behavior for iterator-like inputs` for\n",
      " |      `Model.fit`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Scalar test loss (if the model has a single output and no metrics)\n",
      " |          or list of scalars (if the model has multiple outputs\n",
      " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
      " |          the display labels for the scalar outputs.\n",
      " |      \n",
      " |      Raises:\n",
      " |          RuntimeError: If `model.evaluate` is wrapped in `tf.function`.\n",
      " |          ValueError: in case of invalid arguments.\n",
      " |  \n",
      " |  evaluate_generator(self, generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
      " |      Evaluates the model on a data generator. (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use Model.evaluate, which supports generators.\n",
      " |      \n",
      " |      DEPRECATED:\n",
      " |        `Model.evaluate` now supports generators, so there is no longer any need\n",
      " |        to use this endpoint.\n",
      " |  \n",
      " |  fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_batch_size=None, validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
      " |      Trains the model for a fixed number of epochs (iterations on a dataset).\n",
      " |      \n",
      " |      Arguments:\n",
      " |          x: Input data. It could be:\n",
      " |            - A Numpy array (or array-like), or a list of arrays\n",
      " |              (in case the model has multiple inputs).\n",
      " |            - A TensorFlow tensor, or a list of tensors\n",
      " |              (in case the model has multiple inputs).\n",
      " |            - A dict mapping input names to the corresponding array/tensors,\n",
      " |              if the model has named inputs.\n",
      " |            - A `tf.data` dataset. Should return a tuple\n",
      " |              of either `(inputs, targets)` or\n",
      " |              `(inputs, targets, sample_weights)`.\n",
      " |            - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n",
      " |              or `(inputs, targets, sample_weights)`.\n",
      " |            A more detailed description of unpacking behavior for iterator types\n",
      " |            (Dataset, generator, Sequence) is given below.\n",
      " |          y: Target data. Like the input data `x`,\n",
      " |            it could be either Numpy array(s) or TensorFlow tensor(s).\n",
      " |            It should be consistent with `x` (you cannot have Numpy inputs and\n",
      " |            tensor targets, or inversely). If `x` is a dataset, generator,\n",
      " |            or `keras.utils.Sequence` instance, `y` should\n",
      " |            not be specified (since targets will be obtained from `x`).\n",
      " |          batch_size: Integer or `None`.\n",
      " |              Number of samples per gradient update.\n",
      " |              If unspecified, `batch_size` will default to 32.\n",
      " |              Do not specify the `batch_size` if your data is in the\n",
      " |              form of datasets, generators, or `keras.utils.Sequence` instances\n",
      " |              (since they generate batches).\n",
      " |          epochs: Integer. Number of epochs to train the model.\n",
      " |              An epoch is an iteration over the entire `x` and `y`\n",
      " |              data provided.\n",
      " |              Note that in conjunction with `initial_epoch`,\n",
      " |              `epochs` is to be understood as \"final epoch\".\n",
      " |              The model is not trained for a number of iterations\n",
      " |              given by `epochs`, but merely until the epoch\n",
      " |              of index `epochs` is reached.\n",
      " |          verbose: 0, 1, or 2. Verbosity mode.\n",
      " |              0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
      " |              Note that the progress bar is not particularly useful when\n",
      " |              logged to a file, so verbose=2 is recommended when not running\n",
      " |              interactively (eg, in a production environment).\n",
      " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
      " |              List of callbacks to apply during training.\n",
      " |              See `tf.keras.callbacks`.\n",
      " |          validation_split: Float between 0 and 1.\n",
      " |              Fraction of the training data to be used as validation data.\n",
      " |              The model will set apart this fraction of the training data,\n",
      " |              will not train on it, and will evaluate\n",
      " |              the loss and any model metrics\n",
      " |              on this data at the end of each epoch.\n",
      " |              The validation data is selected from the last samples\n",
      " |              in the `x` and `y` data provided, before shuffling. This argument is\n",
      " |              not supported when `x` is a dataset, generator or\n",
      " |             `keras.utils.Sequence` instance.\n",
      " |          validation_data: Data on which to evaluate\n",
      " |              the loss and any model metrics at the end of each epoch.\n",
      " |              The model will not be trained on this data. Thus, note the fact\n",
      " |              that the validation loss of data provided using `validation_split`\n",
      " |              or `validation_data` is not affected by regularization layers like\n",
      " |              noise and dropuout.\n",
      " |              `validation_data` will override `validation_split`.\n",
      " |              `validation_data` could be:\n",
      " |                - tuple `(x_val, y_val)` of Numpy arrays or tensors\n",
      " |                - tuple `(x_val, y_val, val_sample_weights)` of Numpy arrays\n",
      " |                - dataset\n",
      " |              For the first two cases, `batch_size` must be provided.\n",
      " |              For the last case, `validation_steps` could be provided.\n",
      " |              Note that `validation_data` does not support all the data types that\n",
      " |              are supported in `x`, eg, dict, generator or `keras.utils.Sequence`.\n",
      " |          shuffle: Boolean (whether to shuffle the training data\n",
      " |              before each epoch) or str (for 'batch'). This argument is ignored\n",
      " |              when `x` is a generator. 'batch' is a special option for dealing\n",
      " |              with the limitations of HDF5 data; it shuffles in batch-sized\n",
      " |              chunks. Has no effect when `steps_per_epoch` is not `None`.\n",
      " |          class_weight: Optional dictionary mapping class indices (integers)\n",
      " |              to a weight (float) value, used for weighting the loss function\n",
      " |              (during training only).\n",
      " |              This can be useful to tell the model to\n",
      " |              \"pay more attention\" to samples from\n",
      " |              an under-represented class.\n",
      " |          sample_weight: Optional Numpy array of weights for\n",
      " |              the training samples, used for weighting the loss function\n",
      " |              (during training only). You can either pass a flat (1D)\n",
      " |              Numpy array with the same length as the input samples\n",
      " |              (1:1 mapping between weights and samples),\n",
      " |              or in the case of temporal data,\n",
      " |              you can pass a 2D array with shape\n",
      " |              `(samples, sequence_length)`,\n",
      " |              to apply a different weight to every timestep of every sample. This\n",
      " |              argument is not supported when `x` is a dataset, generator, or\n",
      " |             `keras.utils.Sequence` instance, instead provide the sample_weights\n",
      " |              as the third element of `x`.\n",
      " |          initial_epoch: Integer.\n",
      " |              Epoch at which to start training\n",
      " |              (useful for resuming a previous training run).\n",
      " |          steps_per_epoch: Integer or `None`.\n",
      " |              Total number of steps (batches of samples)\n",
      " |              before declaring one epoch finished and starting the\n",
      " |              next epoch. When training with input tensors such as\n",
      " |              TensorFlow data tensors, the default `None` is equal to\n",
      " |              the number of samples in your dataset divided by\n",
      " |              the batch size, or 1 if that cannot be determined. If x is a\n",
      " |              `tf.data` dataset, and 'steps_per_epoch'\n",
      " |              is None, the epoch will run until the input dataset is exhausted.\n",
      " |              When passing an infinitely repeating dataset, you must specify the\n",
      " |              `steps_per_epoch` argument. This argument is not supported with\n",
      " |              array inputs.\n",
      " |          validation_steps: Only relevant if `validation_data` is provided and\n",
      " |              is a `tf.data` dataset. Total number of steps (batches of\n",
      " |              samples) to draw before stopping when performing validation\n",
      " |              at the end of every epoch. If 'validation_steps' is None, validation\n",
      " |              will run until the `validation_data` dataset is exhausted. In the\n",
      " |              case of an infinitely repeated dataset, it will run into an\n",
      " |              infinite loop. If 'validation_steps' is specified and only part of\n",
      " |              the dataset will be consumed, the evaluation will start from the\n",
      " |              beginning of the dataset at each epoch. This ensures that the same\n",
      " |              validation samples are used every time.\n",
      " |          validation_batch_size: Integer or `None`.\n",
      " |              Number of samples per validation batch.\n",
      " |              If unspecified, will default to `batch_size`.\n",
      " |              Do not specify the `validation_batch_size` if your data is in the\n",
      " |              form of datasets, generators, or `keras.utils.Sequence` instances\n",
      " |              (since they generate batches).\n",
      " |          validation_freq: Only relevant if validation data is provided. Integer\n",
      " |              or `collections_abc.Container` instance (e.g. list, tuple, etc.).\n",
      " |              If an integer, specifies how many training epochs to run before a\n",
      " |              new validation run is performed, e.g. `validation_freq=2` runs\n",
      " |              validation every 2 epochs. If a Container, specifies the epochs on\n",
      " |              which to run validation, e.g. `validation_freq=[1, 2, 10]` runs\n",
      " |              validation at the end of the 1st, 2nd, and 10th epochs.\n",
      " |          max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
      " |              input only. Maximum size for the generator queue.\n",
      " |              If unspecified, `max_queue_size` will default to 10.\n",
      " |          workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
      " |              only. Maximum number of processes to spin up\n",
      " |              when using process-based threading. If unspecified, `workers`\n",
      " |              will default to 1. If 0, will execute the generator on the main\n",
      " |              thread.\n",
      " |          use_multiprocessing: Boolean. Used for generator or\n",
      " |              `keras.utils.Sequence` input only. If `True`, use process-based\n",
      " |              threading. If unspecified, `use_multiprocessing` will default to\n",
      " |              `False`. Note that because this implementation relies on\n",
      " |              multiprocessing, you should not pass non-picklable arguments to\n",
      " |              the generator as they can't be passed easily to children processes.\n",
      " |      \n",
      " |      Unpacking behavior for iterator-like inputs:\n",
      " |          A common pattern is to pass a tf.data.Dataset, generator, or\n",
      " |        tf.keras.utils.Sequence to the `x` argument of fit, which will in fact\n",
      " |        yield not only features (x) but optionally targets (y) and sample weights.\n",
      " |        Keras requires that the output of such iterator-likes be unambiguous. The\n",
      " |        iterator should return a tuple of length 1, 2, or 3, where the optional\n",
      " |        second and third elements will be used for y and sample_weight\n",
      " |        respectively. Any other type provided will be wrapped in a length one\n",
      " |        tuple, effectively treating everything as 'x'. When yielding dicts, they\n",
      " |        should still adhere to the top-level tuple structure.\n",
      " |        e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate\n",
      " |        features, targets, and weights from the keys of a single dict.\n",
      " |          A notable unsupported data type is the namedtuple. The reason is that\n",
      " |        it behaves like both an ordered datatype (tuple) and a mapping\n",
      " |        datatype (dict). So given a namedtuple of the form:\n",
      " |            `namedtuple(\"example_tuple\", [\"y\", \"x\"])`\n",
      " |        it is ambiguous whether to reverse the order of the elements when\n",
      " |        interpreting the value. Even worse is a tuple of the form:\n",
      " |            `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])`\n",
      " |        where it is unclear if the tuple was intended to be unpacked into x, y,\n",
      " |        and sample_weight or passed through as a single element to `x`. As a\n",
      " |        result the data processing code will simply raise a ValueError if it\n",
      " |        encounters a namedtuple. (Along with instructions to remedy the issue.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          A `History` object. Its `History.history` attribute is\n",
      " |          a record of training loss values and metrics values\n",
      " |          at successive epochs, as well as validation loss values\n",
      " |          and validation metrics values (if applicable).\n",
      " |      \n",
      " |      Raises:\n",
      " |          RuntimeError: 1. If the model was never compiled or,\n",
      " |          2. If `model.fit` is  wrapped in `tf.function`.\n",
      " |      \n",
      " |          ValueError: In case of mismatch between the provided input data\n",
      " |              and what the model expects.\n",
      " |  \n",
      " |  fit_generator(self, generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, validation_freq=1, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)\n",
      " |      Fits the model on data yielded batch-by-batch by a Python generator. (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use Model.fit, which supports generators.\n",
      " |      \n",
      " |      DEPRECATED:\n",
      " |        `Model.fit` now supports generators, so there is no longer any need to use\n",
      " |        this endpoint.\n",
      " |  \n",
      " |  get_layer(self, name=None, index=None)\n",
      " |      Retrieves a layer based on either its name (unique) or index.\n",
      " |      \n",
      " |      If `name` and `index` are both provided, `index` will take precedence.\n",
      " |      Indices are based on order of horizontal graph traversal (bottom-up).\n",
      " |      \n",
      " |      Arguments:\n",
      " |          name: String, name of layer.\n",
      " |          index: Integer, index of layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A layer instance.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: In case of invalid layer name or index.\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Retrieves the weights of the model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A flat list of Numpy arrays.\n",
      " |  \n",
      " |  load_weights(self, filepath, by_name=False, skip_mismatch=False, options=None)\n",
      " |      Loads all layer weights, either from a TensorFlow or an HDF5 weight file.\n",
      " |      \n",
      " |      If `by_name` is False weights are loaded based on the network's\n",
      " |      topology. This means the architecture should be the same as when the weights\n",
      " |      were saved.  Note that layers that don't have weights are not taken into\n",
      " |      account in the topological ordering, so adding or removing layers is fine as\n",
      " |      long as they don't have weights.\n",
      " |      \n",
      " |      If `by_name` is True, weights are loaded into layers only if they share the\n",
      " |      same name. This is useful for fine-tuning or transfer-learning models where\n",
      " |      some of the layers have changed.\n",
      " |      \n",
      " |      Only topological loading (`by_name=False`) is supported when loading weights\n",
      " |      from the TensorFlow format. Note that topological loading differs slightly\n",
      " |      between TensorFlow and HDF5 formats for user-defined classes inheriting from\n",
      " |      `tf.keras.Model`: HDF5 loads based on a flattened list of weights, while the\n",
      " |      TensorFlow format loads based on the object-local names of attributes to\n",
      " |      which layers are assigned in the `Model`'s constructor.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          filepath: String, path to the weights file to load. For weight files in\n",
      " |              TensorFlow format, this is the file prefix (the same as was passed\n",
      " |              to `save_weights`).\n",
      " |          by_name: Boolean, whether to load weights by name or by topological\n",
      " |              order. Only topological loading is supported for weight files in\n",
      " |              TensorFlow format.\n",
      " |          skip_mismatch: Boolean, whether to skip loading of layers where there is\n",
      " |              a mismatch in the number of weights, or a mismatch in the shape of\n",
      " |              the weight (only valid when `by_name=True`).\n",
      " |          options: Optional `tf.train.CheckpointOptions` object that specifies\n",
      " |              options for loading weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |          When loading a weight file in TensorFlow format, returns the same status\n",
      " |          object as `tf.train.Checkpoint.restore`. When graph building, restore\n",
      " |          ops are run automatically as soon as the network is built (on first call\n",
      " |          for user-defined classes inheriting from `Model`, immediately if it is\n",
      " |          already built).\n",
      " |      \n",
      " |          When loading weights in HDF5 format, returns `None`.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ImportError: If h5py is not available and the weight file is in HDF5\n",
      " |              format.\n",
      " |          ValueError: If `skip_mismatch` is set to `True` when `by_name` is\n",
      " |            `False`.\n",
      " |  \n",
      " |  make_predict_function(self)\n",
      " |      Creates a function that executes one step of inference.\n",
      " |      \n",
      " |      This method can be overridden to support custom inference logic.\n",
      " |      This method is called by `Model.predict` and `Model.predict_on_batch`.\n",
      " |      \n",
      " |      Typically, this method directly controls `tf.function` and\n",
      " |      `tf.distribute.Strategy` settings, and delegates the actual evaluation\n",
      " |      logic to `Model.predict_step`.\n",
      " |      \n",
      " |      This function is cached the first time `Model.predict` or\n",
      " |      `Model.predict_on_batch` is called. The cache is cleared whenever\n",
      " |      `Model.compile` is called.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Function. The function created by this method should accept a\n",
      " |        `tf.data.Iterator`, and return the outputs of the `Model`.\n",
      " |  \n",
      " |  make_test_function(self)\n",
      " |      Creates a function that executes one step of evaluation.\n",
      " |      \n",
      " |      This method can be overridden to support custom evaluation logic.\n",
      " |      This method is called by `Model.evaluate` and `Model.test_on_batch`.\n",
      " |      \n",
      " |      Typically, this method directly controls `tf.function` and\n",
      " |      `tf.distribute.Strategy` settings, and delegates the actual evaluation\n",
      " |      logic to `Model.test_step`.\n",
      " |      \n",
      " |      This function is cached the first time `Model.evaluate` or\n",
      " |      `Model.test_on_batch` is called. The cache is cleared whenever\n",
      " |      `Model.compile` is called.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Function. The function created by this method should accept a\n",
      " |        `tf.data.Iterator`, and return a `dict` containing values that will\n",
      " |        be passed to `tf.keras.Callbacks.on_test_batch_end`.\n",
      " |  \n",
      " |  make_train_function(self)\n",
      " |      Creates a function that executes one step of training.\n",
      " |      \n",
      " |      This method can be overridden to support custom training logic.\n",
      " |      This method is called by `Model.fit` and `Model.train_on_batch`.\n",
      " |      \n",
      " |      Typically, this method directly controls `tf.function` and\n",
      " |      `tf.distribute.Strategy` settings, and delegates the actual training\n",
      " |      logic to `Model.train_step`.\n",
      " |      \n",
      " |      This function is cached the first time `Model.fit` or\n",
      " |      `Model.train_on_batch` is called. The cache is cleared whenever\n",
      " |      `Model.compile` is called.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Function. The function created by this method should accept a\n",
      " |        `tf.data.Iterator`, and return a `dict` containing values that will\n",
      " |        be passed to `tf.keras.Callbacks.on_train_batch_end`, such as\n",
      " |        `{'loss': 0.2, 'accuracy': 0.7}`.\n",
      " |  \n",
      " |  predict(self, x, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
      " |      Generates output predictions for the input samples.\n",
      " |      \n",
      " |      Computation is done in batches. This method is designed for performance in\n",
      " |      large scale inputs. For small amount of inputs that fit in one batch,\n",
      " |      directly using `__call__` is recommended for faster execution, e.g.,\n",
      " |      `model(x)`, or `model(x, training=False)` if you have layers such as\n",
      " |      `tf.keras.layers.BatchNormalization` that behaves differently during\n",
      " |      inference. Also, note the fact that test loss is not affected by\n",
      " |      regularization layers like noise and dropout.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          x: Input samples. It could be:\n",
      " |            - A Numpy array (or array-like), or a list of arrays\n",
      " |              (in case the model has multiple inputs).\n",
      " |            - A TensorFlow tensor, or a list of tensors\n",
      " |              (in case the model has multiple inputs).\n",
      " |            - A `tf.data` dataset.\n",
      " |            - A generator or `keras.utils.Sequence` instance.\n",
      " |            A more detailed description of unpacking behavior for iterator types\n",
      " |            (Dataset, generator, Sequence) is given in the `Unpacking behavior\n",
      " |            for iterator-like inputs` section of `Model.fit`.\n",
      " |          batch_size: Integer or `None`.\n",
      " |              Number of samples per batch.\n",
      " |              If unspecified, `batch_size` will default to 32.\n",
      " |              Do not specify the `batch_size` if your data is in the\n",
      " |              form of dataset, generators, or `keras.utils.Sequence` instances\n",
      " |              (since they generate batches).\n",
      " |          verbose: Verbosity mode, 0 or 1.\n",
      " |          steps: Total number of steps (batches of samples)\n",
      " |              before declaring the prediction round finished.\n",
      " |              Ignored with the default value of `None`. If x is a `tf.data`\n",
      " |              dataset and `steps` is None, `predict` will\n",
      " |              run until the input dataset is exhausted.\n",
      " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
      " |              List of callbacks to apply during prediction.\n",
      " |              See [callbacks](/api_docs/python/tf/keras/callbacks).\n",
      " |          max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
      " |              input only. Maximum size for the generator queue.\n",
      " |              If unspecified, `max_queue_size` will default to 10.\n",
      " |          workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
      " |              only. Maximum number of processes to spin up when using\n",
      " |              process-based threading. If unspecified, `workers` will default\n",
      " |              to 1. If 0, will execute the generator on the main thread.\n",
      " |          use_multiprocessing: Boolean. Used for generator or\n",
      " |              `keras.utils.Sequence` input only. If `True`, use process-based\n",
      " |              threading. If unspecified, `use_multiprocessing` will default to\n",
      " |              `False`. Note that because this implementation relies on\n",
      " |              multiprocessing, you should not pass non-picklable arguments to\n",
      " |              the generator as they can't be passed easily to children processes.\n",
      " |      \n",
      " |      See the discussion of `Unpacking behavior for iterator-like inputs` for\n",
      " |      `Model.fit`. Note that Model.predict uses the same interpretation rules as\n",
      " |      `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all\n",
      " |      three methods.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Numpy array(s) of predictions.\n",
      " |      \n",
      " |      Raises:\n",
      " |          RuntimeError: If `model.predict` is wrapped in `tf.function`.\n",
      " |          ValueError: In case of mismatch between the provided\n",
      " |              input data and the model's expectations,\n",
      " |              or in case a stateful model receives a number of samples\n",
      " |              that is not a multiple of the batch size.\n",
      " |  \n",
      " |  predict_generator(self, generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
      " |      Generates predictions for the input samples from a data generator. (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use Model.predict, which supports generators.\n",
      " |      \n",
      " |      DEPRECATED:\n",
      " |        `Model.predict` now supports generators, so there is no longer any need\n",
      " |        to use this endpoint.\n",
      " |  \n",
      " |  predict_on_batch(self, x)\n",
      " |      Returns predictions for a single batch of samples.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          x: Input data. It could be: - A Numpy array (or array-like), or a list\n",
      " |            of arrays (in case the model has multiple inputs). - A TensorFlow\n",
      " |            tensor, or a list of tensors (in case the model has multiple inputs).\n",
      " |      \n",
      " |      Returns:\n",
      " |          Numpy array(s) of predictions.\n",
      " |      \n",
      " |      Raises:\n",
      " |          RuntimeError: If `model.predict_on_batch` is wrapped in `tf.function`.\n",
      " |          ValueError: In case of mismatch between given number of inputs and\n",
      " |            expectations of the model.\n",
      " |  \n",
      " |  predict_step(self, data)\n",
      " |      The logic for one inference step.\n",
      " |      \n",
      " |      This method can be overridden to support custom inference logic.\n",
      " |      This method is called by `Model.make_predict_function`.\n",
      " |      \n",
      " |      This method should contain the mathemetical logic for one step of inference.\n",
      " |      This typically includes the forward pass.\n",
      " |      \n",
      " |      Configuration details for *how* this logic is run (e.g. `tf.function` and\n",
      " |      `tf.distribute.Strategy` settings), should be left to\n",
      " |      `Model.make_predict_function`, which can also be overridden.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        data: A nested structure of `Tensor`s.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The result of one inference step, typically the output of calling the\n",
      " |        `Model` on data.\n",
      " |  \n",
      " |  reset_metrics(self)\n",
      " |      Resets the state of all the metrics in the model.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
      " |      >>> outputs = tf.keras.layers.Dense(2)(inputs)\n",
      " |      >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
      " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
      " |      \n",
      " |      >>> x = np.random.random((2, 3))\n",
      " |      >>> y = np.random.randint(0, 2, (2, 2))\n",
      " |      >>> _ = model.fit(x, y, verbose=0)\n",
      " |      >>> assert all(float(m.result()) for m in model.metrics)\n",
      " |      \n",
      " |      >>> model.reset_metrics()\n",
      " |      >>> assert all(float(m.result()) == 0 for m in model.metrics)\n",
      " |  \n",
      " |  reset_states(self)\n",
      " |  \n",
      " |  save(self, filepath, overwrite=True, include_optimizer=True, save_format=None, signatures=None, options=None)\n",
      " |      Saves the model to Tensorflow SavedModel or a single HDF5 file.\n",
      " |      \n",
      " |      The savefile includes:\n",
      " |      \n",
      " |      - The model architecture, allowing to re-instantiate the model.\n",
      " |      - The model weights.\n",
      " |      - The state of the optimizer, allowing to resume training\n",
      " |          exactly where you left off.\n",
      " |      \n",
      " |      This allows you to save the entirety of the state of a model\n",
      " |      in a single file.\n",
      " |      \n",
      " |      Saved models can be reinstantiated via `keras.models.load_model`.\n",
      " |      The model returned by `load_model` is a compiled model ready to be used\n",
      " |      (unless the saved model was never compiled in the first place).\n",
      " |      \n",
      " |      Models built with the Sequential and Functional API can be saved to both the\n",
      " |      HDF5 and SavedModel formats. Subclassed models can only be saved with the\n",
      " |      SavedModel format.\n",
      " |      \n",
      " |      Note that the model weights may have different scoped names after being\n",
      " |      loaded. Scoped names include the model/layer names, such as\n",
      " |      `\"dense_1/kernel:0\"`. It is recommended that you use the layer properties to\n",
      " |       access specific variables, e.g. `model.get_layer(\"dense_1\").kernel`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          filepath: String, PathLike, path to SavedModel or H5 file to save the\n",
      " |              model.\n",
      " |          overwrite: Whether to silently overwrite any existing file at the\n",
      " |              target location, or provide the user with a manual prompt.\n",
      " |          include_optimizer: If True, save optimizer's state together.\n",
      " |          save_format: Either `'tf'` or `'h5'`, indicating whether to save the\n",
      " |              model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X,\n",
      " |              and 'h5' in TF 1.X.\n",
      " |          signatures: Signatures to save with the SavedModel. Applicable to the\n",
      " |              'tf' format only. Please see the `signatures` argument in\n",
      " |              `tf.saved_model.save` for details.\n",
      " |          options: Optional `tf.saved_model.SaveOptions` object that specifies\n",
      " |              options for saving to SavedModel.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      from keras.models import load_model\n",
      " |      \n",
      " |      model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
      " |      del model  # deletes the existing model\n",
      " |      \n",
      " |      # returns a compiled model\n",
      " |      # identical to the previous one\n",
      " |      model = load_model('my_model.h5')\n",
      " |      ```\n",
      " |  \n",
      " |  save_weights(self, filepath, overwrite=True, save_format=None, options=None)\n",
      " |      Saves all layer weights.\n",
      " |      \n",
      " |      Either saves in HDF5 or in TensorFlow format based on the `save_format`\n",
      " |      argument.\n",
      " |      \n",
      " |      When saving in HDF5 format, the weight file has:\n",
      " |        - `layer_names` (attribute), a list of strings\n",
      " |            (ordered names of model layers).\n",
      " |        - For every layer, a `group` named `layer.name`\n",
      " |            - For every such layer group, a group attribute `weight_names`,\n",
      " |                a list of strings\n",
      " |                (ordered names of weights tensor of the layer).\n",
      " |            - For every weight in the layer, a dataset\n",
      " |                storing the weight value, named after the weight tensor.\n",
      " |      \n",
      " |      When saving in TensorFlow format, all objects referenced by the network are\n",
      " |      saved in the same format as `tf.train.Checkpoint`, including any `Layer`\n",
      " |      instances or `Optimizer` instances assigned to object attributes. For\n",
      " |      networks constructed from inputs and outputs using `tf.keras.Model(inputs,\n",
      " |      outputs)`, `Layer` instances used by the network are tracked/saved\n",
      " |      automatically. For user-defined classes which inherit from `tf.keras.Model`,\n",
      " |      `Layer` instances must be assigned to object attributes, typically in the\n",
      " |      constructor. See the documentation of `tf.train.Checkpoint` and\n",
      " |      `tf.keras.Model` for details.\n",
      " |      \n",
      " |      While the formats are the same, do not mix `save_weights` and\n",
      " |      `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should be\n",
      " |      loaded using `Model.load_weights`. Checkpoints saved using\n",
      " |      `tf.train.Checkpoint.save` should be restored using the corresponding\n",
      " |      `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over\n",
      " |      `save_weights` for training checkpoints.\n",
      " |      \n",
      " |      The TensorFlow format matches objects and variables by starting at a root\n",
      " |      object, `self` for `save_weights`, and greedily matching attribute\n",
      " |      names. For `Model.save` this is the `Model`, and for `Checkpoint.save` this\n",
      " |      is the `Checkpoint` even if the `Checkpoint` has a model attached. This\n",
      " |      means saving a `tf.keras.Model` using `save_weights` and loading into a\n",
      " |      `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match\n",
      " |      the `Model`'s variables. See the [guide to training\n",
      " |      checkpoints](https://www.tensorflow.org/guide/checkpoint) for details\n",
      " |      on the TensorFlow format.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          filepath: String or PathLike, path to the file to save the weights to.\n",
      " |              When saving in TensorFlow format, this is the prefix used for\n",
      " |              checkpoint files (multiple files are generated). Note that the '.h5'\n",
      " |              suffix causes weights to be saved in HDF5 format.\n",
      " |          overwrite: Whether to silently overwrite any existing file at the\n",
      " |              target location, or provide the user with a manual prompt.\n",
      " |          save_format: Either 'tf' or 'h5'. A `filepath` ending in '.h5' or\n",
      " |              '.keras' will default to HDF5 if `save_format` is `None`. Otherwise\n",
      " |              `None` defaults to 'tf'.\n",
      " |          options: Optional `tf.train.CheckpointOptions` object that specifies\n",
      " |              options for saving weights.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ImportError: If h5py is not available when attempting to save in HDF5\n",
      " |              format.\n",
      " |          ValueError: For invalid/unknown format arguments.\n",
      " |  \n",
      " |  summary(self, line_length=None, positions=None, print_fn=None)\n",
      " |      Prints a string summary of the network.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          line_length: Total length of printed lines\n",
      " |              (e.g. set this to adapt the display to different\n",
      " |              terminal window sizes).\n",
      " |          positions: Relative or absolute positions of log elements\n",
      " |              in each line. If not provided,\n",
      " |              defaults to `[.33, .55, .67, 1.]`.\n",
      " |          print_fn: Print function to use. Defaults to `print`.\n",
      " |              It will be called on each line of the summary.\n",
      " |              You can set it to a custom function\n",
      " |              in order to capture the string summary.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if `summary()` is called before the model is built.\n",
      " |  \n",
      " |  test_on_batch(self, x, y=None, sample_weight=None, reset_metrics=True, return_dict=False)\n",
      " |      Test the model on a single batch of samples.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          x: Input data. It could be: - A Numpy array (or array-like), or a list\n",
      " |            of arrays (in case the model has multiple inputs). - A TensorFlow\n",
      " |            tensor, or a list of tensors (in case the model has multiple inputs).\n",
      " |            - A dict mapping input names to the corresponding array/tensors, if\n",
      " |            the model has named inputs.\n",
      " |          y: Target data. Like the input data `x`, it could be either Numpy\n",
      " |            array(s) or TensorFlow tensor(s). It should be consistent with `x`\n",
      " |            (you cannot have Numpy inputs and tensor targets, or inversely).\n",
      " |          sample_weight: Optional array of the same length as x, containing\n",
      " |            weights to apply to the model's loss for each sample. In the case of\n",
      " |            temporal data, you can pass a 2D array with shape (samples,\n",
      " |            sequence_length), to apply a different weight to every timestep of\n",
      " |            every sample.\n",
      " |          reset_metrics: If `True`, the metrics returned will be only for this\n",
      " |            batch. If `False`, the metrics will be statefully accumulated across\n",
      " |            batches.\n",
      " |          return_dict: If `True`, loss and metric results are returned as a dict,\n",
      " |            with each key being the name of the metric. If `False`, they are\n",
      " |            returned as a list.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Scalar test loss (if the model has a single output and no metrics)\n",
      " |          or list of scalars (if the model has multiple outputs\n",
      " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
      " |          the display labels for the scalar outputs.\n",
      " |      \n",
      " |      Raises:\n",
      " |          RuntimeError: If `model.test_on_batch` is wrapped in `tf.function`.\n",
      " |          ValueError: In case of invalid user-provided arguments.\n",
      " |  \n",
      " |  test_step(self, data)\n",
      " |      The logic for one evaluation step.\n",
      " |      \n",
      " |      This method can be overridden to support custom evaluation logic.\n",
      " |      This method is called by `Model.make_test_function`.\n",
      " |      \n",
      " |      This function should contain the mathemetical logic for one step of\n",
      " |      evaluation.\n",
      " |      This typically includes the forward pass, loss calculation, and metrics\n",
      " |      updates.\n",
      " |      \n",
      " |      Configuration details for *how* this logic is run (e.g. `tf.function` and\n",
      " |      `tf.distribute.Strategy` settings), should be left to\n",
      " |      `Model.make_test_function`, which can also be overridden.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        data: A nested structure of `Tensor`s.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `dict` containing values that will be passed to\n",
      " |        `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the\n",
      " |        values of the `Model`'s metrics are returned.\n",
      " |  \n",
      " |  to_json(self, **kwargs)\n",
      " |      Returns a JSON string containing the network configuration.\n",
      " |      \n",
      " |      To load a network from a JSON save file, use\n",
      " |      `keras.models.model_from_json(json_string, custom_objects={})`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          **kwargs: Additional keyword arguments\n",
      " |              to be passed to `json.dumps()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A JSON string.\n",
      " |  \n",
      " |  to_yaml(self, **kwargs)\n",
      " |      Returns a yaml string containing the network configuration.\n",
      " |      \n",
      " |      To load a network from a yaml save file, use\n",
      " |      `keras.models.model_from_yaml(yaml_string, custom_objects={})`.\n",
      " |      \n",
      " |      `custom_objects` should be a dictionary mapping\n",
      " |      the names of custom losses / layers / etc to the corresponding\n",
      " |      functions / classes.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          **kwargs: Additional keyword arguments\n",
      " |              to be passed to `yaml.dump()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A YAML string.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ImportError: if yaml module is not found.\n",
      " |  \n",
      " |  train_on_batch(self, x, y=None, sample_weight=None, class_weight=None, reset_metrics=True, return_dict=False)\n",
      " |      Runs a single gradient update on a single batch of data.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          x: Input data. It could be:\n",
      " |            - A Numpy array (or array-like), or a list of arrays\n",
      " |                (in case the model has multiple inputs).\n",
      " |            - A TensorFlow tensor, or a list of tensors\n",
      " |                (in case the model has multiple inputs).\n",
      " |            - A dict mapping input names to the corresponding array/tensors,\n",
      " |                if the model has named inputs.\n",
      " |          y: Target data. Like the input data `x`, it could be either Numpy\n",
      " |            array(s) or TensorFlow tensor(s). It should be consistent with `x`\n",
      " |            (you cannot have Numpy inputs and tensor targets, or inversely).\n",
      " |          sample_weight: Optional array of the same length as x, containing\n",
      " |            weights to apply to the model's loss for each sample. In the case of\n",
      " |            temporal data, you can pass a 2D array with shape (samples,\n",
      " |            sequence_length), to apply a different weight to every timestep of\n",
      " |            every sample.\n",
      " |          class_weight: Optional dictionary mapping class indices (integers) to a\n",
      " |            weight (float) to apply to the model's loss for the samples from this\n",
      " |            class during training. This can be useful to tell the model to \"pay\n",
      " |            more attention\" to samples from an under-represented class.\n",
      " |          reset_metrics: If `True`, the metrics returned will be only for this\n",
      " |            batch. If `False`, the metrics will be statefully accumulated across\n",
      " |            batches.\n",
      " |          return_dict: If `True`, loss and metric results are returned as a dict,\n",
      " |            with each key being the name of the metric. If `False`, they are\n",
      " |            returned as a list.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Scalar training loss\n",
      " |          (if the model has a single output and no metrics)\n",
      " |          or list of scalars (if the model has multiple outputs\n",
      " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
      " |          the display labels for the scalar outputs.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If `model.train_on_batch` is wrapped in `tf.function`.\n",
      " |        ValueError: In case of invalid user-provided arguments.\n",
      " |  \n",
      " |  train_step(self, data)\n",
      " |      The logic for one training step.\n",
      " |      \n",
      " |      This method can be overridden to support custom training logic.\n",
      " |      This method is called by `Model.make_train_function`.\n",
      " |      \n",
      " |      This method should contain the mathemetical logic for one step of training.\n",
      " |      This typically includes the forward pass, loss calculation, backpropagation,\n",
      " |      and metric updates.\n",
      " |      \n",
      " |      Configuration details for *how* this logic is run (e.g. `tf.function` and\n",
      " |      `tf.distribute.Strategy` settings), should be left to\n",
      " |      `Model.make_train_function`, which can also be overridden.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        data: A nested structure of `Tensor`s.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `dict` containing values that will be passed to\n",
      " |        `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the\n",
      " |        values of the `Model`'s metrics are returned. Example:\n",
      " |        `{'loss': 0.2, 'accuracy': 0.7}`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from tensorflow.python.keras.engine.training.Model:\n",
      " |  \n",
      " |  __new__(cls, *args, **kwargs)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.keras.engine.training.Model:\n",
      " |  \n",
      " |  distribute_strategy\n",
      " |      The `tf.distribute.Strategy` this model was created under.\n",
      " |  \n",
      " |  metrics\n",
      " |      Returns the model's metrics added using `compile`, `add_metric` APIs.\n",
      " |      \n",
      " |      Note: Metrics passed to `compile()` are available only after a `keras.Model`\n",
      " |      has been trained/evaluated on actual data.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
      " |      >>> outputs = tf.keras.layers.Dense(2)(inputs)\n",
      " |      >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
      " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
      " |      >>> [m.name for m in model.metrics]\n",
      " |      []\n",
      " |      \n",
      " |      >>> x = np.random.random((2, 3))\n",
      " |      >>> y = np.random.randint(0, 2, (2, 2))\n",
      " |      >>> model.fit(x, y)\n",
      " |      >>> [m.name for m in model.metrics]\n",
      " |      ['loss', 'mae']\n",
      " |      \n",
      " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
      " |      >>> d = tf.keras.layers.Dense(2, name='out')\n",
      " |      >>> output_1 = d(inputs)\n",
      " |      >>> output_2 = d(inputs)\n",
      " |      >>> model = tf.keras.models.Model(\n",
      " |      ...    inputs=inputs, outputs=[output_1, output_2])\n",
      " |      >>> model.add_metric(\n",
      " |      ...    tf.reduce_sum(output_2), name='mean', aggregation='mean')\n",
      " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"])\n",
      " |      >>> model.fit(x, (y, y))\n",
      " |      >>> [m.name for m in model.metrics]\n",
      " |      ['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae',\n",
      " |      'out_1_acc', 'mean']\n",
      " |  \n",
      " |  metrics_names\n",
      " |      Returns the model's display labels for all outputs.\n",
      " |      \n",
      " |      Note: `metrics_names` are available only after a `keras.Model` has been\n",
      " |      trained/evaluated on actual data.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
      " |      >>> outputs = tf.keras.layers.Dense(2)(inputs)\n",
      " |      >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
      " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
      " |      >>> model.metrics_names\n",
      " |      []\n",
      " |      \n",
      " |      >>> x = np.random.random((2, 3))\n",
      " |      >>> y = np.random.randint(0, 2, (2, 2))\n",
      " |      >>> model.fit(x, y)\n",
      " |      >>> model.metrics_names\n",
      " |      ['loss', 'mae']\n",
      " |      \n",
      " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
      " |      >>> d = tf.keras.layers.Dense(2, name='out')\n",
      " |      >>> output_1 = d(inputs)\n",
      " |      >>> output_2 = d(inputs)\n",
      " |      >>> model = tf.keras.models.Model(\n",
      " |      ...    inputs=inputs, outputs=[output_1, output_2])\n",
      " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"])\n",
      " |      >>> model.fit(x, (y, y))\n",
      " |      >>> model.metrics_names\n",
      " |      ['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae',\n",
      " |      'out_1_acc']\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |      List of all non-trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Non-trainable weights are *not* updated during training. They are expected\n",
      " |      to be updated manually in `call()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of non-trainable variables.\n",
      " |  \n",
      " |  run_eagerly\n",
      " |      Settable attribute indicating whether the model should run eagerly.\n",
      " |      \n",
      " |      Running eagerly means that your model will be run step by step,\n",
      " |      like Python code. Your model might run slower, but it should become easier\n",
      " |      for you to debug it by stepping into individual layer calls.\n",
      " |      \n",
      " |      By default, we will attempt to compile your model to a static graph to\n",
      " |      deliver the best execution performance.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Boolean, whether the model should run eagerly.\n",
      " |  \n",
      " |  state_updates\n",
      " |      Deprecated, do NOT use! (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      " |      \n",
      " |      Returns the `updates` from all layers that are stateful.\n",
      " |      \n",
      " |      This is useful for separating training updates and\n",
      " |      state updates, e.g. when we need to update a layer's internal state\n",
      " |      during prediction.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of update ops.\n",
      " |  \n",
      " |  trainable_weights\n",
      " |      List of all trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Trainable weights are updated via gradient descent during training.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of trainable variables.\n",
      " |  \n",
      " |  weights\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __call__(self, *args, **kwargs)\n",
      " |      Wraps `call`, applying pre- and post-processing steps.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        *args: Positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: Keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |      \n",
      " |      Note:\n",
      " |        - The following optional keyword arguments are reserved for specific uses:\n",
      " |          * `training`: Boolean scalar tensor of Python boolean indicating\n",
      " |            whether the `call` is meant for training or inference.\n",
      " |          * `mask`: Boolean input mask.\n",
      " |        - If the layer's `call` method takes a `mask` argument (as some Keras\n",
      " |          layers do), its default value will be set to the mask generated\n",
      " |          for `inputs` by the previous layer (if `input` did come from\n",
      " |          a layer that generated a corresponding mask, i.e. if it came from\n",
      " |          a Keras layer with masking support.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if the layer's `call` method returns None (an invalid value).\n",
      " |        RuntimeError: if `super().__init__()` was not called in the constructor.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_loss(self, losses, **kwargs)\n",
      " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Some losses (for instance, activity regularization losses) may be dependent\n",
      " |      on the inputs passed when calling a layer. Hence, when reusing the same\n",
      " |      layer on different inputs `a` and `b`, some entries in `layer.losses` may\n",
      " |      be dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This method can be used inside a subclassed layer or model's `call`\n",
      " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyLayer(tf.keras.layers.Layer):\n",
      " |        def call(self, inputs):\n",
      " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any loss Tensors passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      losses become part of the model's topology and are tracked in `get_config`.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Activity regularization.\n",
      " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      ```\n",
      " |      \n",
      " |      If this is not the case for your loss (if, for example, your loss references\n",
      " |      a `Variable` of one of the model's layers), you can wrap your loss in a\n",
      " |      zero-argument lambda. These losses are not tracked as part of the model's\n",
      " |      topology since they can't be serialized.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      d = tf.keras.layers.Dense(10)\n",
      " |      x = d(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Weight regularization.\n",
      " |      model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
      " |      ```\n",
      " |      \n",
      " |      Arguments:\n",
      " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n",
      " |          may also be zero-argument callables which create a loss tensor.\n",
      " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
      " |          Accepted values:\n",
      " |            inputs - Deprecated, will be automatically inferred.\n",
      " |  \n",
      " |  add_metric(self, value, name=None, **kwargs)\n",
      " |      Adds metric tensor to the layer.\n",
      " |      \n",
      " |      This method can be used inside the `call()` method of a subclassed layer\n",
      " |      or model.\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyMetricLayer(tf.keras.layers.Layer):\n",
      " |        def __init__(self):\n",
      " |          super(MyMetricLayer, self).__init__(name='my_metric_layer')\n",
      " |          self.mean = metrics_module.Mean(name='metric_1')\n",
      " |      \n",
      " |        def call(self, inputs):\n",
      " |          self.add_metric(self.mean(x))\n",
      " |          self.add_metric(math_ops.reduce_sum(x), name='metric_2')\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any tensor passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      metrics become part of the model's topology and are tracked when you\n",
      " |      save the model via `save()`.\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      model.add_metric(math_ops.reduce_sum(x), name='metric_1')\n",
      " |      ```\n",
      " |      \n",
      " |      Note: Calling `add_metric()` with the result of a metric object on a\n",
      " |      Functional Model, as shown in the example below, is not supported. This is\n",
      " |      because we cannot trace the metric result tensor back to the model's inputs.\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        value: Metric tensor.\n",
      " |        name: String metric name.\n",
      " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
      " |          Accepted values:\n",
      " |          `aggregation` - When the `value` tensor provided is not the result of\n",
      " |          calling a `keras.Metric` instance, it will be aggregated by default\n",
      " |          using a `keras.Metric.Mean`.\n",
      " |  \n",
      " |  add_update(self, updates, inputs=None)\n",
      " |      Add update op(s), potentially dependent on layer inputs. (deprecated arguments)\n",
      " |      \n",
      " |      Warning: SOME ARGUMENTS ARE DEPRECATED: `(inputs)`. They will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      `inputs` is now automatically inferred\n",
      " |      \n",
      " |      Weight updates (for instance, the updates of the moving mean and variance\n",
      " |      in a BatchNormalization layer) may be dependent on the inputs passed\n",
      " |      when calling a layer. Hence, when reusing the same layer on\n",
      " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
      " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This call is ignored when eager execution is enabled (in that case, variable\n",
      " |      updates are run on the fly and thus do not need to be tracked for later\n",
      " |      execution).\n",
      " |      \n",
      " |      Arguments:\n",
      " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
      " |          that returns an update op. A zero-arg callable should be passed in\n",
      " |          order to disable running the updates by setting `trainable=False`\n",
      " |          on this Layer, when executing in Eager mode.\n",
      " |        inputs: Deprecated, will be automatically inferred.\n",
      " |  \n",
      " |  add_variable(self, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! Alias for `add_weight`. (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.add_weight` method instead.\n",
      " |  \n",
      " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, partitioner=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregation.NONE: 0>, **kwargs)\n",
      " |      Adds a new variable to the layer.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        name: Variable name.\n",
      " |        shape: Variable shape. Defaults to scalar if unspecified.\n",
      " |        dtype: The type of the variable. Defaults to `self.dtype` or `float32`.\n",
      " |        initializer: Initializer instance (callable).\n",
      " |        regularizer: Regularizer instance (callable).\n",
      " |        trainable: Boolean, whether the variable should be part of the layer's\n",
      " |          \"trainable_variables\" (e.g. variables, biases)\n",
      " |          or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
      " |          Note that `trainable` cannot be `True` if `synchronization`\n",
      " |          is set to `ON_READ`.\n",
      " |        constraint: Constraint instance (callable).\n",
      " |        partitioner: Partitioner to be passed to the `Trackable` API.\n",
      " |        use_resource: Whether to use `ResourceVariable`.\n",
      " |        synchronization: Indicates when a distributed a variable will be\n",
      " |          aggregated. Accepted values are constants defined in the class\n",
      " |          `tf.VariableSynchronization`. By default the synchronization is set to\n",
      " |          `AUTO` and the current `DistributionStrategy` chooses\n",
      " |          when to synchronize. If `synchronization` is set to `ON_READ`,\n",
      " |          `trainable` must not be set to `True`.\n",
      " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
      " |          Accepted values are constants defined in the class\n",
      " |          `tf.VariableAggregation`.\n",
      " |        **kwargs: Additional keyword arguments. Accepted values are `getter`,\n",
      " |          `collections`, `experimental_autocast` and `caching_device`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The created variable. Usually either a `Variable` or `ResourceVariable`\n",
      " |        instance. If `partitioner` is not `None`, a `PartitionedVariable`\n",
      " |        instance is returned.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called with partitioned variable regularization and\n",
      " |          eager execution is enabled.\n",
      " |        ValueError: When giving unsupported dtype and no initializer or when\n",
      " |          trainable has been set to True with synchronization set as `ON_READ`.\n",
      " |  \n",
      " |  apply(self, inputs, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.__call__` method instead.\n",
      " |      \n",
      " |      This is an alias of `self.__call__`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor(s).\n",
      " |        *args: additional positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |  \n",
      " |  compute_output_signature(self, input_signature)\n",
      " |      Compute the output tensor signature of the layer based on the inputs.\n",
      " |      \n",
      " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
      " |      and dtype information for a tensor. This method allows layers to provide\n",
      " |      output dtype information if it is different from the input dtype.\n",
      " |      For any layer that doesn't implement this function,\n",
      " |      the framework will fall back to use `compute_output_shape`, and will\n",
      " |      assume that the output dtype matches the input dtype.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
      " |          objects, describing a candidate input for the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Single TensorSpec or nested structure of TensorSpec objects, describing\n",
      " |          how the layer would transform the provided input.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If input_signature contains a non-TensorSpec object.\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Count the total number of scalars composing the weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An integer count.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if the layer isn't yet built\n",
      " |            (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_losses_for(self, inputs)\n",
      " |      Deprecated, do NOT use! (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.losses` instead.\n",
      " |      \n",
      " |      Retrieves losses relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of loss tensors of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_updates_for(self, inputs)\n",
      " |      Deprecated, do NOT use! (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.updates` instead.\n",
      " |      \n",
      " |      Retrieves updates relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of update ops of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the layer, from Numpy arrays.\n",
      " |      \n",
      " |      The weights of a layer represent the state of the layer. This function\n",
      " |      sets the weight values from numpy arrays. The weight values should be\n",
      " |      passed in the order they are created by the layer. Note that the layer's\n",
      " |      weights must be instantiated before calling this function by calling\n",
      " |      the layer.\n",
      " |      \n",
      " |      For example, a Dense layer returns a list of two values-- per-output\n",
      " |      weights and the bias value. These can be used to set the weights of another\n",
      " |      Dense layer:\n",
      " |      \n",
      " |      >>> a = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
      " |      >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      " |      >>> a.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> b = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
      " |      >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      " |      >>> b.get_weights()\n",
      " |      [array([[2.],\n",
      " |             [2.],\n",
      " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> b.set_weights(a.get_weights())\n",
      " |      >>> b.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      \n",
      " |      Arguments:\n",
      " |          weights: a list of Numpy arrays. The number\n",
      " |              of arrays and their shape must match\n",
      " |              number of the dimensions of the weights\n",
      " |              of the layer (i.e. it should match the\n",
      " |              output of `get_weights`).\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If the provided weights list does not match the\n",
      " |              layer's specifications.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  activity_regularizer\n",
      " |      Optional regularizer function for the output of this layer.\n",
      " |  \n",
      " |  dtype\n",
      " |      Dtype used by the weights of the layer, set in the constructor.\n",
      " |  \n",
      " |  dynamic\n",
      " |      Whether the layer is dynamic (eager-only); set in the constructor.\n",
      " |  \n",
      " |  inbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  losses\n",
      " |      List of losses added using the `add_loss()` API.\n",
      " |      \n",
      " |      Variable regularization tensors are created when this property is accessed,\n",
      " |      so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n",
      " |      propagate gradients back to the corresponding variables.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      >>> class MyLayer(tf.keras.layers.Layer):\n",
      " |      ...   def call(self, inputs):\n",
      " |      ...     self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
      " |      ...     return inputs\n",
      " |      >>> l = MyLayer()\n",
      " |      >>> l(np.ones((10, 1)))\n",
      " |      >>> l.losses\n",
      " |      [1.0]\n",
      " |      \n",
      " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
      " |      >>> x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      >>> model = tf.keras.Model(inputs, outputs)\n",
      " |      >>> # Activity regularization.\n",
      " |      >>> model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      >>> model.losses\n",
      " |      [<tf.Tensor 'Abs:0' shape=() dtype=float32>]\n",
      " |      \n",
      " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
      " |      >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones')\n",
      " |      >>> x = d(inputs)\n",
      " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      >>> model = tf.keras.Model(inputs, outputs)\n",
      " |      >>> # Weight regularization.\n",
      " |      >>> model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
      " |      >>> model.losses\n",
      " |      [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of tensors.\n",
      " |  \n",
      " |  name\n",
      " |      Name of the layer (string), set in the constructor.\n",
      " |  \n",
      " |  non_trainable_variables\n",
      " |  \n",
      " |  outbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  stateful\n",
      " |  \n",
      " |  supports_masking\n",
      " |      Whether this layer supports computing a mask using `compute_mask`.\n",
      " |  \n",
      " |  trainable\n",
      " |  \n",
      " |  trainable_variables\n",
      " |      Sequence of trainable variables owned by this module and its submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  updates\n",
      " |      DEPRECATED FUNCTION\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      " |  \n",
      " |  variables\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Alias of `self.weights`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  with_name_scope(method) from builtins.type\n",
      " |      Decorator to automatically enter the module name scope.\n",
      " |      \n",
      " |      >>> class MyModule(tf.Module):\n",
      " |      ...   @tf.Module.with_name_scope\n",
      " |      ...   def __call__(self, x):\n",
      " |      ...     if not hasattr(self, 'w'):\n",
      " |      ...       self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))\n",
      " |      ...     return tf.matmul(x, self.w)\n",
      " |      \n",
      " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
      " |      names included the module name:\n",
      " |      \n",
      " |      >>> mod = MyModule()\n",
      " |      >>> mod(tf.ones([1, 2]))\n",
      " |      <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)>\n",
      " |      >>> mod.w\n",
      " |      <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32,\n",
      " |      numpy=..., dtype=float32)>\n",
      " |      \n",
      " |      Args:\n",
      " |        method: The method to wrap.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The original method wrapped such that it enters the module's name scope.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  name_scope\n",
      " |      Returns a `tf.name_scope` instance for this class.\n",
      " |  \n",
      " |  submodules\n",
      " |      Sequence of all sub-modules.\n",
      " |      \n",
      " |      Submodules are modules which are properties of this module, or found as\n",
      " |      properties of modules which are properties of this module (and so on).\n",
      " |      \n",
      " |      >>> a = tf.Module()\n",
      " |      >>> b = tf.Module()\n",
      " |      >>> c = tf.Module()\n",
      " |      >>> a.b = b\n",
      " |      >>> b.c = c\n",
      " |      >>> list(a.submodules) == [b, c]\n",
      " |      True\n",
      " |      >>> list(b.submodules) == [c]\n",
      " |      True\n",
      " |      >>> list(c.submodules) == []\n",
      " |      True\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of all submodules.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help( keras.models.Sequential )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class KerasClassifier in module tensorflow.python.keras.wrappers.scikit_learn:\n",
      "\n",
      "class KerasClassifier(BaseWrapper)\n",
      " |  KerasClassifier(build_fn=None, **sk_params)\n",
      " |  \n",
      " |  Implementation of the scikit-learn classifier API for Keras.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      KerasClassifier\n",
      " |      BaseWrapper\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  fit(self, x, y, **kwargs)\n",
      " |      Constructs a new model with `build_fn` & fit the model to `(x, y)`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          x : array-like, shape `(n_samples, n_features)`\n",
      " |              Training samples where `n_samples` is the number of samples\n",
      " |              and `n_features` is the number of features.\n",
      " |          y : array-like, shape `(n_samples,)` or `(n_samples, n_outputs)`\n",
      " |              True labels for `x`.\n",
      " |          **kwargs: dictionary arguments\n",
      " |              Legal arguments are the arguments of `Sequential.fit`\n",
      " |      \n",
      " |      Returns:\n",
      " |          history : object\n",
      " |              details about the training history at each epoch.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: In case of invalid shape for `y` argument.\n",
      " |  \n",
      " |  predict(self, x, **kwargs)\n",
      " |      Returns the class predictions for the given test data.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          x: array-like, shape `(n_samples, n_features)`\n",
      " |              Test samples where `n_samples` is the number of samples\n",
      " |              and `n_features` is the number of features.\n",
      " |          **kwargs: dictionary arguments\n",
      " |              Legal arguments are the arguments\n",
      " |              of `Sequential.predict_classes`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          preds: array-like, shape `(n_samples,)`\n",
      " |              Class predictions.\n",
      " |  \n",
      " |  predict_proba(self, x, **kwargs)\n",
      " |      Returns class probability estimates for the given test data.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          x: array-like, shape `(n_samples, n_features)`\n",
      " |              Test samples where `n_samples` is the number of samples\n",
      " |              and `n_features` is the number of features.\n",
      " |          **kwargs: dictionary arguments\n",
      " |              Legal arguments are the arguments\n",
      " |              of `Sequential.predict_classes`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          proba: array-like, shape `(n_samples, n_outputs)`\n",
      " |              Class probability estimates.\n",
      " |              In the case of binary classification,\n",
      " |              to match the scikit-learn API,\n",
      " |              will return an array of shape `(n_samples, 2)`\n",
      " |              (instead of `(n_sample, 1)` as in Keras).\n",
      " |  \n",
      " |  score(self, x, y, **kwargs)\n",
      " |      Returns the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          x: array-like, shape `(n_samples, n_features)`\n",
      " |              Test samples where `n_samples` is the number of samples\n",
      " |              and `n_features` is the number of features.\n",
      " |          y: array-like, shape `(n_samples,)` or `(n_samples, n_outputs)`\n",
      " |              True labels for `x`.\n",
      " |          **kwargs: dictionary arguments\n",
      " |              Legal arguments are the arguments of `Sequential.evaluate`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          score: float\n",
      " |              Mean accuracy of predictions on `x` wrt. `y`.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If the underlying model isn't configured to\n",
      " |              compute accuracy. You should pass `metrics=[\"accuracy\"]` to\n",
      " |              the `.compile()` method of the model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseWrapper:\n",
      " |  \n",
      " |  __init__(self, build_fn=None, **sk_params)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  check_params(self, params)\n",
      " |      Checks for user typos in `params`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          params: dictionary; the parameters to be checked\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if any member of `params` is not a valid argument.\n",
      " |  \n",
      " |  filter_sk_params(self, fn, override=None)\n",
      " |      Filters `sk_params` and returns those in `fn`'s arguments.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          fn : arbitrary function\n",
      " |          override: dictionary, values to override `sk_params`\n",
      " |      \n",
      " |      Returns:\n",
      " |          res : dictionary containing variables\n",
      " |              in both `sk_params` and `fn`'s arguments.\n",
      " |  \n",
      " |  get_params(self, **params)\n",
      " |      Gets parameters for this estimator.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          **params: ignored (exists for API compatibility).\n",
      " |      \n",
      " |      Returns:\n",
      " |          Dictionary of parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Sets the parameters of this estimator.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          **params: Dictionary of parameter names mapped to their values.\n",
      " |      \n",
      " |      Returns:\n",
      " |          self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseWrapper:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help( keras.wrappers.scikit_learn.KerasClassifier )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
